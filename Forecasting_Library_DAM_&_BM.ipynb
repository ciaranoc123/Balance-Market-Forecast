{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c79f7298",
   "metadata": {},
   "source": [
    "Found below are the libraries for generating quantile and regular forecasts in both the balancing and day ahead market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ac5217",
   "metadata": {},
   "source": [
    "Datasets access : https://drive.google.com/drive/u/0/folders/1GSJhwvhRZ5X5A0uJRZzkzCuJ8xu9kDcX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db2753e",
   "metadata": {},
   "source": [
    "Multi-Headed, Multi-Input Deep neural network Recurrent neural network for quantile forecast in the BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d0e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# months 1-7 search\n",
    "import os ;\n",
    "# os.environ['HDF5_DISABLE_VERSION_CHECK']='2'\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt \n",
    "from datetime import datetime\n",
    "from datetime import timedelta as td\n",
    "import traceback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import concat\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "from math import floor\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from hyperopt import hp, fmin, tpe\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.activations import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/coconnor/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[:7250,:]\n",
    "Y_train=Y.iloc[:7250,:]\n",
    "X_test=X.iloc[7250:8739,:]\n",
    "Y_test=Y.iloc[7250:8739,:]\n",
    "\n",
    "def qloss(qs, y_true, y_pred):\n",
    "    # Pinball loss for multiple quantiles\n",
    "    q = tf.constant(np.array([qs]), dtype=tf.float32)\n",
    "    e = y_true - y_pred\n",
    "    v = tf.maximum(q*e, (q-1)*e)\n",
    "    return K.mean(v)\n",
    "\n",
    "loss_10th_p = lambda y_true, y_pred: qloss(0.1, y_true, y_pred)\n",
    "loss_30th_p = lambda y_true, y_pred: qloss(0.3, y_true, y_pred)\n",
    "loss_50th_p = lambda y_true, y_pred: qloss(0.5, y_true, y_pred)\n",
    "loss_70th_p = lambda y_true, y_pred: qloss(0.7, y_true, y_pred)\n",
    "loss_90th_p = lambda y_true, y_pred: qloss(0.9, y_true, y_pred)\n",
    "\n",
    "\n",
    "rnn_train_LSTM_1 = X_train.loc[:,\"lag_-3x1\":\"lag_-50x1\"]\n",
    "rnn_train_LSTM_2 = X_train.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "rnn_train_LSTM_3 = X_train.loc[:,\"lag_-2x3\":\"lag_-49x3\"]\n",
    "rnn_train_LSTM_4 = X_train.loc[:,\"lag_0x6\":\"lag_-47x6\"]\n",
    "rnn_train_LSTM_5 = X_train.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "\n",
    "rnn_test_LSTM_1 = X_test.loc[:,\"lag_-3x1\": \"lag_-50x1\"]\n",
    "rnn_test_LSTM_2 = X_test.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "rnn_test_LSTM_3 = X_test.loc[:,\"lag_-2x3\": \"lag_-49x3\"]\n",
    "rnn_test_LSTM_4 = X_test.loc[:,\"lag_0x6\" : \"lag_-47x6\"]\n",
    "rnn_test_LSTM_5 = X_test.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "\n",
    "rnn_train_ffnn_1 = X_train.loc[:,\"lag_2x7\": \"lag_17x7\"]\n",
    "rnn_train_ffnn_2 = X_train.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "rnn_train_ffnn_3 = X_train.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "rnn_train_ffnn_4 = X_train.loc[:,\"lag_2x10\": \"lag_17x10\"]\n",
    "rnn_train_ffnn_5 = X_train.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "\n",
    "rnn_test_ffnn_1 = X_test.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "rnn_test_ffnn_2 = X_test.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "rnn_test_ffnn_3 = X_test.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "rnn_test_ffnn_4 = X_test.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "rnn_test_ffnn_5 = X_test.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "\n",
    "rnn_Y = Y_train.loc[:,\"lag_2y\": \"lag_17y\"]\n",
    "\n",
    "X_scaler_LSTM_1 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_2 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_3 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_4 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler_ffnn_1 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_2 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_3 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_4 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train_LSTM_1 = X_scaler_LSTM_1.fit_transform(rnn_train_LSTM_1)\n",
    "rnn_scaled_train_LSTM_2 = X_scaler_LSTM_2.fit_transform(rnn_train_LSTM_2)\n",
    "rnn_scaled_train_LSTM_3 = X_scaler_LSTM_3.fit_transform(rnn_train_LSTM_3)\n",
    "rnn_scaled_train_LSTM_4 = X_scaler_LSTM_4.fit_transform(rnn_train_LSTM_4)\n",
    "rnn_scaled_train_LSTM_5 = X_scaler_LSTM_5.fit_transform(rnn_train_LSTM_5)\n",
    "\n",
    "rnn_scaled_train_ffnn_1 = X_scaler_ffnn_1.fit_transform(rnn_train_ffnn_1)\n",
    "rnn_scaled_train_ffnn_2 = X_scaler_ffnn_2.fit_transform(rnn_train_ffnn_2)\n",
    "rnn_scaled_train_ffnn_3 = X_scaler_ffnn_3.fit_transform(rnn_train_ffnn_3)\n",
    "rnn_scaled_train_ffnn_4 = X_scaler_ffnn_4.fit_transform(rnn_train_ffnn_4)\n",
    "rnn_scaled_train_ffnn_5 = X_scaler_ffnn_5.fit_transform(rnn_train_ffnn_5)\n",
    "\n",
    "Y_train_Scaled = Y_scaler.fit_transform(Y_train)\n",
    "Y_test_scaled = Y_scaler.transform(Y_test)\n",
    "\n",
    "X_train_Scaled_LSTM = np.hstack(\n",
    "    (rnn_scaled_train_LSTM_1, rnn_scaled_train_LSTM_2, rnn_scaled_train_LSTM_3,\n",
    "     rnn_scaled_train_LSTM_4, rnn_scaled_train_LSTM_5)\n",
    ").reshape(rnn_train_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "X_train_Scaled_ffnn = np.hstack(\n",
    "    (rnn_scaled_train_ffnn_1, rnn_scaled_train_ffnn_2, rnn_scaled_train_ffnn_3,\n",
    "     rnn_scaled_train_ffnn_4, rnn_scaled_train_ffnn_5)\n",
    ").reshape(rnn_train_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled_LSTM = np.hstack(\n",
    "    (X_scaler_LSTM_1.transform(rnn_test_LSTM_1), X_scaler_LSTM_2.transform(rnn_test_LSTM_2),\n",
    "     X_scaler_LSTM_3.transform(rnn_test_LSTM_3), X_scaler_LSTM_4.transform(rnn_test_LSTM_4),\n",
    "     X_scaler_LSTM_5.transform(rnn_test_LSTM_5))\n",
    ").reshape(rnn_test_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled_ffnn = np.hstack(\n",
    "    (X_scaler_ffnn_1.transform(rnn_test_ffnn_1), X_scaler_ffnn_2.transform(rnn_test_ffnn_2),\n",
    "     X_scaler_ffnn_3.transform(rnn_test_ffnn_3), X_scaler_ffnn_4.transform(rnn_test_ffnn_4),\n",
    "     X_scaler_ffnn_5.transform(rnn_test_ffnn_5))\n",
    ").reshape(rnn_test_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "i_shape_lstm = (X_train_Scaled_LSTM.shape[1], X_train_Scaled_LSTM.shape[2])\n",
    "i_shape_ffnn = (X_train_Scaled_ffnn.shape[1], X_train_Scaled_ffnn.shape[2])\n",
    "\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                                       test_start_time: dt, test_end_time: dt):\n",
    "\n",
    "    # These are the dataframes that will be returned from the method.\n",
    "    train_X_LSTM = None\n",
    "    train_X_ffnn = None\n",
    "\n",
    "    train_y = None\n",
    "    test_X_LSTM = None\n",
    "    test_X_ffnn = None\n",
    "    test_y = None\n",
    "    test_df = None\n",
    "    train_df = None\n",
    "\n",
    "    try:\n",
    "\n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "        #             return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        # Remove any rows with nan's etc (there shouldn't be any in the input).\n",
    "        participant_df = participant_df.dropna()\n",
    "\n",
    "        date_format = \"%m/%d/%Y %H:%M\"\n",
    "\n",
    "        # The train dataframe, it will be used later to create train_X and train_y.\n",
    "        train_df = None\n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)\n",
    "        train_df = participant_df[\n",
    "            (participant_df.index >= train_start_time_str) & (participant_df.index < train_end_time_str)].copy(\n",
    "            deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\n",
    "                \"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")\n",
    "        #             return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        # Create the test dataframe, it will be used later to create test_X and test_y\n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)\n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format)\n",
    "        test_df = participant_df[\n",
    "            (participant_df.index >= test_start_time_str) & (participant_df.index < test_end_time_str)].copy(\n",
    "            deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\n",
    "                \"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")\n",
    "        #             return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        rnn_train_LSTM_1 = train_df.loc[:,\"lag_-3x1\":\"lag_-50x1\"]\n",
    "        rnn_train_LSTM_2 = train_df.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "        rnn_train_LSTM_3 = train_df.loc[:,\"lag_-2x3\":\"lag_-49x3\"]\n",
    "        rnn_train_LSTM_4 = train_df.loc[:,\"lag_0x6\":\"lag_-47x6\"]\n",
    "        rnn_train_LSTM_5 = train_df.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "\n",
    "        rnn_test_LSTM_1 = test_df.loc[:,\"lag_-3x1\": \"lag_-50x1\"]\n",
    "        rnn_test_LSTM_2 = test_df.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "        rnn_test_LSTM_3 = test_df.loc[:,\"lag_-2x3\": \"lag_-49x3\"]\n",
    "        rnn_test_LSTM_4 = test_df.loc[:,\"lag_0x6\" : \"lag_-47x6\"]\n",
    "        rnn_test_LSTM_5 = test_df.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "        \n",
    "        rnn_train_ffnn_1 = train_df.loc[:,\"lag_2x7\": \"lag_17x7\"]\n",
    "        rnn_train_ffnn_2 = train_df.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "        rnn_train_ffnn_3 = train_df.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "        rnn_train_ffnn_4 = train_df.loc[:,\"lag_2x10\": \"lag_17x10\"]\n",
    "        rnn_train_ffnn_5 = train_df.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "        \n",
    "        rnn_test_ffnn_1 = test_df.loc[:,\"lag_2x7\": \"lag_17x7\"]\n",
    "        rnn_test_ffnn_2 = test_df.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "        rnn_test_ffnn_3 = test_df.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "        rnn_test_ffnn_4 = test_df.loc[:,\"lag_2x10\": \"lag_17x10\"]\n",
    "        rnn_test_ffnn_5 = test_df.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "\n",
    "        rnn_Y = train_df.loc[:,\"lag_2y\": \"lag_17y\"]\n",
    "\n",
    "        X_scaler_LSTM_1 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_LSTM_2 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_LSTM_3 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_LSTM_4 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_LSTM_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "        X_scaler_ffnn_1 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_ffnn_2 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_ffnn_3 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_ffnn_4 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_ffnn_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "        rnn_scaled_train_LSTM_1 = X_scaler_LSTM_1.fit_transform(rnn_train_LSTM_1)\n",
    "        rnn_scaled_train_LSTM_2 = X_scaler_LSTM_2.fit_transform(rnn_train_LSTM_2)\n",
    "        rnn_scaled_train_LSTM_3 = X_scaler_LSTM_3.fit_transform(rnn_train_LSTM_3)\n",
    "        rnn_scaled_train_LSTM_4 = X_scaler_LSTM_4.fit_transform(rnn_train_LSTM_4)\n",
    "        rnn_scaled_train_LSTM_5 = X_scaler_LSTM_5.fit_transform(rnn_train_LSTM_5)\n",
    "\n",
    "        rnn_scaled_train_ffnn_1 = X_scaler_ffnn_1.fit_transform(rnn_train_ffnn_1)\n",
    "        rnn_scaled_train_ffnn_2 = X_scaler_ffnn_2.fit_transform(rnn_train_ffnn_2)\n",
    "        rnn_scaled_train_ffnn_3 = X_scaler_ffnn_3.fit_transform(rnn_train_ffnn_3)\n",
    "        rnn_scaled_train_ffnn_4 = X_scaler_ffnn_4.fit_transform(rnn_train_ffnn_4)\n",
    "        rnn_scaled_train_ffnn_5 = X_scaler_ffnn_5.fit_transform(rnn_train_ffnn_5)\n",
    "\n",
    "        train_y = Y_scaler.fit_transform(rnn_Y)\n",
    "        Y_scaler_n = Y_scaler.fit(rnn_Y)\n",
    "\n",
    "        train_X_LSTM = np.hstack(\n",
    "            (rnn_scaled_train_LSTM_1, rnn_scaled_train_LSTM_2, rnn_scaled_train_LSTM_3,\n",
    "             rnn_scaled_train_LSTM_4, rnn_scaled_train_LSTM_5)\n",
    "        ).reshape(rnn_train_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "        train_X_ffnn = np.hstack(\n",
    "            (rnn_scaled_train_ffnn_1, rnn_scaled_train_ffnn_2, rnn_scaled_train_ffnn_3,\n",
    "             rnn_scaled_train_ffnn_4, rnn_scaled_train_ffnn_5)\n",
    "        ).reshape(rnn_train_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "        test_X_LSTM = np.hstack(\n",
    "            (X_scaler_LSTM_1.transform(rnn_test_LSTM_1), X_scaler_LSTM_2.transform(rnn_test_LSTM_2),\n",
    "             X_scaler_LSTM_3.transform(rnn_test_LSTM_3), X_scaler_LSTM_4.transform(rnn_test_LSTM_4),\n",
    "             X_scaler_LSTM_5.transform(rnn_test_LSTM_5))\n",
    "        ).reshape(rnn_test_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "        test_X_ffnn = np.hstack(\n",
    "            (X_scaler_ffnn_1.transform(rnn_test_ffnn_1), X_scaler_ffnn_2.transform(rnn_test_ffnn_2),\n",
    "             X_scaler_ffnn_3.transform(rnn_test_ffnn_3), X_scaler_ffnn_4.transform(rnn_test_ffnn_4),\n",
    "             X_scaler_ffnn_5.transform(rnn_test_ffnn_5))\n",
    "        ).reshape(rnn_test_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "        test_y = test_df.iloc[:, 0:16]\n",
    "\n",
    "        return train_X_LSTM, train_X_ffnn, train_y, test_X_LSTM, test_X_ffnn, test_y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X_LSTM, train_X_ffnn, train_y, test_X_LSTM, test_X_ffnn, test_y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "\n",
    "def fit_multitarget_model(model, X_train_LSTM, X_train_ffnn, Y_train, X_test_LSTM, X_test_ffnn, Y_test,\n",
    "                          actuals_and_forecast_df, targets, Y_scaler_n):\n",
    "    \"\"\"\n",
    "    Fits the model to the training data.\n",
    "    Then uses the test data to produce a forecast.\n",
    "    Returns a dataframe containing the forecasts and actual values for each of the target variables.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        Model object i.e. randomforestregressor, linear model or other.\n",
    "    X_train : dataframe\n",
    "        The explanatory variables for the train/calibration set, numeric columns may already have been scaled.\n",
    "    Y_train : dataframe\n",
    "        The target variables for the train/calibration set. Might/Mighn't be scaled.\n",
    "    X_test : dataframe\n",
    "        The explanatory variables for the test set, columns may be scaled. It will comprise of 24 rows (1 row for each delivery period in the trading day).\n",
    "    Y_test : dataframe\n",
    "        The target variables for the test set i.e. what we would like to forecast. Similar to the previous bullet point, the dataframe will contain 24 rows.\n",
    "    actuals_and_forecast_df : dataframe\n",
    "        Initially the dataframe will only contain the actual values for each of the targets. At the end of the method it will also contain the forecast values.\n",
    "    targets : [str]\n",
    "        These are the items that we want to predict/forecast.\n",
    "    scale_target_variables: boolean\n",
    "       The target vector, do we want to scale it?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Returns a dataframe containing the forecasts and actual values for each of the target variables i.e. test set forecast and actuals.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "        Y_scaler = Y_scaler.fit(Y_train)\n",
    "        cols = Y.columns.values.tolist()\n",
    "        \n",
    "        es = EarlyStopping(monitor='val_loss', mode='min',  patience=20)\n",
    "        model.fit([X_train_LSTM, X_train_ffnn], Y_train, epochs=200, verbose=2,  callbacks=[es], validation_split=0.10)\n",
    "        model_test_predictions = None\n",
    "        model_test_predictions = pd.DataFrame(Y_scaler_n.inverse_transform(np.array(model.predict([X_test_LSTM, X_test_ffnn])).reshape(5, 16), columns=cols))\n",
    "            \n",
    "\n",
    "        print(model_test_predictions)\n",
    "        print(\"test number of observations: \" + str(len(Y_test)))\n",
    "\n",
    "        \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_10\"] = model_test_predictions.iloc[:1,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_30\"] = model_test_predictions.iloc[1:2,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_50\"] = model_test_predictions.iloc[2:3,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist()\n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_70\"] = model_test_predictions.iloc[3:4,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "           \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_90\"] = model_test_predictions.iloc[4:,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist()\n",
    "\n",
    "\n",
    "        return actuals_and_forecast_df\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def rolling_walk_forward_validation(model, data, targets, start_time, end_time, training_days, path):\n",
    "    \"\"\"\n",
    "    This method implements the rolling walk forward validation process.\n",
    "    That is,\n",
    "        (a) fit the model on the train data\n",
    "        (b) use the fitted model on the test explanatory variables i.e. forecast 1 day ahead.\n",
    "        (c) Move the training and test datasets forward by 1 day and repeat.\n",
    "    The method will produce\n",
    "        (1) A csv containing the forecast and actual target values i.e. test set output over the horizon of interest.\n",
    "        (2) For each target variable, a graph of the actual and forecast values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: model\n",
    "        The model that will be used to train the data and produce the 1 day ahead forecasts\n",
    "    data: dataframe\n",
    "        Participant dataframe containing the explanatory and target variables.\n",
    "    explanatory_variables_of_interest : [str]\n",
    "        The columns in data that will be used as explanatory variables when fitting the model.\n",
    "        If there are categorical variables we want to use as explanatory variables, they are incorporated via the features_to_encode argument.\n",
    "    targets: [str]\n",
    "        The columns in data that we want to predict/forecast.\n",
    "    features_to_encode: [str]\n",
    "        If there are variables in data that we would like to apply one hot encoding to, we list them here. These one hot encoded vectors are then used as explanatory variables.\n",
    "    prefix_to_include: [str]\n",
    "        Just a string which will be used to name the columns if we apply one hot encoding (related to the features_to_encode argument).\n",
    "     start_time: dt\n",
    "       We will produce a forecast on unseen data for each trading period between [start_time, end_time].\n",
    "     end_time: dt\n",
    "       See previous point.\n",
    "    training_days: int\n",
    "       The number of training days (negative integer expected).\n",
    "    path, unit_name, scenario: str\n",
    "       The combination of path + unit_name + scenario indicate where the csv will be output to.\n",
    "    scale_explanatory_variables: boolean\n",
    "       The explanatory variables, do we want to scale them?\n",
    "    scale_target_variables: boolean\n",
    "       The target variables, do we want to scale them?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Output will include a  csv of the forecast/actual target values and a graph of the same.\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)\n",
    "        results = pd.DataFrame()\n",
    "\n",
    "        # Each time we\n",
    "        # (a) fit the model on the calibration/train data\n",
    "        # (b) apply it to the test data i.e. forecast 1 day ahead.\n",
    "        # Repeat.\n",
    "        date_format = \"%m/%d/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "\n",
    "        while start_time < end_time:\n",
    "\n",
    "            # Train interval\n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time\n",
    "\n",
    "            # Test interval, the test period is always the day ahead forecast\n",
    "            test_start_time = train_end_time + td(hours=8)\n",
    "            test_end_time = test_start_time + td(minutes=30)\n",
    "\n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "\n",
    "            # Generate the calibration and test dataframes.\n",
    "            train_X_LSTM, train_X_ffnn, train_y, test_X_LSTM, test_X_ffnn, test_y, test_df, train_df, Y_scaler_n = generate_train_and_test_dataframes(\n",
    "                participant_df=dat, train_start_time=train_start_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                train_end_time=train_end_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                test_start_time=test_start_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                test_end_time=test_end_time.strftime(\"%m/%d/%Y %H:%M\"))\n",
    "\n",
    "            if train_X_LSTM is None or len(train_X_LSTM) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(\n",
    "                    train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            if test_X_LSTM is None or len(test_X_LSTM) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(\n",
    "                    test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            # Fit the model to the train datasets, produce a forecast and return a dataframe containing the forecast/actuals.\n",
    "            actuals_and_forecast_df = fit_multitarget_model(model=model, Y_scaler_n=Y_scaler_n,\n",
    "                                                            X_train_LSTM=train_X_LSTM, X_train_ffnn=train_X_ffnn,\n",
    "                                                            Y_train=train_y,\n",
    "                                                            X_test_LSTM=test_X_LSTM, X_test_ffnn=test_X_ffnn,\n",
    "                                                            Y_test=test_y,\n",
    "                                                            actuals_and_forecast_df=test_df.iloc[:, 0:16],\n",
    "                                                            targets=Y.columns.values.tolist())\n",
    "\n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "\n",
    "            start_time = start_time + td(hours=8)\n",
    "\n",
    "        results.to_csv(path + \".csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "def create_model():    \n",
    "    visible1 = Input(shape=(i_shape_lstm))\n",
    "    x1=visible1\n",
    "    \n",
    "    for i in range(1):\n",
    "        x1= LSTM(64, return_sequences= True, activation='sigmoid', input_shape=i_shape_lstm)(x1)\n",
    "    x1 = Dropout(0.222222)(x1)\n",
    "    \n",
    "    input_1 = Flatten()(x1)   \n",
    "    \n",
    "    \n",
    "    visible2 = Input(shape=(i_shape_ffnn))   \n",
    "    x2= visible2 \n",
    "    \n",
    "    for i in range(2):\n",
    "        x2 = Dense(64, 'tanh')(x2)\n",
    "    x2 = Dropout(0.088889)(x2)\n",
    "    \n",
    "    input_2 = Flatten()(x2) \n",
    "            \n",
    "    \n",
    "    merged = concatenate([input_1, input_2])\n",
    "    \n",
    "    a = Dense(128, 'tanh')(merged)\n",
    "    b = Dense(128, 'tanh')(merged)\n",
    "    c = Dense(128, 'tanh')(merged)\n",
    "    d = Dense(128, 'tanh')(merged)\n",
    "    e = Dense(128, 'tanh')(merged)\n",
    "        \n",
    "            \n",
    "    for i in range(1):\n",
    "            a = Dense(128, 'relu')(a) \n",
    "    output_1 =  Dense(16, name='out_10')(a)\n",
    "    \n",
    "    for i in range(1):\n",
    "            b = Dense(128, 'relu')(b) \n",
    "    output_2 =  Dense(16, name='out_30')(b)\n",
    "    \n",
    "    for i in range(1):\n",
    "            c = Dense(128, 'relu')(c) \n",
    "    output_3 =  Dense(16, name='out_50')(c)\n",
    "    \n",
    "    for i in range(1):\n",
    "            d = Dense(128, 'relu')(d) \n",
    "    output_4 =  Dense(16, name='out_70')(d)\n",
    "    \n",
    "    for i in range(1):\n",
    "            e = Dense(128, 'relu')(e) \n",
    "    output_5 =  Dense(16, name='out_90')(e)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[visible1, visible2], outputs=[output_1, output_2, output_3, output_4, output_5])    \n",
    "    opt = Adam(learning_rate= 0.000100)\n",
    "\n",
    "    model.compile(loss=[loss_10th_p, loss_30th_p, loss_50th_p, loss_70th_p, loss_90th_p], optimizer=opt)\n",
    "    return model\n",
    "\n",
    "mmo = KerasRegressor(build_fn=create_model, epochs=200, batch_size=32, verbose=2)\n",
    "\n",
    "rolling_walk_forward_validation(model=mmo, data=dat, start_time='6/1/2020 00:00', end_time='9/1/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(), training_days=-210,\n",
    "                                path=\"/home/coconnor/MH_Q_1-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c02aab8",
   "metadata": {},
   "source": [
    "Multi-Headed Deep neural network Recurrent neural network for forecasting the BM price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0239cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# months 1-7 search\n",
    "import os ;\n",
    "# os.environ['HDF5_DISABLE_VERSION_CHECK']='2'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from datetime import timedelta as td\n",
    "import traceback\n",
    "# from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import concat\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from math import floor\n",
    "from bayes_opt import BayesianOptimization\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "# tensorflow &keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "# from keras.layers import concatenate\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "from keras.activations import *\n",
    "\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import talos\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "# dat = pd.read_csv(\"C:/Users/ciara/OneDrive/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "# =============================================================================\n",
    "# dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "dat = pd.read_csv(\"/home/coconnor/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "# =============================================================================\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[:11666,:]\n",
    "Y_train=Y.iloc[:11666,:]\n",
    "X_test=X.iloc[11666:13155,:]\n",
    "Y_test=Y.iloc[11666:13155,:]\n",
    "\n",
    "rnn_train_LSTM_1 = X_train.loc[:,\"lag_-3x1\":\"lag_-50x1\"]\n",
    "rnn_train_LSTM_2 = X_train.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "rnn_train_LSTM_3 = X_train.loc[:,\"lag_-2x3\":\"lag_-49x3\"]\n",
    "rnn_train_LSTM_4 = X_train.loc[:,\"lag_0x6\":\"lag_-47x6\"]\n",
    "rnn_train_LSTM_5 = X_train.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "\n",
    "rnn_test_LSTM_1 = X_test.loc[:,\"lag_-3x1\": \"lag_-50x1\"]\n",
    "rnn_test_LSTM_2 = X_test.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "rnn_test_LSTM_3 = X_test.loc[:,\"lag_-2x3\": \"lag_-49x3\"]\n",
    "rnn_test_LSTM_4 = X_test.loc[:,\"lag_0x6\" : \"lag_-47x6\"]\n",
    "rnn_test_LSTM_5 = X_test.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "\n",
    "rnn_train_ffnn_1 = X_train.loc[:,\"lag_2x7\": \"lag_17x7\"]\n",
    "rnn_train_ffnn_2 = X_train.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "rnn_train_ffnn_3 = X_train.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "rnn_train_ffnn_4 = X_train.loc[:,\"lag_2x10\": \"lag_17x10\"]\n",
    "rnn_train_ffnn_5 = X_train.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "\n",
    "rnn_test_ffnn_1 = X_test.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "rnn_test_ffnn_2 = X_test.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "rnn_test_ffnn_3 = X_test.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "rnn_test_ffnn_4 = X_test.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "rnn_test_ffnn_5 = X_test.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "\n",
    "rnn_Y = Y_train.loc[:,\"lag_2y\": \"lag_17y\"]\n",
    "\n",
    "X_scaler_LSTM_1 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_2 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_3 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_4 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler_ffnn_1 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_2 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_3 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_4 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train_LSTM_1 = X_scaler_LSTM_1.fit_transform(rnn_train_LSTM_1)\n",
    "rnn_scaled_train_LSTM_2 = X_scaler_LSTM_2.fit_transform(rnn_train_LSTM_2)\n",
    "rnn_scaled_train_LSTM_3 = X_scaler_LSTM_3.fit_transform(rnn_train_LSTM_3)\n",
    "rnn_scaled_train_LSTM_4 = X_scaler_LSTM_4.fit_transform(rnn_train_LSTM_4)\n",
    "rnn_scaled_train_LSTM_5 = X_scaler_LSTM_5.fit_transform(rnn_train_LSTM_5)\n",
    "\n",
    "rnn_scaled_train_ffnn_1 = X_scaler_ffnn_1.fit_transform(rnn_train_ffnn_1)\n",
    "rnn_scaled_train_ffnn_2 = X_scaler_ffnn_2.fit_transform(rnn_train_ffnn_2)\n",
    "rnn_scaled_train_ffnn_3 = X_scaler_ffnn_3.fit_transform(rnn_train_ffnn_3)\n",
    "rnn_scaled_train_ffnn_4 = X_scaler_ffnn_4.fit_transform(rnn_train_ffnn_4)\n",
    "rnn_scaled_train_ffnn_5 = X_scaler_ffnn_5.fit_transform(rnn_train_ffnn_5)\n",
    "\n",
    "Y_train_Scaled = Y_scaler.fit_transform(Y_train)\n",
    "Y_test_scaled = Y_scaler.transform(Y_test)\n",
    "\n",
    "X_train_Scaled_LSTM = np.hstack(\n",
    "    (rnn_scaled_train_LSTM_1, rnn_scaled_train_LSTM_2, rnn_scaled_train_LSTM_3,\n",
    "     rnn_scaled_train_LSTM_4, rnn_scaled_train_LSTM_5)\n",
    ").reshape(rnn_train_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "X_train_Scaled_ffnn = np.hstack(\n",
    "    (rnn_scaled_train_ffnn_1, rnn_scaled_train_ffnn_2, rnn_scaled_train_ffnn_3,\n",
    "     rnn_scaled_train_ffnn_4, rnn_scaled_train_ffnn_5)\n",
    ").reshape(rnn_train_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled_LSTM = np.hstack(\n",
    "    (X_scaler_LSTM_1.transform(rnn_test_LSTM_1), X_scaler_LSTM_2.transform(rnn_test_LSTM_2),\n",
    "     X_scaler_LSTM_3.transform(rnn_test_LSTM_3), X_scaler_LSTM_4.transform(rnn_test_LSTM_4),\n",
    "     X_scaler_LSTM_5.transform(rnn_test_LSTM_5))\n",
    ").reshape(rnn_test_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled_ffnn = np.hstack(\n",
    "    (X_scaler_ffnn_1.transform(rnn_test_ffnn_1), X_scaler_ffnn_2.transform(rnn_test_ffnn_2),\n",
    "     X_scaler_ffnn_3.transform(rnn_test_ffnn_3), X_scaler_ffnn_4.transform(rnn_test_ffnn_4),\n",
    "     X_scaler_ffnn_5.transform(rnn_test_ffnn_5))\n",
    ").reshape(rnn_test_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "i_shape_lstm = (X_train_Scaled_LSTM.shape[1], X_train_Scaled_LSTM.shape[2])\n",
    "i_shape_ffnn = (X_train_Scaled_ffnn.shape[1], X_train_Scaled_ffnn.shape[2])\n",
    "\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                                       test_start_time: dt, test_end_time: dt):\n",
    "\n",
    "\n",
    "    train_X_LSTM = None\n",
    "    train_X_ffnn = None\n",
    "\n",
    "    train_y = None\n",
    "    test_X_LSTM = None\n",
    "    test_X_ffnn = None\n",
    "    test_y = None\n",
    "    test_df = None\n",
    "    train_df = None\n",
    "\n",
    "    try:\n",
    "\n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "\n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        participant_df = participant_df.dropna()\n",
    "\n",
    "        date_format = \"%m/%d/%Y %H:%M\"\n",
    "\n",
    "        train_df = None\n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)\n",
    "        train_df = participant_df[\n",
    "            (participant_df.index >= train_start_time_str) & (participant_df.index < train_end_time_str)].copy(\n",
    "            deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\n",
    "                \"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")\n",
    "\n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)\n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format)\n",
    "        test_df = participant_df[\n",
    "            (participant_df.index >= test_start_time_str) & (participant_df.index < test_end_time_str)].copy(\n",
    "            deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\n",
    "                \"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")\n",
    "\n",
    "        rnn_train_LSTM_1 = train_df.loc[:,\"lag_-3x1\":\"lag_-50x1\"]\n",
    "        rnn_train_LSTM_2 = train_df.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "        rnn_train_LSTM_3 = train_df.loc[:,\"lag_-2x3\":\"lag_-49x3\"]\n",
    "        rnn_train_LSTM_4 = train_df.loc[:,\"lag_0x6\":\"lag_-47x6\"]\n",
    "        rnn_train_LSTM_5 = train_df.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "\n",
    "        rnn_test_LSTM_1 = test_df.loc[:,\"lag_-3x1\": \"lag_-50x1\"]\n",
    "        rnn_test_LSTM_2 = test_df.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "        rnn_test_LSTM_3 = test_df.loc[:,\"lag_-2x3\": \"lag_-49x3\"]\n",
    "        rnn_test_LSTM_4 = test_df.loc[:,\"lag_0x6\" : \"lag_-47x6\"]\n",
    "        rnn_test_LSTM_5 = test_df.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "        \n",
    "        rnn_train_ffnn_1 = train_df.loc[:,\"lag_2x7\": \"lag_17x7\"]\n",
    "        rnn_train_ffnn_2 = train_df.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "        rnn_train_ffnn_3 = train_df.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "        rnn_train_ffnn_4 = train_df.loc[:,\"lag_2x10\": \"lag_17x10\"]\n",
    "        rnn_train_ffnn_5 = train_df.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "        \n",
    "        rnn_test_ffnn_1 = test_df.loc[:,\"lag_2x7\": \"lag_17x7\"]\n",
    "        rnn_test_ffnn_2 = test_df.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "        rnn_test_ffnn_3 = test_df.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "        rnn_test_ffnn_4 = test_df.loc[:,\"lag_2x10\": \"lag_17x10\"]\n",
    "        rnn_test_ffnn_5 = test_df.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "\n",
    "        rnn_Y = train_df.loc[:,\"lag_2y\": \"lag_17y\"]\n",
    "        \n",
    "        X_scaler_LSTM_1 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_LSTM_2 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_LSTM_3 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_LSTM_4 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_LSTM_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "        X_scaler_ffnn_1 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_ffnn_2 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_ffnn_3 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_ffnn_4 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_ffnn_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "        rnn_scaled_train_LSTM_1 = X_scaler_LSTM_1.fit_transform(rnn_train_LSTM_1)\n",
    "        rnn_scaled_train_LSTM_2 = X_scaler_LSTM_2.fit_transform(rnn_train_LSTM_2)\n",
    "        rnn_scaled_train_LSTM_3 = X_scaler_LSTM_3.fit_transform(rnn_train_LSTM_3)\n",
    "        rnn_scaled_train_LSTM_4 = X_scaler_LSTM_4.fit_transform(rnn_train_LSTM_4)\n",
    "        rnn_scaled_train_LSTM_5 = X_scaler_LSTM_5.fit_transform(rnn_train_LSTM_5)\n",
    "\n",
    "        rnn_scaled_train_ffnn_1 = X_scaler_ffnn_1.fit_transform(rnn_train_ffnn_1)\n",
    "        rnn_scaled_train_ffnn_2 = X_scaler_ffnn_2.fit_transform(rnn_train_ffnn_2)\n",
    "        rnn_scaled_train_ffnn_3 = X_scaler_ffnn_3.fit_transform(rnn_train_ffnn_3)\n",
    "        rnn_scaled_train_ffnn_4 = X_scaler_ffnn_4.fit_transform(rnn_train_ffnn_4)\n",
    "        rnn_scaled_train_ffnn_5 = X_scaler_ffnn_5.fit_transform(rnn_train_ffnn_5)\n",
    "\n",
    "        train_y = Y_scaler.fit_transform(rnn_Y)\n",
    "        Y_scaler_n = Y_scaler.fit(rnn_Y)\n",
    "\n",
    "        train_X_LSTM = np.hstack(\n",
    "            (rnn_scaled_train_LSTM_1, rnn_scaled_train_LSTM_2, rnn_scaled_train_LSTM_3,\n",
    "             rnn_scaled_train_LSTM_4, rnn_scaled_train_LSTM_5)\n",
    "        ).reshape(rnn_train_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "        train_X_ffnn = np.hstack(\n",
    "            (rnn_scaled_train_ffnn_1, rnn_scaled_train_ffnn_2, rnn_scaled_train_ffnn_3,\n",
    "             rnn_scaled_train_ffnn_4, rnn_scaled_train_ffnn_5)\n",
    "        ).reshape(rnn_train_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "        test_X_LSTM = np.hstack(\n",
    "            (X_scaler_LSTM_1.transform(rnn_test_LSTM_1), X_scaler_LSTM_2.transform(rnn_test_LSTM_2),\n",
    "             X_scaler_LSTM_3.transform(rnn_test_LSTM_3), X_scaler_LSTM_4.transform(rnn_test_LSTM_4),\n",
    "             X_scaler_LSTM_5.transform(rnn_test_LSTM_5))\n",
    "        ).reshape(rnn_test_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "        test_X_ffnn = np.hstack(\n",
    "            (X_scaler_ffnn_1.transform(rnn_test_ffnn_1), X_scaler_ffnn_2.transform(rnn_test_ffnn_2),\n",
    "             X_scaler_ffnn_3.transform(rnn_test_ffnn_3), X_scaler_ffnn_4.transform(rnn_test_ffnn_4),\n",
    "             X_scaler_ffnn_5.transform(rnn_test_ffnn_5))\n",
    "        ).reshape(rnn_test_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "        test_y = test_df.iloc[:, 0:16]\n",
    "\n",
    "        return train_X_LSTM, train_X_ffnn, train_y, test_X_LSTM, test_X_ffnn, test_y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X_LSTM, train_X_ffnn, train_y, test_X_LSTM, test_X_ffnn, test_y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "\n",
    "def fit_multitarget_model(model, X_train_LSTM, X_train_ffnn, Y_train, X_test_LSTM, X_test_ffnn, Y_test,\n",
    "                          actuals_and_forecast_df, targets, Y_scaler_n):\n",
    "\n",
    "    try:\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "        Y_scaler = Y_scaler.fit(Y_train)\n",
    "        cols = Y.columns.values.tolist()\n",
    "\n",
    "        model.fit([X_train_LSTM, X_train_ffnn], Y_train)\n",
    "        model_test_predictions = None\n",
    "        #         model_test_predictions = model.predict(X_test)\n",
    "        model_test_predictions = pd.DataFrame(\n",
    "            Y_scaler_n.inverse_transform(model.predict([X_test_LSTM, X_test_ffnn]).reshape(1, 16)), columns=cols,\n",
    "            index=Y_test.index)\n",
    "        print(model_test_predictions)\n",
    "        print(\"test number of observations: \" + str(len(Y_test)))\n",
    "        model_test_mse = mean_squared_error(Y_test, model_test_predictions)\n",
    "        model_test_rmse = round(np.sqrt(model_test_mse), 2)\n",
    "        model_test_mae = round(mean_absolute_error(Y_test, model_test_predictions), 2)\n",
    "        print(\"test rmse: \" + str(model_test_rmse))\n",
    "        print(\"test mae: \" + str(model_test_mae))\n",
    "\n",
    "        for i in range(0, len(cols)):\n",
    "            actuals_and_forecast_df[cols[i] + \"_Forecast\"] = model_test_predictions.iloc[:, i].tolist() if len(\n",
    "                cols) > 1 else model_test_predictions.tolist()\n",
    "            predictor_test_mse = mean_squared_error(Y_test[cols[i]], model_test_predictions.iloc[:, i]) if len(\n",
    "                cols) > 1 else mean_squared_error(Y_test[cols[i]], model_test_predictions.tolist())\n",
    "            predictor_test_rmse = round(np.sqrt(predictor_test_mse), 2)\n",
    "            predictor_test_mae = round(mean_absolute_error(Y_test[cols[i]], model_test_predictions.iloc[:, i]),\n",
    "                                       2) if len(cols) > 1 else round(\n",
    "                mean_absolute_error(Y_test[cols[i]], model_test_predictions.tolist()), 2)\n",
    "            print(cols[i] + \" test rmse: \" + str(predictor_test_rmse))\n",
    "            print(cols[i] + \" test mae: \" + str(predictor_test_mae))\n",
    "\n",
    "        Error_i = ([model_test_rmse, model_test_mae])\n",
    "        print(Error_i)\n",
    "        actuals_and_forecast_df = actuals_and_forecast_df.append(Error_i)\n",
    "\n",
    "\n",
    "\n",
    "        return actuals_and_forecast_df\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def rolling_walk_forward_validation(model, data, targets, start_time, end_time, training_days, path):\n",
    "\n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)\n",
    "        results = pd.DataFrame()\n",
    "\n",
    "\n",
    "        date_format = \"%m/%d/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "\n",
    "        while start_time < end_time:\n",
    "\n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time\n",
    "\n",
    "            test_start_time = train_end_time + td(hours=8)\n",
    "            test_end_time = test_start_time + td(minutes=30)\n",
    "\n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "\n",
    "            train_X_LSTM, train_X_ffnn, train_y, test_X_LSTM, test_X_ffnn, test_y, test_df, train_df, Y_scaler_n = generate_train_and_test_dataframes(\n",
    "                participant_df=dat, train_start_time=train_start_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                train_end_time=train_end_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                test_start_time=test_start_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                test_end_time=test_end_time.strftime(\"%m/%d/%Y %H:%M\"))\n",
    "\n",
    "            if train_X_LSTM is None or len(train_X_LSTM) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(\n",
    "                    train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            if test_X_LSTM is None or len(test_X_LSTM) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(\n",
    "                    test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            actuals_and_forecast_df = fit_multitarget_model(model=model, Y_scaler_n=Y_scaler_n,\n",
    "                                                            X_train_LSTM=train_X_LSTM, X_train_ffnn=train_X_ffnn,\n",
    "                                                            Y_train=train_y,\n",
    "                                                            X_test_LSTM=test_X_LSTM, X_test_ffnn=test_X_ffnn,\n",
    "                                                            Y_test=test_y,\n",
    "                                                            actuals_and_forecast_df=test_df.iloc[:, 0:16],\n",
    "                                                            targets=Y.columns.values.tolist())\n",
    "\n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "\n",
    "            start_time = start_time + td(hours=8)\n",
    "\n",
    "        results.to_csv(path + \".csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "def create_model():    \n",
    "            visible1 = Input(shape=(i_shape_lstm))            \n",
    "            dense1 = LSTM(128, return_sequences= True, activation='tanh' , input_shape=i_shape_lstm)(visible1)\n",
    "            dense2 = LSTM(128, return_sequences= True, activation='tanh' , input_shape=i_shape_lstm)(dense1)\n",
    "            do_lstm = Dropout(0.044444, seed=123)(dense2)\n",
    "            dense3 = LSTM(128)(do_lstm)\n",
    "            flat1 = Flatten()(dense3)    \n",
    "    \n",
    "            visible2 = Input(shape=(i_shape_ffnn))        \n",
    "            dense5 = Dense(16, activation='relu')(visible2)        \n",
    "            dense6 = Dense(16, activation=LeakyReLU)(dense5)\n",
    "            do_ffnn = Dropout(0.200000, seed=123)(dense6)\n",
    "            dense7 = Dense(16, activation='relu')(do_ffnn)\n",
    "            flat2 = Flatten()(dense7)\n",
    "    \n",
    "            merged = concatenate([flat1, flat2])\n",
    "            dense_f = Dense(256, activation='relu')(merged)        \n",
    "            outputs = Dense(16)(dense_f)\n",
    "\n",
    "            model = Model(inputs=[visible1, visible2], outputs=outputs)    \n",
    "            opt = Adam(lr=0.004522)\n",
    "            model.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mean_absolute_error'])\n",
    "            return model\n",
    "        \n",
    "es = EarlyStopping(monitor='mean_absolute_error', mode='min', verbose=0, patience=20)\n",
    "mmo = KerasRegressor(build_fn=create_model, epochs=500, batch_size=48, verbose=2, callbacks=[es])\n",
    "\n",
    "rolling_walk_forward_validation(model=mmo, data=dat, start_time='9/1/2020 00:00', end_time='12/1/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(), training_days=-300,\n",
    "                                path=\"/home/coconnor/BM_results_MH_1-13\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8082ba84",
   "metadata": {},
   "source": [
    "SH DNN regular forecast BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0297fa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os ;\n",
    "# os.environ['HDF5_DISABLE_VERSION_CHECK']='2'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import datetime as dt \n",
    "from datetime import datetime\n",
    "from datetime import timedelta as td\n",
    "import traceback\n",
    "# from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import concat\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from math import floor\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "#tensorflow &keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "\n",
    "#sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "\n",
    "from hyperopt import hp, fmin, tpe\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "date_format=\"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/coconnor/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat=pd.DataFrame(dat)\n",
    "dat=dat.bfill(axis ='rows')\n",
    "dat=dat.ffill(axis ='rows')\n",
    "dat=dat._get_numeric_data()\n",
    "\n",
    "\n",
    "\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[:7250,:]\n",
    "Y_train=Y.iloc[:7250,:]\n",
    "X_test=X.iloc[7250:8739,:]\n",
    "Y_test=Y.iloc[7250:8739,:]\n",
    "\n",
    "rnn_train1_a=X_train.loc[:,\"lag_-3x1\":\"lag_-18x1\"]\n",
    "rnn_train1_b=X_train.loc[:,\"lag_-19x1\":\"lag_-34x1\"]\n",
    "rnn_train1_c=X_train.loc[:,\"lag_-35x1\":\"lag_-50x1\"]        \n",
    "rnn_train2_a=X_train.loc[:,\"lag_-3x2\":\"lag_-18x2\"]\n",
    "rnn_train2_b=X_train.loc[:,\"lag_-19x2\":\"lag_-34x2\"]\n",
    "rnn_train2_c=X_train.loc[:,\"lag_-35x2\":\"lag_-50x2\"]        \n",
    "rnn_train3_a=X_train.loc[:,\"lag_-2x3\":\"lag_-17x3\"]\n",
    "rnn_train3_b=X_train.loc[:,\"lag_-18x3\":\"lag_-33x3\"]\n",
    "rnn_train3_c=X_train.loc[:,\"lag_-34x3\":\"lag_-49x3\"]      \n",
    "rnn_train4_a=X_train.loc[:,\"lag_0x6\":\"lag_-15x6\"]\n",
    "rnn_train4_b=X_train.loc[:,\"lag_-16x6\":\"lag_-31x6\"]\n",
    "rnn_train4_c=X_train.loc[:,\"lag_-32x6\":\"lag_-47x6\"]        \n",
    "rnn_train5_a=X_train.loc[:,\"lag_-2x12\":\"lag_-17x12\"]\n",
    "rnn_train5_b=X_train.loc[:,\"lag_-18x12\":\"lag_-33x12\"]\n",
    "rnn_train5_c=X_train.loc[:,\"lag_-34x12\":\"lag_-49x12\"]        \n",
    "rnn_train6=X_train.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "rnn_train7=X_train.loc[:,\"lag_2x8\":\"lag_17x8\"]\n",
    "rnn_train8=X_train.loc[:,\"lag_2x9\":\"lag_17x9\"]\n",
    "rnn_train9=X_train.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "rnn_train10=X_train.loc[:,\"lag_2x11\":\"lag_17x11\"]        \n",
    "        \n",
    "rnn_test1_a= X_test.loc[:,\"lag_-3x1\":\"lag_-18x1\"]\n",
    "rnn_test1_b= X_test.loc[:,\"lag_-19x1\":\"lag_-34x1\"]\n",
    "rnn_test1_c= X_test.loc[:,\"lag_-35x1\":\"lag_-50x1\"]\n",
    "rnn_test2_a= X_test.loc[:,\"lag_-3x2\":\"lag_-18x2\"]\n",
    "rnn_test2_b= X_test.loc[:,\"lag_-19x2\":\"lag_-34x2\"]\n",
    "rnn_test2_c= X_test.loc[:,\"lag_-35x2\":\"lag_-50x2\"]\n",
    "rnn_test3_a= X_test.loc[:,\"lag_-2x3\":\"lag_-17x3\"]\n",
    "rnn_test3_b= X_test.loc[:,\"lag_-18x3\":\"lag_-33x3\"]\n",
    "rnn_test3_c= X_test.loc[:,\"lag_-34x3\":\"lag_-49x3\"]\n",
    "rnn_test4_a= X_test.loc[:,\"lag_0x6\":\"lag_-15x6\"]\n",
    "rnn_test4_b= X_test.loc[:,\"lag_-16x6\":\"lag_-31x6\"]\n",
    "rnn_test4_c= X_test.loc[:,\"lag_-32x6\":\"lag_-47x6\"]\n",
    "rnn_test5_a= X_test.loc[:,\"lag_-2x12\":\"lag_-17x12\"]\n",
    "rnn_test5_b= X_test.loc[:,\"lag_-18x12\":\"lag_-33x12\"]\n",
    "rnn_test5_c= X_test.loc[:,\"lag_-34x12\":\"lag_-49x12\"]\n",
    "rnn_test6=X_test.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "rnn_test7=X_test.loc[:,\"lag_2x8\":\"lag_17x8\"]\n",
    "rnn_test8=X_test.loc[:,\"lag_2x9\":\"lag_17x9\"]\n",
    "rnn_test9=X_test.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "rnn_test10=X_test.loc[:,\"lag_2x11\":\"lag_17x11\"]\n",
    "\n",
    "rnn_Y=Y_train.loc[:,\"lag_2y\" : \"lag_17y\"]\n",
    "\n",
    "\n",
    "X_scaler1_a = preprocessing.MinMaxScaler()\n",
    "X_scaler1_b = preprocessing.MinMaxScaler()\n",
    "X_scaler1_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler2_a = preprocessing.MinMaxScaler()\n",
    "X_scaler2_b = preprocessing.MinMaxScaler()\n",
    "X_scaler2_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler3_a = preprocessing.MinMaxScaler()\n",
    "X_scaler3_b = preprocessing.MinMaxScaler()\n",
    "X_scaler3_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler4_a = preprocessing.MinMaxScaler()\n",
    "X_scaler4_b = preprocessing.MinMaxScaler()\n",
    "X_scaler4_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler5_a = preprocessing.MinMaxScaler()\n",
    "X_scaler5_b = preprocessing.MinMaxScaler()\n",
    "X_scaler5_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "\n",
    "X_scaler6 = preprocessing.MinMaxScaler()\n",
    "X_scaler7 = preprocessing.MinMaxScaler()\n",
    "X_scaler8 = preprocessing.MinMaxScaler()\n",
    "X_scaler9 = preprocessing.MinMaxScaler()\n",
    "X_scaler10 = preprocessing.MinMaxScaler()\n",
    "\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train1_a = X_scaler1_a.fit_transform(rnn_train1_a)\n",
    "rnn_scaled_train1_b = X_scaler1_b.fit_transform(rnn_train1_b)\n",
    "rnn_scaled_train1_c = X_scaler1_c.fit_transform(rnn_train1_c)\n",
    "\n",
    "rnn_scaled_train2_a = X_scaler2_a.fit_transform(rnn_train2_a)\n",
    "rnn_scaled_train2_b = X_scaler2_b.fit_transform(rnn_train2_b)\n",
    "rnn_scaled_train2_c = X_scaler2_c.fit_transform(rnn_train2_c)\n",
    "\n",
    "rnn_scaled_train3_a = X_scaler3_a.fit_transform(rnn_train3_a)\n",
    "rnn_scaled_train3_b = X_scaler3_b.fit_transform(rnn_train3_b)\n",
    "rnn_scaled_train3_c = X_scaler3_c.fit_transform(rnn_train3_c)\n",
    "\n",
    "rnn_scaled_train4_a = X_scaler4_a.fit_transform(rnn_train4_a)\n",
    "rnn_scaled_train4_b = X_scaler4_b.fit_transform(rnn_train4_b)\n",
    "rnn_scaled_train4_c = X_scaler4_c.fit_transform(rnn_train4_c)\n",
    "\n",
    "rnn_scaled_train5_a = X_scaler5_a.fit_transform(rnn_train5_a)\n",
    "rnn_scaled_train5_b = X_scaler5_b.fit_transform(rnn_train5_b)\n",
    "rnn_scaled_train5_c = X_scaler5_c.fit_transform(rnn_train5_c)\n",
    "\n",
    "rnn_scaled_train6 = X_scaler6.fit_transform(rnn_train6)\n",
    "rnn_scaled_train7 = X_scaler7.fit_transform(rnn_train7)\n",
    "rnn_scaled_train8 = X_scaler8.fit_transform(rnn_train8)\n",
    "rnn_scaled_train9 = X_scaler9.fit_transform(rnn_train9)\n",
    "rnn_scaled_train10 = X_scaler10.fit_transform(rnn_train10)\n",
    "\n",
    "Y_train_Scaled   = Y_scaler.fit_transform(Y_train)\n",
    "\n",
    "X_train_Scaled = np.hstack(\n",
    "    (rnn_scaled_train1_a, rnn_scaled_train1_b, rnn_scaled_train1_c, rnn_scaled_train2_a, rnn_scaled_train2_b, rnn_scaled_train2_c,\n",
    "     rnn_scaled_train3_a, rnn_scaled_train3_b, rnn_scaled_train3_c, rnn_scaled_train4_a, rnn_scaled_train4_b, rnn_scaled_train4_c,\n",
    "     rnn_scaled_train5_a, rnn_scaled_train5_b, rnn_scaled_train5_c,rnn_scaled_train6,rnn_scaled_train7, rnn_scaled_train8,\n",
    "     rnn_scaled_train9, rnn_scaled_train10)\n",
    ").reshape(rnn_train6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled = np.hstack(\n",
    "    (X_scaler1_a.transform(rnn_test1_a),X_scaler1_b.transform(rnn_test1_b),X_scaler1_c.transform(rnn_test1_c),\n",
    "     X_scaler2_a.transform(rnn_test2_a),X_scaler2_b.transform(rnn_test2_b),X_scaler2_c.transform(rnn_test2_c),\n",
    "     X_scaler3_a.transform(rnn_test3_a),X_scaler3_b.transform(rnn_test3_b),X_scaler3_c.transform(rnn_test3_c),\n",
    "     X_scaler4_a.transform(rnn_test4_a),X_scaler4_b.transform(rnn_test4_b),X_scaler4_c.transform(rnn_test4_c),\n",
    "     X_scaler5_a.transform(rnn_test5_a),X_scaler5_b.transform(rnn_test5_b),X_scaler5_c.transform(rnn_test5_c),\n",
    "     X_scaler6.transform(rnn_test6),X_scaler7.transform(rnn_test7),X_scaler8.transform(rnn_test8),\n",
    "     X_scaler9.transform(rnn_test9), X_scaler10.transform(rnn_test10))\n",
    ").reshape(rnn_test6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "\n",
    "i_shape=(X_train_Scaled.shape[1], X_train_Scaled.shape[2])\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                        test_start_time: dt, test_end_time: dt):\n",
    "\n",
    "  \n",
    "    train_X = None\n",
    "    train_y = None \n",
    "    test_X = None \n",
    "    test_y = None \n",
    "    test_df = None\n",
    "    train_df = None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "        \n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        participant_df = participant_df.dropna()\n",
    "        \n",
    "        date_format=\"%m/%d/%Y %H:%M\"\n",
    "      \n",
    "        train_df = None\n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)\n",
    "        train_df = participant_df[(participant_df.index>=train_start_time_str) & (participant_df.index<train_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")            \n",
    "\n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)    \n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format) \n",
    "        test_df = participant_df[(participant_df.index>=test_start_time_str) & (participant_df.index<test_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")            \n",
    "\n",
    "        \n",
    "        rnn_train1_a=train_df.loc[:,\"lag_-3x1\":\"lag_-18x1\"]\n",
    "        rnn_train1_b=train_df.loc[:,\"lag_-19x1\":\"lag_-34x1\"]\n",
    "        rnn_train1_c=train_df.loc[:,\"lag_-35x1\":\"lag_-50x1\"]\n",
    "        \n",
    "        rnn_train2_a=train_df.loc[:,\"lag_-3x2\":\"lag_-18x2\"]\n",
    "        rnn_train2_b=train_df.loc[:,\"lag_-19x2\":\"lag_-34x2\"]\n",
    "        rnn_train2_c=train_df.loc[:,\"lag_-35x2\":\"lag_-50x2\"]\n",
    "        \n",
    "        rnn_train3_a=train_df.loc[:,\"lag_-2x3\":\"lag_-17x3\"]\n",
    "        rnn_train3_b=train_df.loc[:,\"lag_-18x3\":\"lag_-33x3\"]\n",
    "        rnn_train3_c=train_df.loc[:,\"lag_-34x3\":\"lag_-49x3\"]\n",
    "        \n",
    "        rnn_train4_a=train_df.loc[:,\"lag_0x6\":\"lag_-15x6\"]\n",
    "        rnn_train4_b=train_df.loc[:,\"lag_-16x6\":\"lag_-31x6\"]\n",
    "        rnn_train4_c=train_df.loc[:,\"lag_-32x6\":\"lag_-47x6\"]\n",
    "        \n",
    "        rnn_train5_a=train_df.loc[:,\"lag_-2x12\":\"lag_-17x12\"]\n",
    "        rnn_train5_b=train_df.loc[:,\"lag_-18x12\":\"lag_-33x12\"]\n",
    "        rnn_train5_c=train_df.loc[:,\"lag_-34x12\":\"lag_-49x12\"]\n",
    "        \n",
    "        rnn_train6=train_df.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "        rnn_train7=train_df.loc[:,\"lag_2x8\":\"lag_17x8\"]\n",
    "        rnn_train8=train_df.loc[:,\"lag_2x9\":\"lag_17x9\"]\n",
    "        rnn_train9=train_df.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "        rnn_train10=train_df.loc[:,\"lag_2x11\":\"lag_17x11\"]\n",
    "\n",
    "        rnn_test1_a=test_df.loc[:,\"lag_-3x1\":\"lag_-18x1\"]\n",
    "        rnn_test1_b=test_df.loc[:,\"lag_-19x1\":\"lag_-34x1\"]\n",
    "        rnn_test1_c=test_df.loc[:,\"lag_-35x1\":\"lag_-50x1\"]\n",
    "\n",
    "        rnn_test2_a=test_df.loc[:,\"lag_-3x2\":\"lag_-18x2\"]\n",
    "        rnn_test2_b=test_df.loc[:,\"lag_-19x2\":\"lag_-34x2\"]\n",
    "        rnn_test2_c=test_df.loc[:,\"lag_-35x2\":\"lag_-50x2\"]\n",
    "\n",
    "        rnn_test3_a=test_df.loc[:,\"lag_-2x3\":\"lag_-17x3\"]\n",
    "        rnn_test3_b=test_df.loc[:,\"lag_-18x3\":\"lag_-33x3\"]\n",
    "        rnn_test3_c=test_df.loc[:,\"lag_-34x3\":\"lag_-49x3\"]\n",
    "\n",
    "        rnn_test4_a=test_df.loc[:,\"lag_0x6\":\"lag_-15x6\"]\n",
    "        rnn_test4_b=test_df.loc[:,\"lag_-16x6\":\"lag_-31x6\"]\n",
    "        rnn_test4_c=test_df.loc[:,\"lag_-32x6\":\"lag_-47x6\"]\n",
    "\n",
    "        rnn_test5_a=test_df.loc[:,\"lag_-2x12\":\"lag_-17x12\"]\n",
    "        rnn_test5_b=test_df.loc[:,\"lag_-18x12\":\"lag_-33x12\"]\n",
    "        rnn_test5_c=test_df.loc[:,\"lag_-34x12\":\"lag_-49x12\"]\n",
    "\n",
    "        rnn_test6=test_df.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "        rnn_test7=test_df.loc[:,\"lag_2x8\":\"lag_17x8\"]\n",
    "        rnn_test8=test_df.loc[:,\"lag_2x9\":\"lag_17x9\"]\n",
    "        rnn_test9=test_df.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "        rnn_test10=test_df.loc[:,\"lag_2x11\":\"lag_17x11\"]\n",
    "\n",
    "        rnn_Y=train_df.loc[:,\"lag_2y\" : \"lag_17y\"]\n",
    "        \n",
    "        \n",
    "        X_scaler1_a = preprocessing.MinMaxScaler()\n",
    "        X_scaler1_b = preprocessing.MinMaxScaler()\n",
    "        X_scaler1_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "        X_scaler2_a = preprocessing.MinMaxScaler()\n",
    "        X_scaler2_b = preprocessing.MinMaxScaler()\n",
    "        X_scaler2_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "        X_scaler3_a = preprocessing.MinMaxScaler()\n",
    "        X_scaler3_b = preprocessing.MinMaxScaler()\n",
    "        X_scaler3_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "        X_scaler4_a = preprocessing.MinMaxScaler()\n",
    "        X_scaler4_b = preprocessing.MinMaxScaler()\n",
    "        X_scaler4_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "        X_scaler5_a = preprocessing.MinMaxScaler()\n",
    "        X_scaler5_b = preprocessing.MinMaxScaler()\n",
    "        X_scaler5_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "\n",
    "        X_scaler6 = preprocessing.MinMaxScaler()\n",
    "        X_scaler7 = preprocessing.MinMaxScaler()\n",
    "        X_scaler8 = preprocessing.MinMaxScaler()\n",
    "        X_scaler9 = preprocessing.MinMaxScaler()\n",
    "        X_scaler10 = preprocessing.MinMaxScaler()\n",
    "\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "        rnn_scaled_train1_a = X_scaler1_a.fit_transform(rnn_train1_a)\n",
    "        rnn_scaled_train1_b = X_scaler1_b.fit_transform(rnn_train1_b)\n",
    "        rnn_scaled_train1_c = X_scaler1_c.fit_transform(rnn_train1_c)\n",
    "\n",
    "        rnn_scaled_train2_a = X_scaler2_a.fit_transform(rnn_train2_a)\n",
    "        rnn_scaled_train2_b = X_scaler2_b.fit_transform(rnn_train2_b)\n",
    "        rnn_scaled_train2_c = X_scaler2_c.fit_transform(rnn_train2_c)\n",
    "\n",
    "        rnn_scaled_train3_a = X_scaler3_a.fit_transform(rnn_train3_a)\n",
    "        rnn_scaled_train3_b = X_scaler3_b.fit_transform(rnn_train3_b)\n",
    "        rnn_scaled_train3_c = X_scaler3_c.fit_transform(rnn_train3_c)\n",
    "\n",
    "        rnn_scaled_train4_a = X_scaler4_a.fit_transform(rnn_train4_a)\n",
    "        rnn_scaled_train4_b = X_scaler4_b.fit_transform(rnn_train4_b)\n",
    "        rnn_scaled_train4_c = X_scaler4_c.fit_transform(rnn_train4_c)\n",
    "\n",
    "        rnn_scaled_train5_a = X_scaler5_a.fit_transform(rnn_train5_a)\n",
    "        rnn_scaled_train5_b = X_scaler5_b.fit_transform(rnn_train5_b)\n",
    "        rnn_scaled_train5_c = X_scaler5_c.fit_transform(rnn_train5_c)\n",
    "\n",
    "        rnn_scaled_train6 = X_scaler6.fit_transform(rnn_train6)\n",
    "        rnn_scaled_train7 = X_scaler7.fit_transform(rnn_train7)\n",
    "        rnn_scaled_train8 = X_scaler8.fit_transform(rnn_train8)\n",
    "        rnn_scaled_train9 = X_scaler9.fit_transform(rnn_train9)\n",
    "        rnn_scaled_train10 = X_scaler10.fit_transform(rnn_train10)\n",
    "\n",
    "        train_y   = Y_scaler.fit_transform(rnn_Y)\n",
    "        Y_scaler_n=Y_scaler.fit(rnn_Y)\n",
    "        train_X = np.hstack(\n",
    "            (rnn_scaled_train1_a, rnn_scaled_train1_b, rnn_scaled_train1_c, rnn_scaled_train2_a, rnn_scaled_train2_b, rnn_scaled_train2_c,\n",
    "             rnn_scaled_train3_a, rnn_scaled_train3_b, rnn_scaled_train3_c, rnn_scaled_train4_a, rnn_scaled_train4_b, rnn_scaled_train4_c,\n",
    "             rnn_scaled_train5_a, rnn_scaled_train5_b, rnn_scaled_train5_c,rnn_scaled_train6,rnn_scaled_train7, rnn_scaled_train8,\n",
    "             rnn_scaled_train9, rnn_scaled_train10)\n",
    "        ).reshape(rnn_train6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "\n",
    "        test_X = np.hstack(\n",
    "            (X_scaler1_a.transform(rnn_test1_a),X_scaler1_b.transform(rnn_test1_b),X_scaler1_c.transform(rnn_test1_c),\n",
    "             X_scaler2_a.transform(rnn_test2_a),X_scaler2_b.transform(rnn_test2_b),X_scaler2_c.transform(rnn_test2_c),\n",
    "             X_scaler3_a.transform(rnn_test3_a),X_scaler3_b.transform(rnn_test3_b),X_scaler3_c.transform(rnn_test3_c),\n",
    "             X_scaler4_a.transform(rnn_test4_a),X_scaler4_b.transform(rnn_test4_b),X_scaler4_c.transform(rnn_test4_c),\n",
    "             X_scaler5_a.transform(rnn_test5_a),X_scaler5_b.transform(rnn_test5_b),X_scaler5_c.transform(rnn_test5_c),\n",
    "             X_scaler6.transform(rnn_test6),X_scaler7.transform(rnn_test7),X_scaler8.transform(rnn_test8),\n",
    "             X_scaler9.transform(rnn_test9), X_scaler10.transform(rnn_test10))\n",
    "        ).reshape(rnn_test6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "        \n",
    "        test_y = test_df.iloc[:, 0:16]\n",
    "                                \n",
    "        return train_X, train_y, test_X, test_y, test_df, train_df, Y_scaler_n\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X, train_y, test_X, test_y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "\n",
    "def fit_multitarget_model(model, X_train, Y_train, X_test, Y_test, actuals_and_forecast_df, targets, Y_scaler_n):\n",
    " \n",
    "    try:\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "        Y_scaler = Y_scaler.fit(Y_train)\n",
    "        cols = Y.columns.values.tolist()                         \n",
    "\n",
    "        model.fit(X_train, Y_train) \n",
    "        model_test_predictions=None        \n",
    "#         model_test_predictions = model.predict(X_test)        \n",
    "        model_test_predictions = pd.DataFrame(Y_scaler_n.inverse_transform(model.predict(X_test).reshape(1,16)), columns=cols, index=Y_test.index)\n",
    "        print(model_test_predictions)\n",
    "        print(\"test number of observations: \" + str(len(Y_test)))\n",
    "        model_test_mse = mean_squared_error(Y_test, model_test_predictions)\n",
    "        model_test_rmse = round(np.sqrt(model_test_mse),2)\n",
    "        model_test_mae = round(mean_absolute_error(Y_test, model_test_predictions),2)\n",
    "        print(\"test rmse: \" + str(model_test_rmse))\n",
    "        print(\"test mae: \" + str(model_test_mae))\n",
    "        \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast\"] = model_test_predictions.iloc[:,i].tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "            predictor_test_mse = mean_squared_error(Y_test[cols[i]], model_test_predictions.iloc[:,i]) if len(cols) > 1 else mean_squared_error(Y_test[cols[i]], model_test_predictions.tolist())\n",
    "            predictor_test_rmse = round(np.sqrt(predictor_test_mse), 2)\n",
    "            predictor_test_mae = round(mean_absolute_error(Y_test[cols[i]], model_test_predictions.iloc[:,i]),2) if len(cols) > 1 else round(mean_absolute_error(Y_test[cols[i]], model_test_predictions.tolist()),2)\n",
    "            print(cols[i] + \" test rmse: \" + str(predictor_test_rmse))\n",
    "            print(cols[i] + \" test mae: \" + str(predictor_test_mae))    \n",
    "            \n",
    "        Error_i= ([model_test_rmse, model_test_mae])\n",
    "        print(Error_i)\n",
    "        actuals_and_forecast_df = actuals_and_forecast_df.append(Error_i)\n",
    "        \n",
    "#return the test set, the target and forecast values.               \n",
    "#         actuals_and_forecast_df.sort_index(inplace=True)\n",
    "#         test_columns = [\"SettlementPeriod\"]\n",
    "#         for i in range(0,len(cols)):\n",
    "#             test_columns.extend([cols[i]+\"_Forecast\",cols[i]])\n",
    "#         actuals_and_forecast_df = actuals_and_forecast_df[test_columns]\n",
    "           \n",
    "        return actuals_and_forecast_df\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "  \n",
    "    \n",
    "def rolling_walk_forward_validation(model, data, targets, start_time, end_time, training_days, path):\n",
    "  \n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)            \n",
    "        results = pd.DataFrame()\n",
    "            \n",
    "\n",
    "        date_format=\"%m/%d/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "        \n",
    "        while start_time < end_time:\n",
    "            \n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time\n",
    "    \n",
    "            test_start_time = train_end_time + td(hours=8)\n",
    "            test_end_time = test_start_time + td(minutes=30)\n",
    "            \n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "    \n",
    "            train_X, train_y, test_X, test_y, test_df, train_df, Y_scaler_n= generate_train_and_test_dataframes(participant_df=dat, train_start_time=train_start_time.strftime(\"%m/%d/%Y %H:%M\"), train_end_time=train_end_time.strftime(\"%m/%d/%Y %H:%M\"), \n",
    "                            test_start_time=test_start_time.strftime(\"%m/%d/%Y %H:%M\"), test_end_time=test_end_time.strftime(\"%m/%d/%Y %H:%M\"))\n",
    "            \n",
    "            \n",
    "            if train_X is None or len(train_X) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "    \n",
    "            if test_X is None or len(test_X) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "            \n",
    "            actuals_and_forecast_df = fit_multitarget_model(model=model,Y_scaler_n=Y_scaler_n, X_train=train_X, Y_train=train_y,\n",
    "                                            X_test=test_X, Y_test=test_y, actuals_and_forecast_df=test_df.iloc[:,0:16], targets=Y.columns.values.tolist())\n",
    "\n",
    "    \n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "        \n",
    "            start_time = start_time + td(minutes=30)\n",
    "            \n",
    "        results.to_csv(path  + \".csv\", index = False)\n",
    "        \n",
    "\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "\n",
    "def create_model():\n",
    "            nn = Sequential()\n",
    "            nn.add(Flatten(input_shape=i_shape))\n",
    "\n",
    "            for i in range(2):\n",
    "                nn.add(Dense(64, input_shape=i_shape, activation='tanh'))\n",
    "                nn.add(BatchNormalization())\n",
    "                \n",
    "            for i in range(1):\n",
    "                nn.add(Dense(128, activation='tanh'))\n",
    "                nn.add(BatchNormalization())\n",
    "   \n",
    "            for i in range(1):\n",
    "                nn.add(Dense(128, activation='relu'))\n",
    "                nn.add(BatchNormalization())\n",
    "           \n",
    "            nn.add(Dropout(0.133333, seed=123))\n",
    "            nn.add(BatchNormalization())\n",
    "            \n",
    "            for i in range(1):\n",
    "                nn.add(Dense(128, activation='relu'))\n",
    "                nn.add(BatchNormalization())\n",
    "                \n",
    "            nn.add(Dense(16, activation=LeakyReLU))\n",
    "            nn.add(Dense(16))\n",
    "            opt = Adam(lr = 0.004522)\n",
    "            nn.compile(loss='mean_squared_error', optimizer=opt, metrics=['mean_absolute_error'])\n",
    "            \n",
    "            return nn\n",
    "es = EarlyStopping(monitor='mean_absolute_error', mode='min', verbose=0, patience=20)\n",
    "mmo = KerasRegressor(build_fn=create_model, epochs=300, batch_size=16, verbose=2, callbacks=[es])\n",
    "\n",
    "\n",
    "rolling_walk_forward_validation(model=mmo, data=dat, start_time='6/1/2020 00:00', end_time='9/1/2020  10:00',       \n",
    "                                targets=Y.columns.values.tolist(), training_days=-210,\n",
    "                                path=\"/home/coconnor/BM_results_SH_1-10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463b2ac1",
   "metadata": {},
   "source": [
    "SH DNN quantile forecast BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f57d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os ;\n",
    "# os.environ['HDF5_DISABLE_VERSION_CHECK']='2'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import datetime as dt \n",
    "from datetime import datetime\n",
    "from datetime import timedelta as td\n",
    "import traceback\n",
    "# from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import concat\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from math import floor\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "#tensorflow &keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "#sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "\n",
    "from hyperopt import hp, fmin, tpe\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "date_format=\"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/coconnor/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat=pd.DataFrame(dat)\n",
    "dat=dat.bfill(axis ='rows')\n",
    "dat=dat.ffill(axis ='rows')\n",
    "dat=dat._get_numeric_data()\n",
    "\n",
    "\n",
    "\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[:7250,:]\n",
    "Y_train=Y.iloc[:7250,:]\n",
    "X_test=X.iloc[7250:8739,:]\n",
    "Y_test=Y.iloc[7250:8739,:]\n",
    "\n",
    "def qloss(qs, y_true, y_pred):\n",
    "    # Pinball loss for multiple quantiles\n",
    "    q = tf.constant(np.array([qs]), dtype=tf.float32)\n",
    "    e = y_true - y_pred\n",
    "    v = tf.maximum(q*e, (q-1)*e)\n",
    "    return K.mean(v)\n",
    "\n",
    "loss_10th_p = lambda y_true, y_pred: qloss(0.1, y_true, y_pred)\n",
    "loss_30th_p = lambda y_true, y_pred: qloss(0.3, y_true, y_pred)\n",
    "loss_50th_p = lambda y_true, y_pred: qloss(0.5, y_true, y_pred)\n",
    "loss_70th_p = lambda y_true, y_pred: qloss(0.7, y_true, y_pred)\n",
    "loss_90th_p = lambda y_true, y_pred: qloss(0.9, y_true, y_pred)\n",
    "     \n",
    "rnn_train1_a=X_train.loc[:,\"lag_-3x1\":\"lag_-18x1\"]\n",
    "rnn_train1_b=X_train.loc[:,\"lag_-19x1\":\"lag_-34x1\"]\n",
    "rnn_train1_c=X_train.loc[:,\"lag_-35x1\":\"lag_-50x1\"]        \n",
    "rnn_train2_a=X_train.loc[:,\"lag_-3x2\":\"lag_-18x2\"]\n",
    "rnn_train2_b=X_train.loc[:,\"lag_-19x2\":\"lag_-34x2\"]\n",
    "rnn_train2_c=X_train.loc[:,\"lag_-35x2\":\"lag_-50x2\"]        \n",
    "rnn_train3_a=X_train.loc[:,\"lag_-2x3\":\"lag_-17x3\"]\n",
    "rnn_train3_b=X_train.loc[:,\"lag_-18x3\":\"lag_-33x3\"]\n",
    "rnn_train3_c=X_train.loc[:,\"lag_-34x3\":\"lag_-49x3\"]      \n",
    "rnn_train4_a=X_train.loc[:,\"lag_0x6\":\"lag_-15x6\"]\n",
    "rnn_train4_b=X_train.loc[:,\"lag_-16x6\":\"lag_-31x6\"]\n",
    "rnn_train4_c=X_train.loc[:,\"lag_-32x6\":\"lag_-47x6\"]        \n",
    "rnn_train5_a=X_train.loc[:,\"lag_-2x12\":\"lag_-17x12\"]\n",
    "rnn_train5_b=X_train.loc[:,\"lag_-18x12\":\"lag_-33x12\"]\n",
    "rnn_train5_c=X_train.loc[:,\"lag_-34x12\":\"lag_-49x12\"]        \n",
    "rnn_train6=X_train.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "rnn_train7=X_train.loc[:,\"lag_2x8\":\"lag_17x8\"]\n",
    "rnn_train8=X_train.loc[:,\"lag_2x9\":\"lag_17x9\"]\n",
    "rnn_train9=X_train.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "rnn_train10=X_train.loc[:,\"lag_2x11\":\"lag_17x11\"]        \n",
    "        \n",
    "rnn_test1_a= X_test.loc[:,\"lag_-3x1\":\"lag_-18x1\"]\n",
    "rnn_test1_b= X_test.loc[:,\"lag_-19x1\":\"lag_-34x1\"]\n",
    "rnn_test1_c= X_test.loc[:,\"lag_-35x1\":\"lag_-50x1\"]\n",
    "rnn_test2_a= X_test.loc[:,\"lag_-3x2\":\"lag_-18x2\"]\n",
    "rnn_test2_b= X_test.loc[:,\"lag_-19x2\":\"lag_-34x2\"]\n",
    "rnn_test2_c= X_test.loc[:,\"lag_-35x2\":\"lag_-50x2\"]\n",
    "rnn_test3_a= X_test.loc[:,\"lag_-2x3\":\"lag_-17x3\"]\n",
    "rnn_test3_b= X_test.loc[:,\"lag_-18x3\":\"lag_-33x3\"]\n",
    "rnn_test3_c= X_test.loc[:,\"lag_-34x3\":\"lag_-49x3\"]\n",
    "rnn_test4_a= X_test.loc[:,\"lag_0x6\":\"lag_-15x6\"]\n",
    "rnn_test4_b= X_test.loc[:,\"lag_-16x6\":\"lag_-31x6\"]\n",
    "rnn_test4_c= X_test.loc[:,\"lag_-32x6\":\"lag_-47x6\"]\n",
    "rnn_test5_a= X_test.loc[:,\"lag_-2x12\":\"lag_-17x12\"]\n",
    "rnn_test5_b= X_test.loc[:,\"lag_-18x12\":\"lag_-33x12\"]\n",
    "rnn_test5_c= X_test.loc[:,\"lag_-34x12\":\"lag_-49x12\"]\n",
    "rnn_test6=X_test.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "rnn_test7=X_test.loc[:,\"lag_2x8\":\"lag_17x8\"]\n",
    "rnn_test8=X_test.loc[:,\"lag_2x9\":\"lag_17x9\"]\n",
    "rnn_test9=X_test.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "rnn_test10=X_test.loc[:,\"lag_2x11\":\"lag_17x11\"]\n",
    "\n",
    "rnn_Y=Y_train.loc[:,\"lag_2y\" : \"lag_17y\"]\n",
    "\n",
    "\n",
    "X_scaler1_a = preprocessing.MinMaxScaler()\n",
    "X_scaler1_b = preprocessing.MinMaxScaler()\n",
    "X_scaler1_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler2_a = preprocessing.MinMaxScaler()\n",
    "X_scaler2_b = preprocessing.MinMaxScaler()\n",
    "X_scaler2_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler3_a = preprocessing.MinMaxScaler()\n",
    "X_scaler3_b = preprocessing.MinMaxScaler()\n",
    "X_scaler3_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler4_a = preprocessing.MinMaxScaler()\n",
    "X_scaler4_b = preprocessing.MinMaxScaler()\n",
    "X_scaler4_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler5_a = preprocessing.MinMaxScaler()\n",
    "X_scaler5_b = preprocessing.MinMaxScaler()\n",
    "X_scaler5_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "\n",
    "X_scaler6 = preprocessing.MinMaxScaler()\n",
    "X_scaler7 = preprocessing.MinMaxScaler()\n",
    "X_scaler8 = preprocessing.MinMaxScaler()\n",
    "X_scaler9 = preprocessing.MinMaxScaler()\n",
    "X_scaler10 = preprocessing.MinMaxScaler()\n",
    "\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train1_a = X_scaler1_a.fit_transform(rnn_train1_a)\n",
    "rnn_scaled_train1_b = X_scaler1_b.fit_transform(rnn_train1_b)\n",
    "rnn_scaled_train1_c = X_scaler1_c.fit_transform(rnn_train1_c)\n",
    "\n",
    "rnn_scaled_train2_a = X_scaler2_a.fit_transform(rnn_train2_a)\n",
    "rnn_scaled_train2_b = X_scaler2_b.fit_transform(rnn_train2_b)\n",
    "rnn_scaled_train2_c = X_scaler2_c.fit_transform(rnn_train2_c)\n",
    "\n",
    "rnn_scaled_train3_a = X_scaler3_a.fit_transform(rnn_train3_a)\n",
    "rnn_scaled_train3_b = X_scaler3_b.fit_transform(rnn_train3_b)\n",
    "rnn_scaled_train3_c = X_scaler3_c.fit_transform(rnn_train3_c)\n",
    "\n",
    "rnn_scaled_train4_a = X_scaler4_a.fit_transform(rnn_train4_a)\n",
    "rnn_scaled_train4_b = X_scaler4_b.fit_transform(rnn_train4_b)\n",
    "rnn_scaled_train4_c = X_scaler4_c.fit_transform(rnn_train4_c)\n",
    "\n",
    "rnn_scaled_train5_a = X_scaler5_a.fit_transform(rnn_train5_a)\n",
    "rnn_scaled_train5_b = X_scaler5_b.fit_transform(rnn_train5_b)\n",
    "rnn_scaled_train5_c = X_scaler5_c.fit_transform(rnn_train5_c)\n",
    "\n",
    "rnn_scaled_train6 = X_scaler6.fit_transform(rnn_train6)\n",
    "rnn_scaled_train7 = X_scaler7.fit_transform(rnn_train7)\n",
    "rnn_scaled_train8 = X_scaler8.fit_transform(rnn_train8)\n",
    "rnn_scaled_train9 = X_scaler9.fit_transform(rnn_train9)\n",
    "rnn_scaled_train10 = X_scaler10.fit_transform(rnn_train10)\n",
    "\n",
    "Y_train_Scaled   = Y_scaler.fit_transform(Y_train)\n",
    "\n",
    "X_train_Scaled = np.hstack(\n",
    "    (rnn_scaled_train1_a, rnn_scaled_train1_b, rnn_scaled_train1_c, rnn_scaled_train2_a, rnn_scaled_train2_b, rnn_scaled_train2_c,\n",
    "     rnn_scaled_train3_a, rnn_scaled_train3_b, rnn_scaled_train3_c, rnn_scaled_train4_a, rnn_scaled_train4_b, rnn_scaled_train4_c,\n",
    "     rnn_scaled_train5_a, rnn_scaled_train5_b, rnn_scaled_train5_c,rnn_scaled_train6,rnn_scaled_train7, rnn_scaled_train8,\n",
    "     rnn_scaled_train9, rnn_scaled_train10)\n",
    ").reshape(rnn_train6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled = np.hstack(\n",
    "    (X_scaler1_a.transform(rnn_test1_a),X_scaler1_b.transform(rnn_test1_b),X_scaler1_c.transform(rnn_test1_c),\n",
    "     X_scaler2_a.transform(rnn_test2_a),X_scaler2_b.transform(rnn_test2_b),X_scaler2_c.transform(rnn_test2_c),\n",
    "     X_scaler3_a.transform(rnn_test3_a),X_scaler3_b.transform(rnn_test3_b),X_scaler3_c.transform(rnn_test3_c),\n",
    "     X_scaler4_a.transform(rnn_test4_a),X_scaler4_b.transform(rnn_test4_b),X_scaler4_c.transform(rnn_test4_c),\n",
    "     X_scaler5_a.transform(rnn_test5_a),X_scaler5_b.transform(rnn_test5_b),X_scaler5_c.transform(rnn_test5_c),\n",
    "     X_scaler6.transform(rnn_test6),X_scaler7.transform(rnn_test7),X_scaler8.transform(rnn_test8),\n",
    "     X_scaler9.transform(rnn_test9), X_scaler10.transform(rnn_test10))\n",
    ").reshape(rnn_test6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "\n",
    "i_shape=(X_train_Scaled.shape[1], X_train_Scaled.shape[2])\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                        test_start_time: dt, test_end_time: dt):\n",
    "\n",
    "    train_X = None\n",
    "    train_y = None \n",
    "    test_X = None \n",
    "    test_y = None \n",
    "    test_df = None\n",
    "    train_df = None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "#             return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        #Remove any rows with nan's etc (there shouldn't be any in the input).        \n",
    "        participant_df = participant_df.dropna()\n",
    "        \n",
    "        date_format=\"%m/%d/%Y %H:%M\"\n",
    "      \n",
    "        train_df = None\n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)\n",
    "        train_df = participant_df[(participant_df.index>=train_start_time_str) & (participant_df.index<train_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")            \n",
    "\n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)    \n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format) \n",
    "        test_df = participant_df[(participant_df.index>=test_start_time_str) & (participant_df.index<test_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")            \n",
    "\n",
    "        \n",
    "        rnn_train1_a=train_df.loc[:,\"lag_-3x1\":\"lag_-18x1\"]\n",
    "        rnn_train1_b=train_df.loc[:,\"lag_-19x1\":\"lag_-34x1\"]\n",
    "        rnn_train1_c=train_df.loc[:,\"lag_-35x1\":\"lag_-50x1\"]\n",
    "        \n",
    "        rnn_train2_a=train_df.loc[:,\"lag_-3x2\":\"lag_-18x2\"]\n",
    "        rnn_train2_b=train_df.loc[:,\"lag_-19x2\":\"lag_-34x2\"]\n",
    "        rnn_train2_c=train_df.loc[:,\"lag_-35x2\":\"lag_-50x2\"]\n",
    "        \n",
    "        rnn_train3_a=train_df.loc[:,\"lag_-2x3\":\"lag_-17x3\"]\n",
    "        rnn_train3_b=train_df.loc[:,\"lag_-18x3\":\"lag_-33x3\"]\n",
    "        rnn_train3_c=train_df.loc[:,\"lag_-34x3\":\"lag_-49x3\"]\n",
    "        \n",
    "        rnn_train4_a=train_df.loc[:,\"lag_0x6\":\"lag_-15x6\"]\n",
    "        rnn_train4_b=train_df.loc[:,\"lag_-16x6\":\"lag_-31x6\"]\n",
    "        rnn_train4_c=train_df.loc[:,\"lag_-32x6\":\"lag_-47x6\"]\n",
    "        \n",
    "        rnn_train5_a=train_df.loc[:,\"lag_-2x12\":\"lag_-17x12\"]\n",
    "        rnn_train5_b=train_df.loc[:,\"lag_-18x12\":\"lag_-33x12\"]\n",
    "        rnn_train5_c=train_df.loc[:,\"lag_-34x12\":\"lag_-49x12\"]\n",
    "        \n",
    "        rnn_train6=train_df.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "        rnn_train7=train_df.loc[:,\"lag_2x8\":\"lag_17x8\"]\n",
    "        rnn_train8=train_df.loc[:,\"lag_2x9\":\"lag_17x9\"]\n",
    "        rnn_train9=train_df.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "        rnn_train10=train_df.loc[:,\"lag_2x11\":\"lag_17x11\"]\n",
    "\n",
    "        rnn_test1_a=test_df.loc[:,\"lag_-3x1\":\"lag_-18x1\"]\n",
    "        rnn_test1_b=test_df.loc[:,\"lag_-19x1\":\"lag_-34x1\"]\n",
    "        rnn_test1_c=test_df.loc[:,\"lag_-35x1\":\"lag_-50x1\"]\n",
    "\n",
    "        rnn_test2_a=test_df.loc[:,\"lag_-3x2\":\"lag_-18x2\"]\n",
    "        rnn_test2_b=test_df.loc[:,\"lag_-19x2\":\"lag_-34x2\"]\n",
    "        rnn_test2_c=test_df.loc[:,\"lag_-35x2\":\"lag_-50x2\"]\n",
    "\n",
    "        rnn_test3_a=test_df.loc[:,\"lag_-2x3\":\"lag_-17x3\"]\n",
    "        rnn_test3_b=test_df.loc[:,\"lag_-18x3\":\"lag_-33x3\"]\n",
    "        rnn_test3_c=test_df.loc[:,\"lag_-34x3\":\"lag_-49x3\"]\n",
    "\n",
    "        rnn_test4_a=test_df.loc[:,\"lag_0x6\":\"lag_-15x6\"]\n",
    "        rnn_test4_b=test_df.loc[:,\"lag_-16x6\":\"lag_-31x6\"]\n",
    "        rnn_test4_c=test_df.loc[:,\"lag_-32x6\":\"lag_-47x6\"]\n",
    "\n",
    "        rnn_test5_a=test_df.loc[:,\"lag_-2x12\":\"lag_-17x12\"]\n",
    "        rnn_test5_b=test_df.loc[:,\"lag_-18x12\":\"lag_-33x12\"]\n",
    "        rnn_test5_c=test_df.loc[:,\"lag_-34x12\":\"lag_-49x12\"]\n",
    "\n",
    "        rnn_test6=test_df.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "        rnn_test7=test_df.loc[:,\"lag_2x8\":\"lag_17x8\"]\n",
    "        rnn_test8=test_df.loc[:,\"lag_2x9\":\"lag_17x9\"]\n",
    "        rnn_test9=test_df.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "        rnn_test10=test_df.loc[:,\"lag_2x11\":\"lag_17x11\"]\n",
    "\n",
    "        rnn_Y=train_df.loc[:,\"lag_2y\" : \"lag_17y\"]\n",
    "\n",
    "        X_scaler1_a = preprocessing.MinMaxScaler()\n",
    "        X_scaler1_b = preprocessing.MinMaxScaler()\n",
    "        X_scaler1_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "        X_scaler2_a = preprocessing.MinMaxScaler()\n",
    "        X_scaler2_b = preprocessing.MinMaxScaler()\n",
    "        X_scaler2_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "        X_scaler3_a = preprocessing.MinMaxScaler()\n",
    "        X_scaler3_b = preprocessing.MinMaxScaler()\n",
    "        X_scaler3_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "        X_scaler4_a = preprocessing.MinMaxScaler()\n",
    "        X_scaler4_b = preprocessing.MinMaxScaler()\n",
    "        X_scaler4_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "        X_scaler5_a = preprocessing.MinMaxScaler()\n",
    "        X_scaler5_b = preprocessing.MinMaxScaler()\n",
    "        X_scaler5_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "\n",
    "        X_scaler6 = preprocessing.MinMaxScaler()\n",
    "        X_scaler7 = preprocessing.MinMaxScaler()\n",
    "        X_scaler8 = preprocessing.MinMaxScaler()\n",
    "        X_scaler9 = preprocessing.MinMaxScaler()\n",
    "        X_scaler10 = preprocessing.MinMaxScaler()\n",
    "\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "        rnn_scaled_train1_a = X_scaler1_a.fit_transform(rnn_train1_a)\n",
    "        rnn_scaled_train1_b = X_scaler1_b.fit_transform(rnn_train1_b)\n",
    "        rnn_scaled_train1_c = X_scaler1_c.fit_transform(rnn_train1_c)\n",
    "\n",
    "        rnn_scaled_train2_a = X_scaler2_a.fit_transform(rnn_train2_a)\n",
    "        rnn_scaled_train2_b = X_scaler2_b.fit_transform(rnn_train2_b)\n",
    "        rnn_scaled_train2_c = X_scaler2_c.fit_transform(rnn_train2_c)\n",
    "\n",
    "        rnn_scaled_train3_a = X_scaler3_a.fit_transform(rnn_train3_a)\n",
    "        rnn_scaled_train3_b = X_scaler3_b.fit_transform(rnn_train3_b)\n",
    "        rnn_scaled_train3_c = X_scaler3_c.fit_transform(rnn_train3_c)\n",
    "\n",
    "        rnn_scaled_train4_a = X_scaler4_a.fit_transform(rnn_train4_a)\n",
    "        rnn_scaled_train4_b = X_scaler4_b.fit_transform(rnn_train4_b)\n",
    "        rnn_scaled_train4_c = X_scaler4_c.fit_transform(rnn_train4_c)\n",
    "\n",
    "        rnn_scaled_train5_a = X_scaler5_a.fit_transform(rnn_train5_a)\n",
    "        rnn_scaled_train5_b = X_scaler5_b.fit_transform(rnn_train5_b)\n",
    "        rnn_scaled_train5_c = X_scaler5_c.fit_transform(rnn_train5_c)\n",
    "\n",
    "        rnn_scaled_train6 = X_scaler6.fit_transform(rnn_train6)\n",
    "        rnn_scaled_train7 = X_scaler7.fit_transform(rnn_train7)\n",
    "        rnn_scaled_train8 = X_scaler8.fit_transform(rnn_train8)\n",
    "        rnn_scaled_train9 = X_scaler9.fit_transform(rnn_train9)\n",
    "        rnn_scaled_train10 = X_scaler10.fit_transform(rnn_train10)\n",
    "\n",
    "        train_y   = Y_scaler.fit_transform(rnn_Y)\n",
    "        Y_scaler_n=Y_scaler.fit(rnn_Y)\n",
    "        train_X = np.hstack(\n",
    "            (rnn_scaled_train1_a, rnn_scaled_train1_b, rnn_scaled_train1_c, rnn_scaled_train2_a, rnn_scaled_train2_b, rnn_scaled_train2_c,\n",
    "             rnn_scaled_train3_a, rnn_scaled_train3_b, rnn_scaled_train3_c, rnn_scaled_train4_a, rnn_scaled_train4_b, rnn_scaled_train4_c,\n",
    "             rnn_scaled_train5_a, rnn_scaled_train5_b, rnn_scaled_train5_c,rnn_scaled_train6,rnn_scaled_train7, rnn_scaled_train8,\n",
    "             rnn_scaled_train9, rnn_scaled_train10)\n",
    "        ).reshape(rnn_train6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "\n",
    "        test_X = np.hstack(\n",
    "            (X_scaler1_a.transform(rnn_test1_a),X_scaler1_b.transform(rnn_test1_b),X_scaler1_c.transform(rnn_test1_c),\n",
    "             X_scaler2_a.transform(rnn_test2_a),X_scaler2_b.transform(rnn_test2_b),X_scaler2_c.transform(rnn_test2_c),\n",
    "             X_scaler3_a.transform(rnn_test3_a),X_scaler3_b.transform(rnn_test3_b),X_scaler3_c.transform(rnn_test3_c),\n",
    "             X_scaler4_a.transform(rnn_test4_a),X_scaler4_b.transform(rnn_test4_b),X_scaler4_c.transform(rnn_test4_c),\n",
    "             X_scaler5_a.transform(rnn_test5_a),X_scaler5_b.transform(rnn_test5_b),X_scaler5_c.transform(rnn_test5_c),\n",
    "             X_scaler6.transform(rnn_test6),X_scaler7.transform(rnn_test7),X_scaler8.transform(rnn_test8),\n",
    "             X_scaler9.transform(rnn_test9), X_scaler10.transform(rnn_test10))\n",
    "        ).reshape(rnn_test6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "        \n",
    "        test_y = test_df.iloc[:, 0:16]\n",
    "                                \n",
    "        return train_X, train_y, test_X, test_y, test_df, train_df, Y_scaler_n\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X, train_y, test_X, test_y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "\n",
    "def fit_multitarget_model(model, X_train, Y_train, X_test, Y_test, actuals_and_forecast_df, targets, Y_scaler_n):\n",
    "  \n",
    "    try:\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "        Y_scaler = Y_scaler.fit(Y_train)\n",
    "        cols = Y.columns.values.tolist()                         \n",
    "\n",
    "        model.fit(X_train, Y_train, validation_split=0.1) \n",
    "        model_test_predictions=None        \n",
    "\n",
    "        model_test_predictions = pd.DataFrame(Y_scaler_n.inverse_transform(model.predict(X_test).reshape(5,16)), columns=cols)\n",
    "        print(model_test_predictions)\n",
    "        print(\"test number of observations: \" + str(len(Y_test)))\n",
    "\n",
    "        \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_10\"] = model_test_predictions.iloc[:1,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_30\"] = model_test_predictions.iloc[1:2,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_50\"] = model_test_predictions.iloc[2:3,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist()\n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_70\"] = model_test_predictions.iloc[3:4,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "           \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_90\"] = model_test_predictions.iloc[4:,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist()\n",
    "            \n",
    "            \n",
    "        return actuals_and_forecast_df\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "  \n",
    "    \n",
    "def rolling_walk_forward_validation(model, data, targets, start_time, end_time, training_days, path):\n",
    " \n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)            \n",
    "        results = pd.DataFrame()\n",
    "            \n",
    "\n",
    "        date_format=\"%m/%d/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "        \n",
    "        while start_time < end_time:\n",
    "            \n",
    "            #Train interval\n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time\n",
    "    \n",
    "            #Test interval, the test period is always the day ahead forecast\n",
    "            test_start_time = train_end_time + td(hours=8)\n",
    "            test_end_time = test_start_time + td(minutes=30)\n",
    "            \n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "    \n",
    "            #Generate the calibration and test dataframes.\n",
    "            train_X, train_y, test_X, test_y, test_df, train_df, Y_scaler_n= generate_train_and_test_dataframes(participant_df=dat, train_start_time=train_start_time.strftime(\"%m/%d/%Y %H:%M\"), train_end_time=train_end_time.strftime(\"%m/%d/%Y %H:%M\"), \n",
    "                            test_start_time=test_start_time.strftime(\"%m/%d/%Y %H:%M\"), test_end_time=test_end_time.strftime(\"%m/%d/%Y %H:%M\"))\n",
    "            \n",
    "            \n",
    "            if train_X is None or len(train_X) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "    \n",
    "            if test_X is None or len(test_X) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "            \n",
    "            #Fit the model to the train datasets, produce a forecast and return a dataframe containing the forecast/actuals.\n",
    "            actuals_and_forecast_df = fit_multitarget_model(model=model,Y_scaler_n=Y_scaler_n, X_train=train_X, Y_train=train_y,\n",
    "                                            X_test=test_X, Y_test=test_y, actuals_and_forecast_df=test_df.iloc[:,0:16], targets=Y.columns.values.tolist())\n",
    "\n",
    "    \n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "        \n",
    "            start_time = start_time + td(hours=8)\n",
    "            \n",
    "        results.to_csv(path  + \".csv\", index = False)\n",
    "        \n",
    "\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "\n",
    "def create_model():    \n",
    "    net_input=Input(shape=i_shape)    \n",
    "    \n",
    "    x = Flatten(input_shape=i_shape)(net_input)\n",
    "    \n",
    "    for i in range(2):\n",
    "        x = Dense(192, 'sigmoid')(x)\n",
    "    \n",
    "    for i in range(3):\n",
    "        x = Dense(128, 'relu')(x)\n",
    "    x = Dropout(0.400000)(x)\n",
    "    \n",
    "    a = Dense(64, 'sigmoid')(x)\n",
    "    b = Dense(64, 'sigmoid')(x)\n",
    "    c = Dense(64, 'sigmoid')(x)\n",
    "    d = Dense(64, 'sigmoid')(x)\n",
    "    e = Dense(64, 'sigmoid')(x)  \n",
    "    \n",
    "    \n",
    "    for i in range(1):\n",
    "            a = Dense(64, 'relu')(a) \n",
    "    output_1 =  Dense(16, name='out_10')(a)\n",
    "    \n",
    "    for i in range(1):\n",
    "            b = Dense(64, 'relu')(b) \n",
    "    output_2 =  Dense(16, name='out_30')(b)\n",
    "    \n",
    "    for i in range(1):\n",
    "            c = Dense(64, 'relu')(c) \n",
    "    output_3 =  Dense(16, name='out_50')(c)\n",
    "    \n",
    "    for i in range(1):\n",
    "            d = Dense(64, 'relu')(d) \n",
    "    output_4 =  Dense(16, name='out_70')(d)\n",
    "    \n",
    "    for i in range(1):\n",
    "            e = Dense(64, 'relu')(e) \n",
    "    output_5 =  Dense(16, name='out_90')(e)    \n",
    "    \n",
    "    opt = Adam(learning_rate=0.000100)\n",
    "    model=Model(inputs=net_input, outputs=[output_1, output_2, output_3, output_4, output_5])    \n",
    "    model.compile(loss=[loss_10th_p, loss_30th_p, loss_50th_p, loss_70th_p, loss_90th_p], optimizer=opt)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=40)\n",
    "mmo = KerasRegressor(build_fn=create_model, epochs=200, batch_size=16, verbose=2, callbacks=[es])\n",
    "\n",
    "\n",
    "rolling_walk_forward_validation(model=mmo, data=dat, start_time='6/1/2020 00:00', end_time='9/1/2020  00:00',       \n",
    "                                targets=Y.columns.values.tolist(), training_days=-210,\n",
    "                                path=\"/home/coconnor/SH_Q_1-3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d36fc6",
   "metadata": {},
   "source": [
    "light gradient boosting machine for quantile forecast in the BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26501288",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "from datetime import datetime\n",
    "# from pandas import Timedelta as td\n",
    "import traceback\n",
    "# from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "date_format=\"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/coconnor/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat=pd.DataFrame(dat)\n",
    "dat=dat.bfill(axis ='rows')\n",
    "dat=dat.ffill(axis ='rows')\n",
    "dat=dat._get_numeric_data()\n",
    "\n",
    "Y=dat.iloc[:, 0:16]\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                        test_start_time: dt, test_end_time: dt):\n",
    "\n",
    "    train_X = None\n",
    "    train_y = None \n",
    "    test_X = None \n",
    "    test_y = None \n",
    "    test_df = None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        participant_df = participant_df.dropna()\n",
    "\n",
    "        date_format=\"%m/%d/%Y %H:%M\"\n",
    "\n",
    "        train_df = None\n",
    "        \n",
    "\n",
    "        \n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)    \n",
    "        train_df = participant_df[(participant_df.index>=train_start_time_str) & (participant_df.index<train_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")            \n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "\n",
    "        \n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)    \n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format) \n",
    "        test_df = participant_df[(participant_df.index>=test_start_time_str) & (participant_df.index<test_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")            \n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "\n",
    "        train_X = train_df.iloc[:, 16:]\n",
    "        test_X = test_df.iloc[:, 16:]\n",
    "        train_y = train_df.iloc[:, 0:16]\n",
    "        test_y = test_df.iloc[:, 0:16]\n",
    "                                \n",
    "        return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X, train_y, test_X, test_y, test_df\n",
    "    \n",
    "    \n",
    "def fit_multitarget_model(model_1, model_2, model_3, model_4, model_5, X_train, Y_train, X_test, Y_test, actuals_and_forecast_df, targets):\n",
    "\n",
    "    try:\n",
    "        \n",
    "#         model.fit(X_train, Y_train) if len(targets) > 1 else model.fit(X_train, Y_train.values.ravel())  \n",
    "        model_1.fit(X_train, Y_train) \n",
    "        model_2.fit(X_train, Y_train) \n",
    "        model_3.fit(X_train, Y_train) \n",
    "        model_4.fit(X_train, Y_train) \n",
    "        model_5.fit(X_train, Y_train) \n",
    "\n",
    "\n",
    "        model_test_predictions_1=None  \n",
    "        model_test_predictions_3=None  \n",
    "        model_test_predictions_5=None  \n",
    "        model_test_predictions_7=None  \n",
    "        model_test_predictions_9=None  \n",
    "        model_test_predictions_1 = model_1.predict(X_test)     \n",
    "        model_test_predictions_3 = model_2.predict(X_test) \n",
    "        model_test_predictions_5 = model_3.predict(X_test)     \n",
    "        model_test_predictions_7 = model_4.predict(X_test)     \n",
    "        model_test_predictions_9 = model_5.predict(X_test)          \n",
    "                    \n",
    "        cols = Y_train.columns.values.tolist()   \n",
    "        \n",
    "        \n",
    "        print(\"train number of observations: \" + str(len(Y_train)))\n",
    "\n",
    "        \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_10\"] = model_test_predictions_1[:,i].tolist() if len(cols) > 1 else model_test_predictions_1.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_30\"] = model_test_predictions_3[:,i].tolist() if len(cols) > 1 else model_test_predictions_3.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_50\"] = model_test_predictions_5[:,i].tolist() if len(cols) > 1 else model_test_predictions_5.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_70\"] = model_test_predictions_7[:,i].tolist() if len(cols) > 1 else model_test_predictions_7.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_90\"] = model_test_predictions_9[:,i].tolist() if len(cols) > 1 else model_test_predictions_9.tolist() \n",
    "          \n",
    "           \n",
    "        return actuals_and_forecast_df\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "def rolling_walk_forward_validation(model_1, model_2, model_3, model_4, model_5, data, targets, start_time, end_time, training_days, path):\n",
    "\n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)            \n",
    "        results = pd.DataFrame()\n",
    "            \n",
    "        #Each time we \n",
    "        # (a) fit the model on the calibration/train data\n",
    "        # (b) apply it to the test data i.e. forecast 1 day ahead.\n",
    "        #Repeat.\n",
    "        date_format=\"%m/%d/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "        \n",
    "        while start_time < end_time:\n",
    "            \n",
    "            #Train interval\n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time \n",
    "    \n",
    "            #Test interval, the test period is always the day ahead forecast\n",
    "            test_start_time = train_end_time + td(hours=8)\n",
    "            test_end_time = test_start_time + td(minutes=30)\n",
    "            \n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "    \n",
    "            #Generate the calibration and test dataframes.\n",
    "            train_X, train_y, test_X, test_y, test_df = generate_train_and_test_dataframes(participant_df=dat, train_start_time=train_start_time.strftime(\"%m/%d/%Y %H:%M\"), train_end_time=train_end_time.strftime(\"%m/%d/%Y %H:%M\"), \n",
    "                            test_start_time=test_start_time.strftime(\"%m/%d/%Y %H:%M\"), test_end_time=test_end_time.strftime(\"%m/%d/%Y %H:%M\"))\n",
    "            \n",
    "            if train_X is None or len(train_X) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "    \n",
    "            if test_X is None or len(test_X) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "            \n",
    "            actuals_and_forecast_df = fit_multitarget_model(model_1=model_1,model_2=model_2,model_3=model_3, model_4=model_4,model_5=model_5, X_train=train_X, Y_train=train_y,\n",
    "                                            X_test=test_X, Y_test=test_y, actuals_and_forecast_df=test_df,targets=Y.columns.values.tolist())\n",
    "    \n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "            print(results)\n",
    "            start_time = start_time + td(hours=8)\n",
    "            \n",
    "        results.to_csv(path  + \".csv\", index = False)\n",
    "        \n",
    "          \n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "\n",
    "\n",
    "rolling_walk_forward_validation(model_1=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.1, learning_rate = 0.05, num_leaves=10,  max_depth = 4, n_estimators =  100)),\n",
    "                                model_2=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.3, learning_rate = 0.05, num_leaves=10,  max_depth = 4, n_estimators =  100)),\n",
    "                                model_3=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.5, learning_rate = 0.05, num_leaves=10,  max_depth = 4, n_estimators =  100)),\n",
    "                                model_4=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.7, learning_rate = 0.05, num_leaves=10,  max_depth = 4, n_estimators =  100)),\n",
    "                                model_5=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.9, learning_rate = 0.05, num_leaves=10,  max_depth = 4, n_estimators =  100)),\n",
    "                                data=dat, start_time='06/1/2020 00:00',end_time='09/1/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(),training_days=-210, path=\"/home/coconnor/lgbm_Q_1-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230bf3b4",
   "metadata": {},
   "source": [
    "Single headed Deep neural network for quantile forecast in the DAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e25ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os;\n",
    "# os.environ['HDF5_DISABLE_VERSION_CHECK']='2'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from datetime import timedelta as td\n",
    "import traceback\n",
    "# from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import concat\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from math import floor\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "# tensorflow &keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "date_format = \"%d/%m/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/coconnor/DAM_VAR_1-3.csv\", index_col=\"DeliveryPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "\n",
    "Y = dat.iloc[:, 0:24]\n",
    "X = dat.iloc[:, 24:]\n",
    "X_train = X.iloc[:12360, :]\n",
    "Y_train = Y.iloc[:12360, :]\n",
    "X_test = X.iloc[12360:13104, :]\n",
    "Y_test = Y.iloc[12360:13104, :]\n",
    "\n",
    "\n",
    "def qloss(qs, y_true, y_pred):\n",
    "    # Pinball loss for multiple quantiles\n",
    "    q = tf.constant(np.array([qs]), dtype=tf.float32)\n",
    "    e = y_true - y_pred\n",
    "    v = tf.maximum(q * e, (q - 1) * e)\n",
    "    return K.mean(v)\n",
    "\n",
    "\n",
    "loss_10th_p = lambda y_true, y_pred: qloss(0.1, y_true, y_pred)\n",
    "loss_30th_p = lambda y_true, y_pred: qloss(0.3, y_true, y_pred)\n",
    "loss_50th_p = lambda y_true, y_pred: qloss(0.5, y_true, y_pred)\n",
    "loss_70th_p = lambda y_true, y_pred: qloss(0.7, y_true, y_pred)\n",
    "loss_90th_p = lambda y_true, y_pred: qloss(0.9, y_true, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "rnn_train_1 = X_train.loc[:, \"EURPrices-24\":\"EURPrices-167\"]\n",
    "rnn_train_2 = X_train.loc[:, \"WF\": \"WF-143\"]\n",
    "rnn_train_3 = X_train.loc[:, \"DF\": \"DF-143\"]\n",
    "\n",
    "rnn_test_1 = X_test.loc[:, \"EURPrices-24\":\"EURPrices-167\"]\n",
    "rnn_test_2 = X_test.loc[:, \"WF\": \"WF-143\"]\n",
    "rnn_test_3 = X_test.loc[:, \"DF\": \"DF-143\"]\n",
    "\n",
    "rnn_Y = Y_train.loc[:, \"EURPrices\":\"EURPrices+23\"]\n",
    "rnn_test_Y = Y_test.loc[:, \"EURPrices\":\"EURPrices+23\"]\n",
    "\n",
    "X_scaler1 = preprocessing.MinMaxScaler()\n",
    "X_scaler2 = preprocessing.MinMaxScaler()\n",
    "X_scaler3 = preprocessing.MinMaxScaler()\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train_1 = X_scaler1.fit_transform(rnn_train_1)\n",
    "rnn_scaled_train_2 = X_scaler2.fit_transform(rnn_train_2)\n",
    "rnn_scaled_train_3 = X_scaler3.fit_transform(rnn_train_3)\n",
    "\n",
    "Y_train_Scaled = Y_scaler.fit_transform(rnn_Y)\n",
    "Y_test_scaled = Y_scaler.transform(rnn_test_Y)\n",
    "\n",
    "X_train_Scaled = np.hstack(\n",
    "    (rnn_scaled_train_1, rnn_scaled_train_2, rnn_scaled_train_3)\n",
    ").reshape(rnn_train_1.shape[0], 3, 144).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled = np.hstack(\n",
    "    (X_scaler1.transform(rnn_test_1), X_scaler2.transform(rnn_test_2), X_scaler3.transform(rnn_test_3))\n",
    ").reshape(rnn_test_1.shape[0], 3, 144).transpose(0, 2, 1)\n",
    "\n",
    "i_shape = (X_train_Scaled.shape[1], X_train_Scaled.shape[2])\n",
    "\n",
    "\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                                       test_start_time: dt, test_end_time: dt):\n",
    "\n",
    "\n",
    "    # These are the dataframes that will be returned from the method.\n",
    "    train_X = None\n",
    "    train_y = None\n",
    "    test_X = None\n",
    "    test_y = None\n",
    "    test_df = None\n",
    "    train_df = None\n",
    "\n",
    "    try:\n",
    "\n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "        #             return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        # Remove any rows with nan's etc (there shouldn't be any in the input).\n",
    "        participant_df = participant_df.dropna()\n",
    "\n",
    "        date_format = \"%d/%m/%Y %H:%M\"\n",
    "\n",
    "        # The train dataframe, it will be used later to create train_X and train_y.\n",
    "        train_df = None\n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)\n",
    "        train_df = participant_df[\n",
    "            (participant_df.index >= train_start_time_str) & (participant_df.index < train_end_time_str)].copy(\n",
    "            deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\n",
    "                \"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")\n",
    "        #             return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        # Create the test dataframe, it will be used later to create test_X and test_y\n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)\n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format)\n",
    "        test_df = participant_df[\n",
    "            (participant_df.index >= test_start_time_str) & (participant_df.index < test_end_time_str)].copy(\n",
    "            deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\n",
    "                \"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")\n",
    "        #             return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        rnn_train_1 = train_df.loc[:, \"EURPrices-24\":\"EURPrices-167\"]\n",
    "        rnn_train_2 = train_df.loc[:, \"WF\": \"WF-143\"]\n",
    "        rnn_train_3 = train_df.loc[:, \"DF\": \"DF-143\"]\n",
    "\n",
    "        rnn_test_1 = test_df.loc[:, \"EURPrices-24\":\"EURPrices-167\"]\n",
    "        rnn_test_2 = test_df.loc[:, \"WF\": \"WF-143\"]\n",
    "        rnn_test_3 = test_df.loc[:, \"DF\": \"DF-143\"]\n",
    "\n",
    "        rnn_Y = train_df.loc[:, \"EURPrices\":\"EURPrices+23\"]\n",
    "        rnn_test_Y = test_df.loc[:, \"EURPrices\":\"EURPrices+23\"]\n",
    "\n",
    "        X_scaler1 = preprocessing.MinMaxScaler()\n",
    "        X_scaler2 = preprocessing.MinMaxScaler()\n",
    "        X_scaler3 = preprocessing.MinMaxScaler()\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "        rnn_scaled_train_1 = X_scaler1.fit_transform(rnn_train_1)\n",
    "        rnn_scaled_train_2 = X_scaler2.fit_transform(rnn_train_2)\n",
    "        rnn_scaled_train_3 = X_scaler3.fit_transform(rnn_train_3)\n",
    "\n",
    "        train_y = Y_scaler.fit_transform(rnn_Y)\n",
    "        Y_scaler_n = Y_scaler.fit(rnn_Y)\n",
    "\n",
    "        Y_test_scaled = Y_scaler.transform(rnn_test_Y)\n",
    "\n",
    "        train_X = np.hstack(\n",
    "            (rnn_scaled_train_1, rnn_scaled_train_2, rnn_scaled_train_3)\n",
    "        ).reshape(rnn_train_1.shape[0], 3, 144).transpose(0, 2, 1)\n",
    "\n",
    "        test_X = np.hstack(\n",
    "            (X_scaler1.transform(rnn_test_1), X_scaler2.transform(rnn_test_2), X_scaler3.transform(rnn_test_3))\n",
    "        ).reshape(rnn_test_1.shape[0], 3, 144).transpose(0, 2, 1)\n",
    "\n",
    "        test_y = test_df.iloc[:, 0:24]\n",
    "\n",
    "        return train_X, train_y, test_X, test_y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X, train_y, test_X, test_y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "\n",
    "def fit_multitarget_model(model, X_train, Y_train, X_test, Y_test, actuals_and_forecast_df, targets, Y_scaler_n):\n",
    "    try:\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "        Y_scaler = Y_scaler.fit(Y_train)\n",
    "        cols = Y.columns.values.tolist()\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min',  patience=20)\n",
    "\n",
    "        model.fit(X_train, Y_train, epochs=200, verbose=2,  callbacks=[es], validation_split=0.1)\n",
    "        model_test_predictions=None\n",
    "        model_test_predictions = pd.DataFrame(Y_scaler_n.inverse_transform(np.array(model.predict(X_test).reshape(5,24))), columns=cols)\n",
    "\n",
    "        print(model_test_predictions)\n",
    "        print(\"test number of observations: \" + str(len(Y_test)))\n",
    "\n",
    "        for i in range(0, len(cols)):\n",
    "            actuals_and_forecast_df[cols[i] + \"_Forecast_10\"] = model_test_predictions.iloc[:1, i].T.tolist() if len(\n",
    "                cols) > 1 else model_test_predictions.tolist()\n",
    "\n",
    "        for i in range(0, len(cols)):\n",
    "            actuals_and_forecast_df[cols[i] + \"_Forecast_30\"] = model_test_predictions.iloc[1:2, i].T.tolist() if len(\n",
    "                cols) > 1 else model_test_predictions.tolist()\n",
    "\n",
    "        for i in range(0, len(cols)):\n",
    "            actuals_and_forecast_df[cols[i] + \"_Forecast_50\"] = model_test_predictions.iloc[2:3, i].T.tolist() if len(\n",
    "                cols) > 1 else model_test_predictions.tolist()\n",
    "\n",
    "        for i in range(0, len(cols)):\n",
    "            actuals_and_forecast_df[cols[i] + \"_Forecast_70\"] = model_test_predictions.iloc[3:4, i].T.tolist() if len(\n",
    "                cols) > 1 else model_test_predictions.tolist()\n",
    "\n",
    "        for i in range(0, len(cols)):\n",
    "            actuals_and_forecast_df[cols[i] + \"_Forecast_90\"] = model_test_predictions.iloc[4:, i].T.tolist() if len(\n",
    "                cols) > 1 else model_test_predictions.tolist()\n",
    "\n",
    "        return actuals_and_forecast_df\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def rolling_walk_forward_validation(model, data, targets, start_time, end_time, training_days, path):\n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)\n",
    "        results = pd.DataFrame()\n",
    "\n",
    "        date_format = \"%d/%m/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "\n",
    "        while start_time < end_time:\n",
    "\n",
    "            # Train interval\n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time\n",
    "\n",
    "            # Test interval, the test period is always the day ahead forecast\n",
    "            test_start_time = train_end_time + td(hours=0)\n",
    "            test_end_time = test_start_time + td(hours=1)\n",
    "\n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "\n",
    "            # Generate the calibration and test dataframes.\n",
    "            train_X, train_y, test_X, test_y, test_df, train_df, Y_scaler_n = generate_train_and_test_dataframes(\n",
    "                participant_df=dat, train_start_time=train_start_time.strftime(\"%d/%m/%Y %H:%M\"),\n",
    "                train_end_time=train_end_time.strftime(\"%d/%m/%Y %H:%M\"),\n",
    "                test_start_time=test_start_time.strftime(\"%d/%m/%Y %H:%M\"),\n",
    "                test_end_time=test_end_time.strftime(\"%d/%m/%Y %H:%M\"))\n",
    "\n",
    "            if train_X is None or len(train_X) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(\n",
    "                    train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            if test_X is None or len(test_X) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(\n",
    "                    test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            # Fit the model to the train datasets, produce a forecast and return a dataframe containing the forecast/actuals.\n",
    "            actuals_and_forecast_df = fit_multitarget_model(model=model, Y_scaler_n=Y_scaler_n, X_train=train_X,\n",
    "                                                            Y_train=train_y,\n",
    "                                                            X_test=test_X, Y_test=test_y,\n",
    "                                                            actuals_and_forecast_df=test_df.iloc[:, 0:24],\n",
    "                                                            targets=Y.columns.values.tolist())\n",
    "\n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "            start_time = start_time + td(hours=24)\n",
    "\n",
    "        results.to_csv(path + \".csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    net_input = Input(shape=i_shape)\n",
    "\n",
    "    x = Flatten(input_shape=i_shape)(net_input)\n",
    "\n",
    "    for i in range(3):\n",
    "        x = Dense(144, 'sigmoid')(x)\n",
    "\n",
    "    for i in range(3):\n",
    "        x = Dense(240, 'tanh')(x)\n",
    "    x = Dropout(0.311111)(x)\n",
    "\n",
    "    a = Dense(24, 'tanh')(x)\n",
    "    b = Dense(24, 'tanh')(x)\n",
    "    c = Dense(24, 'tanh')(x)\n",
    "    d = Dense(24, 'tanh')(x)\n",
    "    e = Dense(24, 'tanh')(x)\n",
    "\n",
    "    for i in range(1):\n",
    "        a = Dense(48, 'relu')(a)\n",
    "    output_1 = Dense(24, name='out_10')(a)\n",
    "\n",
    "    for i in range(1):\n",
    "        b = Dense(48, 'relu')(b)\n",
    "    output_2 = Dense(24, name='out_30')(b)\n",
    "\n",
    "    for i in range(1):\n",
    "        c = Dense(48, 'relu')(c)\n",
    "    output_3 = Dense(24, name='out_50')(c)\n",
    "\n",
    "    for i in range(1):\n",
    "        d = Dense(48, 'relu')(d)\n",
    "    output_4 = Dense(24, name='out_70')(d)\n",
    "\n",
    "    for i in range(1):\n",
    "        e = Dense(48, 'relu')(e)\n",
    "    output_5 = Dense(24, name='out_90')(e)\n",
    "\n",
    "    opt = Adam(learning_rate=0.000100)\n",
    "    model = Model(inputs=net_input, outputs=[output_1, output_2, output_3, output_4, output_5])\n",
    "    model.compile(loss=[loss_10th_p, loss_30th_p, loss_50th_p, loss_70th_p, loss_90th_p], optimizer=opt)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "mmo = KerasRegressor(build_fn=create_model, epochs=200, batch_size=16, verbose=2)\n",
    "\n",
    "rolling_walk_forward_validation(model=mmo, data=dat, start_time='1/6/2020 00:00', end_time='1/9/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(), training_days=-210,\n",
    "                                path=\"/home/coconnor/SH_Q_DAM_1-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1ff09e",
   "metadata": {},
   "source": [
    "light gradient boosting library for quantile forecast in the DAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8713fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "from datetime import datetime\n",
    "# from pandas import Timedelta as td\n",
    "import traceback\n",
    "# from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "date_format = \"%d/%m/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/coconnor/DAM_VAR_1-3.csv\", index_col=\"DeliveryPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "\n",
    "Y = dat.iloc[:, 0:24]\n",
    "\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                        test_start_time: dt, test_end_time: dt):\n",
    "\n",
    "    train_X = None\n",
    "    train_y = None \n",
    "    test_X = None \n",
    "    test_y = None \n",
    "    test_df = None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        participant_df = participant_df.dropna()\n",
    "\n",
    "        date_format = \"%d/%m/%Y %H:%M\"\n",
    "\n",
    "        train_df = None\n",
    "        \n",
    "\n",
    "        \n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)    \n",
    "        train_df = participant_df[(participant_df.index>=train_start_time_str) & (participant_df.index<train_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")            \n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "\n",
    "        \n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)    \n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format) \n",
    "        test_df = participant_df[(participant_df.index>=test_start_time_str) & (participant_df.index<test_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")            \n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "\n",
    "        train_X = train_df.iloc[:, 24:]\n",
    "        test_X = test_df.iloc[:, 24:]\n",
    "        train_y = train_df.iloc[:, 0:24]\n",
    "        test_y = test_df.iloc[:, 0:24]\n",
    "                                \n",
    "        return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X, train_y, test_X, test_y, test_df\n",
    "    \n",
    "    \n",
    "def fit_multitarget_model(model_1, model_2, model_3, model_4, model_5, X_train, Y_train, X_test, Y_test, actuals_and_forecast_df, targets):\n",
    "\n",
    "    try:\n",
    "        \n",
    "#         model.fit(X_train, Y_train) if len(targets) > 1 else model.fit(X_train, Y_train.values.ravel())  \n",
    "        model_1.fit(X_train, Y_train) \n",
    "        model_2.fit(X_train, Y_train) \n",
    "        model_3.fit(X_train, Y_train) \n",
    "        model_4.fit(X_train, Y_train) \n",
    "        model_5.fit(X_train, Y_train) \n",
    "\n",
    "\n",
    "        model_test_predictions_1=None  \n",
    "        model_test_predictions_3=None  \n",
    "        model_test_predictions_5=None  \n",
    "        model_test_predictions_7=None  \n",
    "        model_test_predictions_9=None  \n",
    "        model_test_predictions_1 = model_1.predict(X_test)     \n",
    "        model_test_predictions_3 = model_2.predict(X_test) \n",
    "        model_test_predictions_5 = model_3.predict(X_test)     \n",
    "        model_test_predictions_7 = model_4.predict(X_test)     \n",
    "        model_test_predictions_9 = model_5.predict(X_test)          \n",
    "                    \n",
    "        cols = Y_train.columns.values.tolist()   \n",
    "        \n",
    "        \n",
    "        print(\"train number of observations: \" + str(len(Y_train)))\n",
    "\n",
    "        \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_10\"] = model_test_predictions_1[:,i].tolist() if len(cols) > 1 else model_test_predictions_1.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_30\"] = model_test_predictions_3[:,i].tolist() if len(cols) > 1 else model_test_predictions_3.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_50\"] = model_test_predictions_5[:,i].tolist() if len(cols) > 1 else model_test_predictions_5.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_70\"] = model_test_predictions_7[:,i].tolist() if len(cols) > 1 else model_test_predictions_7.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_90\"] = model_test_predictions_9[:,i].tolist() if len(cols) > 1 else model_test_predictions_9.tolist() \n",
    "          \n",
    "           \n",
    "        return actuals_and_forecast_df\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "def rolling_walk_forward_validation(model_1, model_2, model_3, model_4, model_5, data, targets, start_time, end_time, training_days, path):\n",
    "\n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)            \n",
    "        results = pd.DataFrame()\n",
    "            \n",
    "\n",
    "        date_format = \"%d/%m/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "        \n",
    "        while start_time < end_time:\n",
    "            \n",
    "            #Train interval\n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time \n",
    "    \n",
    "            #Test interval, the test period is always the day ahead forecast\n",
    "            test_start_time = train_end_time + td(hours=0)\n",
    "            test_end_time = test_start_time + td(hours=1)\n",
    "            \n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "    \n",
    "            #Generate the calibration and test dataframes.\n",
    "            train_X, train_y, test_X, test_y, test_df = generate_train_and_test_dataframes(participant_df=dat, train_start_time=train_start_time.strftime(\"%d/%m/%Y %H:%M\"), train_end_time=train_end_time.strftime(\"%d/%m/%Y %H:%M\"), \n",
    "                            test_start_time=test_start_time.strftime(\"%d/%m/%Y %H:%M\"), test_end_time=test_end_time.strftime(\"%d/%m/%Y %H:%M\"))\n",
    "            \n",
    "            if train_X is None or len(train_X) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "    \n",
    "            if test_X is None or len(test_X) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "            \n",
    "            actuals_and_forecast_df = fit_multitarget_model(model_1=model_1,model_2=model_2,model_3=model_3, model_4=model_4,model_5=model_5, X_train=train_X, Y_train=train_y,\n",
    "                                            X_test=test_X, Y_test=test_y, actuals_and_forecast_df=test_df,targets=Y.columns.values.tolist())\n",
    "    \n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "            print(results)\n",
    "            start_time = start_time + td(hours=24)\n",
    "            \n",
    "        results.to_csv(path  + \".csv\", index = False)\n",
    "        \n",
    "          \n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "\n",
    "\n",
    "rolling_walk_forward_validation(model_1=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.1, learning_rate = 0.01, num_leaves=20,  max_depth = 18, n_estimators =  60000)),\n",
    "                                model_2=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.3, learning_rate = 0.01, num_leaves=20,  max_depth = 18, n_estimators =  60000)),\n",
    "                                model_3=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.5, learning_rate = 0.01, num_leaves=20,  max_depth = 18, n_estimators =  60000)),\n",
    "                                model_4=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.7, learning_rate = 0.01, num_leaves=20,  max_depth = 18, n_estimators =  60000)),\n",
    "                                model_5=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.9, learning_rate = 0.01, num_leaves=20,  max_depth = 18, n_estimators =  60000)),\n",
    "                                data=dat, start_time='1/6/2020 00:00', end_time='1/9/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(),training_days=-210, path=\"/home/coconnor/lgbm_Q_DAM_1-3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d08def",
   "metadata": {},
   "source": [
    "random forest & KNN library for quantile forecast in the DAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "from datetime import datetime\n",
    "# from pandas import Timedelta as td\n",
    "import traceback\n",
    "# from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn_quantile import RandomForestQuantileRegressor\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "date_format = \"%d/%m/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/coconnor/DAM_VAR_1-3.csv\", index_col=\"DeliveryPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "\n",
    "Y = dat.iloc[:, 0:24]\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                        test_start_time: dt, test_end_time: dt):\n",
    "\n",
    "    train_X = None\n",
    "    train_y = None \n",
    "    test_X = None \n",
    "    test_y = None \n",
    "    test_df = None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        participant_df = participant_df.dropna()\n",
    "\n",
    "        date_format = \"%d/%m/%Y %H:%M\"\n",
    "\n",
    "        train_df = None\n",
    "        \n",
    "\n",
    "        \n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)    \n",
    "        train_df = participant_df[(participant_df.index>=train_start_time_str) & (participant_df.index<train_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")            \n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "\n",
    "        \n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)    \n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format) \n",
    "        test_df = participant_df[(participant_df.index>=test_start_time_str) & (participant_df.index<test_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")            \n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "\n",
    "        train_X = train_df.iloc[:, 24:]\n",
    "        test_X = test_df.iloc[:, 24:]\n",
    "        train_y = train_df.iloc[:, 0:24]\n",
    "        test_y = test_df.iloc[:, 0:24]\n",
    "                                \n",
    "        return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X, train_y, test_X, test_y, test_df\n",
    "    \n",
    "    \n",
    "def fit_multitarget_model(model, X_train, Y_train, X_test, Y_test, actuals_and_forecast_df, targets):\n",
    "  \n",
    "    try:\n",
    "        \n",
    "        model.fit(X_train, Y_train) \n",
    "\n",
    "        model_test_predictions=None  \n",
    "        model_test_predictions = model.predict(X_test).reshape(5,24)                            \n",
    "        cols = Y_train.columns.values.tolist()   \n",
    "        \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_10\"] = model_test_predictions[0:1,i].tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_30\"] = model_test_predictions[1:2,i].tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_50\"] = model_test_predictions[2:3,i].tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_70\"] = model_test_predictions[3:4,i].tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_90\"] = model_test_predictions[4:,i].tolist() if len(cols) > 1 else model_test_predictions.tolist()\n",
    "            \n",
    "        print(\"train number of observations: \" + str(len(Y_train)))\n",
    "        \n",
    "        \n",
    "        \n",
    "           \n",
    "        return actuals_and_forecast_df\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "def rolling_walk_forward_validation(model, data, targets, start_time, end_time, training_days, path):\n",
    " \n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)            \n",
    "        results = pd.DataFrame()\n",
    "            \n",
    "\n",
    "        date_format=\"%d/%m/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "        \n",
    "        while start_time < end_time:\n",
    "            \n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time \n",
    "    \n",
    "            test_start_time = train_end_time + td(hours=0)\n",
    "            test_end_time = test_start_time + td(hours=1)\n",
    "            \n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "    \n",
    "            train_X, train_y, test_X, test_y, test_df = generate_train_and_test_dataframes(participant_df=dat, train_start_time=train_start_time.strftime(\"%d/%m/%Y %H:%M\"), train_end_time=train_end_time.strftime(\"%d/%m/%Y %H:%M\"), \n",
    "                            test_start_time=test_start_time.strftime(\"%d/%m/%Y %H:%M\"), test_end_time=test_end_time.strftime(\"%d/%m/%Y %H:%M\"))\n",
    "            \n",
    "            if train_X is None or len(train_X) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "    \n",
    "            if test_X is None or len(test_X) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "            \n",
    "            actuals_and_forecast_df = fit_multitarget_model(model=model, X_train=train_X, Y_train=train_y,\n",
    "                                            X_test=test_X, Y_test=test_y, actuals_and_forecast_df=test_df,targets=Y.columns.values.tolist())\n",
    "    \n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "            print(results)\n",
    "            start_time = start_time + td(hours=24)\n",
    "            \n",
    "        results.to_csv(path  + \".csv\", index = False)\n",
    "        \n",
    "          \n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "rolling_walk_forward_validation(model = MultiOutputRegressor(RandomForestQuantileRegressor(q=[0.10, 0.30, 0.50, 0.70, 0.90], max_depth=2 , min_samples_leaf=2 , n_estimators=100 , min_samples_split=2 )),\n",
    "                                data=dat, start_time='1/6/2020 00:00', end_time='1/9/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(),training_days=-210, path=\"/home/coconnor/rf_Q_DAM_1-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c722678c",
   "metadata": {},
   "source": [
    "Lear library for regular forecast BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ade87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "from datetime import timedelta as td\n",
    "from datetime import datetime\n",
    "# from pandas import Timedelta as td\n",
    "import traceback\n",
    "# from pandas.plotting import scatter_matrix\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.robust import mad\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pandas import concat\n",
    "from math import sqrt\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.utils import check_array\n",
    "# from epftoolbox.models import LEAR\n",
    "from epftoolbox.models import evaluate_lear_in_test_dataset\n",
    "\n",
    "from sklearn.linear_model import LassoLarsIC, Lasso\n",
    "from epftoolbox.data import scaling\n",
    "from epftoolbox.data import read_data\n",
    "from epftoolbox.models import LEAR\n",
    "from epftoolbox.models import evaluate_lear_in_test_dataset\n",
    "from epftoolbox.evaluation import MAE, sMAPE\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "\n",
    "date_format=\"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "# dat = pd.read_csv(\"C:/Users/ciara/OneDrive/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "dat = pd.read_csv(\"/home/coconnor/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "#dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat=pd.DataFrame(dat)\n",
    "dat=dat.bfill(axis ='rows')\n",
    "dat=dat.ffill(axis ='rows')\n",
    "dat=dat._get_numeric_data()\n",
    "\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[:7250,:]\n",
    "Y_train=Y.iloc[:7250,:]\n",
    "X_test=X.iloc[7250:8739,:]\n",
    "Y_test=Y.iloc[7250:8739,:]\n",
    "\n",
    "X_1 = X_train[\n",
    "    [\"lag_-3x1\", \"lag_-4x1\", \"lag_-5x1\", \"lag_-6x1\", \"lag_-7x1\", \"lag_-8x1\", \"lag_-9x1\", \"lag_-10x1\", \"lag_-11x1\",\n",
    "     \"lag_-12x1\", \"lag_-13x1\", \"lag_-14x1\", \"lag_-15x1\", \"lag_-16x1\", \"lag_-17x1\", \"lag_-18x1\",\"lag_-19x1\", \"lag_-20x1\", \"lag_-21x1\", \"lag_-22x1\", \"lag_-23x1\", \"lag_-24x1\", \"lag_-25x1\", \"lag_-26x1\",\n",
    "     \"lag_-27x1\", \"lag_-28x1\", \"lag_-29x1\", \"lag_-30x1\", \"lag_-31x1\", \"lag_-32x1\", \"lag_-33x1\", \"lag_-34x1\",\"lag_-35x1\", \"lag_-36x1\", \"lag_-37x1\", \"lag_-38x1\", \"lag_-39x1\", \"lag_-40x1\", \"lag_-41x1\", \"lag_-42x1\",\n",
    "     \"lag_-43x1\", \"lag_-44x1\", \"lag_-45x1\", \"lag_-46x1\", \"lag_-47x1\", \"lag_-48x1\", \"lag_-49x1\", \"lag_-50x1\"]]\n",
    "\n",
    "\n",
    "X_2 = X_train[\n",
    "    [\"lag_-3x2\", \"lag_-4x2\", \"lag_-5x2\", \"lag_-6x2\", \"lag_-7x2\", \"lag_-8x2\", \"lag_-9x2\", \"lag_-10x2\", \"lag_-11x2\",\n",
    "     \"lag_-12x2\", \"lag_-13x2\", \"lag_-14x2\", \"lag_-15x2\", \"lag_-16x2\", \"lag_-17x2\", \"lag_-18x2\",\"lag_-19x2\", \"lag_-20x2\", \"lag_-21x2\", \"lag_-22x2\", \"lag_-23x2\", \"lag_-24x2\", \"lag_-25x2\", \"lag_-26x2\",\n",
    "     \"lag_-27x2\", \"lag_-28x2\", \"lag_-29x2\", \"lag_-30x2\", \"lag_-31x2\", \"lag_-32x2\", \"lag_-33x2\", \"lag_-34x2\",\"lag_-35x2\", \"lag_-36x2\", \"lag_-37x2\", \"lag_-38x2\", \"lag_-39x2\", \"lag_-40x2\", \"lag_-41x2\", \"lag_-42x2\",\n",
    "     \"lag_-43x2\", \"lag_-44x2\", \"lag_-45x2\", \"lag_-46x2\", \"lag_-47x2\", \"lag_-48x2\", \"lag_-49x2\", \"lag_-50x2\"]]\n",
    "\n",
    "\n",
    "X_3 = X_train[\n",
    "    [\"lag_-2x3\", \"lag_-3x3\", \"lag_-4x3\", \"lag_-5x3\", \"lag_-6x3\", \"lag_-7x3\", \"lag_-8x3\", \"lag_-9x3\", \"lag_-10x3\",\n",
    "     \"lag_-11x3\", \"lag_-12x3\", \"lag_-13x3\", \"lag_-14x3\", \"lag_-15x3\", \"lag_-16x3\", \"lag_-17x3\",\"lag_-18x3\", \"lag_-19x3\", \"lag_-20x3\", \"lag_-21x3\", \"lag_-22x3\", \"lag_-23x3\", \"lag_-24x3\", \"lag_-25x3\",\n",
    "     \"lag_-26x3\", \"lag_-27x3\", \"lag_-28x3\", \"lag_-29x3\", \"lag_-30x3\", \"lag_-31x3\", \"lag_-32x3\", \"lag_-33x3\",\"lag_-34x3\", \"lag_-35x3\", \"lag_-36x3\", \"lag_-37x3\", \"lag_-38x3\", \"lag_-39x3\", \"lag_-40x3\", \"lag_-41x3\",\n",
    "     \"lag_-42x3\", \"lag_-43x3\", \"lag_-44x3\", \"lag_-45x3\", \"lag_-46x3\", \"lag_-47x3\", \"lag_-48x3\", \"lag_-49x3\"]]\n",
    "\n",
    "\n",
    "X_4 = X_train[\n",
    "    [\"lag_0x6\", \"lag_-1x6\", \"lag_-2x6\", \"lag_-3x6\", \"lag_-4x6\", \"lag_-5x6\", \"lag_-6x6\", \"lag_-7x6\", \"lag_-8x6\",\n",
    "     \"lag_-9x6\", \"lag_-10x6\", \"lag_-11x6\", \"lag_-12x6\", \"lag_-13x6\", \"lag_-14x6\", \"lag_-15x6\",\"lag_-16x6\", \"lag_-17x6\", \"lag_-18x6\", \"lag_-19x6\", \"lag_-20x6\", \"lag_-21x6\", \"lag_-22x6\", \"lag_-23x6\",\n",
    "     \"lag_-24x6\", \"lag_-25x6\", \"lag_-26x6\", \"lag_-27x6\", \"lag_-28x6\", \"lag_-29x6\", \"lag_-30x6\", \"lag_-31x6\",\"lag_-32x6\", \"lag_-33x6\", \"lag_-34x6\", \"lag_-35x6\", \"lag_-36x6\", \"lag_-37x6\", \"lag_-38x6\", \"lag_-39x6\",\n",
    "     \"lag_-40x6\", \"lag_-41x6\", \"lag_-42x6\", \"lag_-43x6\", \"lag_-44x6\", \"lag_-45x6\", \"lag_-46x6\", \"lag_-47x6\"]]\n",
    "\n",
    "\n",
    "X_5 = X_train[\n",
    "    [\"lag_-2x12\", \"lag_-3x12\", \"lag_-4x12\", \"lag_-5x12\", \"lag_-6x12\", \"lag_-7x12\", \"lag_-8x12\", \"lag_-9x12\",\n",
    "     \"lag_-10x12\", \"lag_-11x12\", \"lag_-12x12\", \"lag_-13x12\", \"lag_-14x12\", \"lag_-15x12\", \"lag_-16x12\", \"lag_-17x12\",\"lag_-18x12\", \"lag_-19x12\", \"lag_-20x12\", \"lag_-21x12\", \"lag_-22x12\", \"lag_-23x12\", \"lag_-24x12\", \"lag_-25x12\",\n",
    "     \"lag_-26x12\", \"lag_-27x12\", \"lag_-28x12\", \"lag_-29x12\", \"lag_-30x12\", \"lag_-31x12\", \"lag_-32x12\", \"lag_-33x12\",\"lag_-34x12\", \"lag_-35x12\", \"lag_-36x12\", \"lag_-37x12\", \"lag_-38x12\", \"lag_-39x12\", \"lag_-40x12\", \"lag_-41x12\",\n",
    "     \"lag_-42x12\", \"lag_-43x12\", \"lag_-44x12\", \"lag_-45x12\", \"lag_-46x12\", \"lag_-47x12\", \"lag_-48x12\", \"lag_-49x12\"]]\n",
    "\n",
    "\n",
    "X_6 = X_train[\n",
    "    [\"lag_2x7\", \"lag_3x7\", \"lag_4x7\", \"lag_5x7\", \"lag_6x7\", \"lag_7x7\", \"lag_8x7\", \"lag_9x7\", \"lag_10x7\", \"lag_11x7\",\n",
    "     \"lag_12x7\", \"lag_13x7\", \"lag_14x7\", \"lag_15x7\", \"lag_16x7\", \"lag_17x7\"]]\n",
    "X_7 = X_train[\n",
    "    [\"lag_2x8\", \"lag_3x8\", \"lag_4x8\", \"lag_5x8\", \"lag_6x8\", \"lag_7x8\", \"lag_8x8\", \"lag_9x8\", \"lag_10x8\", \"lag_11x8\",\n",
    "     \"lag_12x8\", \"lag_13x8\", \"lag_14x8\", \"lag_15x8\", \"lag_16x8\", \"lag_17x8\"]]\n",
    "X_8 = X_train[\n",
    "    [\"lag_2x9\", \"lag_3x9\", \"lag_4x9\", \"lag_5x9\", \"lag_6x9\", \"lag_7x9\", \"lag_8x9\", \"lag_9x9\", \"lag_10x9\", \"lag_11x9\",\n",
    "     \"lag_12x9\", \"lag_13x9\", \"lag_14x9\", \"lag_15x9\", \"lag_16x9\", \"lag_17x9\"]]\n",
    "X_9 = X_train[\n",
    "    [\"lag_2x10\", \"lag_3x10\", \"lag_4x10\", \"lag_5x10\", \"lag_6x10\", \"lag_7x10\", \"lag_8x10\", \"lag_9x10\", \"lag_10x10\",\n",
    "     \"lag_11x10\", \"lag_12x10\", \"lag_13x10\", \"lag_14x10\", \"lag_15x10\", \"lag_16x10\", \"lag_17x10\"]]\n",
    "X_10 = X_train[\n",
    "    [\"lag_2x11\", \"lag_3x11\", \"lag_4x11\", \"lag_5x11\", \"lag_6x11\", \"lag_7x11\", \"lag_8x11\", \"lag_9x11\", \"lag_10x11\",\n",
    "     \"lag_11x11\", \"lag_12x11\", \"lag_13x11\", \"lag_14x11\", \"lag_15x11\", \"lag_16x11\", \"lag_17x11\"]]\n",
    "\n",
    "X_test1 = X_test[\n",
    "    [\"lag_-3x1\", \"lag_-4x1\", \"lag_-5x1\", \"lag_-6x1\", \"lag_-7x1\", \"lag_-8x1\", \"lag_-9x1\", \"lag_-10x1\", \"lag_-11x1\",\n",
    "     \"lag_-12x1\", \"lag_-13x1\", \"lag_-14x1\", \"lag_-15x1\", \"lag_-16x1\", \"lag_-17x1\", \"lag_-18x1\",\"lag_-19x1\", \"lag_-20x1\", \"lag_-21x1\", \"lag_-22x1\", \"lag_-23x1\", \"lag_-24x1\", \"lag_-25x1\", \"lag_-26x1\",\n",
    "     \"lag_-27x1\", \"lag_-28x1\", \"lag_-29x1\", \"lag_-30x1\", \"lag_-31x1\", \"lag_-32x1\", \"lag_-33x1\", \"lag_-34x1\",\"lag_-35x1\", \"lag_-36x1\", \"lag_-37x1\", \"lag_-38x1\", \"lag_-39x1\", \"lag_-40x1\", \"lag_-41x1\", \"lag_-42x1\",\n",
    "     \"lag_-43x1\", \"lag_-44x1\", \"lag_-45x1\", \"lag_-46x1\", \"lag_-47x1\", \"lag_-48x1\", \"lag_-49x1\", \"lag_-50x1\"]]\n",
    "\n",
    "\n",
    "X_test2 = X_test[\n",
    "    [\"lag_-3x2\", \"lag_-4x2\", \"lag_-5x2\", \"lag_-6x2\", \"lag_-7x2\", \"lag_-8x2\", \"lag_-9x2\", \"lag_-10x2\", \"lag_-11x2\",\n",
    "     \"lag_-12x2\", \"lag_-13x2\", \"lag_-14x2\", \"lag_-15x2\", \"lag_-16x2\", \"lag_-17x2\", \"lag_-18x2\",\"lag_-19x2\", \"lag_-20x2\", \"lag_-21x2\", \"lag_-22x2\", \"lag_-23x2\", \"lag_-24x2\", \"lag_-25x2\", \"lag_-26x2\",\n",
    "     \"lag_-27x2\", \"lag_-28x2\", \"lag_-29x2\", \"lag_-30x2\", \"lag_-31x2\", \"lag_-32x2\", \"lag_-33x2\", \"lag_-34x2\",\"lag_-35x2\", \"lag_-36x2\", \"lag_-37x2\", \"lag_-38x2\", \"lag_-39x2\", \"lag_-40x2\", \"lag_-41x2\", \"lag_-42x2\",\n",
    "     \"lag_-43x2\", \"lag_-44x2\", \"lag_-45x2\", \"lag_-46x2\", \"lag_-47x2\", \"lag_-48x2\", \"lag_-49x2\", \"lag_-50x2\"]]\n",
    "\n",
    "\n",
    "X_test3 = X_test[\n",
    "    [\"lag_-2x3\", \"lag_-3x3\", \"lag_-4x3\", \"lag_-5x3\", \"lag_-6x3\", \"lag_-7x3\", \"lag_-8x3\", \"lag_-9x3\", \"lag_-10x3\",\n",
    "     \"lag_-11x3\", \"lag_-12x3\", \"lag_-13x3\", \"lag_-14x3\", \"lag_-15x3\", \"lag_-16x3\", \"lag_-17x3\",\"lag_-18x3\", \"lag_-19x3\", \"lag_-20x3\", \"lag_-21x3\", \"lag_-22x3\", \"lag_-23x3\", \"lag_-24x3\", \"lag_-25x3\",\n",
    "     \"lag_-26x3\", \"lag_-27x3\", \"lag_-28x3\", \"lag_-29x3\", \"lag_-30x3\", \"lag_-31x3\", \"lag_-32x3\", \"lag_-33x3\",\"lag_-34x3\", \"lag_-35x3\", \"lag_-36x3\", \"lag_-37x3\", \"lag_-38x3\", \"lag_-39x3\", \"lag_-40x3\", \"lag_-41x3\",\n",
    "     \"lag_-42x3\", \"lag_-43x3\", \"lag_-44x3\", \"lag_-45x3\", \"lag_-46x3\", \"lag_-47x3\", \"lag_-48x3\", \"lag_-49x3\"]]\n",
    "\n",
    "\n",
    "X_test4 = X_test[\n",
    "    [\"lag_0x6\", \"lag_-1x6\", \"lag_-2x6\", \"lag_-3x6\", \"lag_-4x6\", \"lag_-5x6\", \"lag_-6x6\", \"lag_-7x6\", \"lag_-8x6\",\n",
    "     \"lag_-9x6\", \"lag_-10x6\", \"lag_-11x6\", \"lag_-12x6\", \"lag_-13x6\", \"lag_-14x6\", \"lag_-15x6\",\"lag_-16x6\", \"lag_-17x6\", \"lag_-18x6\", \"lag_-19x6\", \"lag_-20x6\", \"lag_-21x6\", \"lag_-22x6\", \"lag_-23x6\",\n",
    "     \"lag_-24x6\", \"lag_-25x6\", \"lag_-26x6\", \"lag_-27x6\", \"lag_-28x6\", \"lag_-29x6\", \"lag_-30x6\", \"lag_-31x6\",\"lag_-32x6\", \"lag_-33x6\", \"lag_-34x6\", \"lag_-35x6\", \"lag_-36x6\", \"lag_-37x6\", \"lag_-38x6\", \"lag_-39x6\",\n",
    "     \"lag_-40x6\", \"lag_-41x6\", \"lag_-42x6\", \"lag_-43x6\", \"lag_-44x6\", \"lag_-45x6\", \"lag_-46x6\", \"lag_-47x6\"]]\n",
    "\n",
    "\n",
    "X_test5 = X_test[\n",
    "    [\"lag_-2x12\", \"lag_-3x12\", \"lag_-4x12\", \"lag_-5x12\", \"lag_-6x12\", \"lag_-7x12\", \"lag_-8x12\", \"lag_-9x12\",\n",
    "     \"lag_-10x12\", \"lag_-11x12\", \"lag_-12x12\", \"lag_-13x12\", \"lag_-14x12\", \"lag_-15x12\", \"lag_-16x12\", \"lag_-17x12\",\"lag_-18x12\", \"lag_-19x12\", \"lag_-20x12\", \"lag_-21x12\", \"lag_-22x12\", \"lag_-23x12\", \"lag_-24x12\", \"lag_-25x12\",\n",
    "     \"lag_-26x12\", \"lag_-27x12\", \"lag_-28x12\", \"lag_-29x12\", \"lag_-30x12\", \"lag_-31x12\", \"lag_-32x12\", \"lag_-33x12\",\"lag_-34x12\", \"lag_-35x12\", \"lag_-36x12\", \"lag_-37x12\", \"lag_-38x12\", \"lag_-39x12\", \"lag_-40x12\", \"lag_-41x12\",\n",
    "     \"lag_-42x12\", \"lag_-43x12\", \"lag_-44x12\", \"lag_-45x12\", \"lag_-46x12\", \"lag_-47x12\", \"lag_-48x12\", \"lag_-49x12\"]]\n",
    "\n",
    "\n",
    "X_test6 = X_test[\n",
    "    [\"lag_2x7\", \"lag_3x7\", \"lag_4x7\", \"lag_5x7\", \"lag_6x7\", \"lag_7x7\", \"lag_8x7\", \"lag_9x7\", \"lag_10x7\", \"lag_11x7\",\n",
    "     \"lag_12x7\", \"lag_13x7\", \"lag_14x7\", \"lag_15x7\", \"lag_16x7\", \"lag_17x7\"]]\n",
    "X_test7 = X_test[\n",
    "    [\"lag_2x8\", \"lag_3x8\", \"lag_4x8\", \"lag_5x8\", \"lag_6x8\", \"lag_7x8\", \"lag_8x8\", \"lag_9x8\", \"lag_10x8\", \"lag_11x8\",\n",
    "     \"lag_12x8\", \"lag_13x8\", \"lag_14x8\", \"lag_15x8\", \"lag_16x8\", \"lag_17x8\"]]\n",
    "X_test8 = X_test[\n",
    "    [\"lag_2x9\", \"lag_3x9\", \"lag_4x9\", \"lag_5x9\", \"lag_6x9\", \"lag_7x9\", \"lag_8x9\", \"lag_9x9\", \"lag_10x9\", \"lag_11x9\",\n",
    "     \"lag_12x9\", \"lag_13x9\", \"lag_14x9\", \"lag_15x9\", \"lag_16x9\", \"lag_17x9\"]]\n",
    "X_test9 = X_test[\n",
    "    [\"lag_2x10\", \"lag_3x10\", \"lag_4x10\", \"lag_5x10\", \"lag_6x10\", \"lag_7x10\", \"lag_8x10\", \"lag_9x10\", \"lag_10x10\",\n",
    "     \"lag_11x10\", \"lag_12x10\", \"lag_13x10\", \"lag_14x10\", \"lag_15x10\", \"lag_16x10\", \"lag_17x10\"]]\n",
    "X_test10 = X_test[\n",
    "    [\"lag_2x11\", \"lag_3x11\", \"lag_4x11\", \"lag_5x11\", \"lag_6x11\", \"lag_7x11\", \"lag_8x11\", \"lag_9x11\", \"lag_10x11\",\n",
    "     \"lag_11x11\", \"lag_12x11\", \"lag_13x11\", \"lag_14x11\", \"lag_15x11\", \"lag_16x11\", \"lag_17x11\"]]\n",
    "\n",
    "Y_1 = Y_train[\n",
    "    [\"lag_2y\", \"lag_3y\", \"lag_4y\", \"lag_5y\", \"lag_6y\", \"lag_7y\", \"lag_8y\", \"lag_9y\", \"lag_10y\", \"lag_11y\", \"lag_12y\",\n",
    "     \"lag_13y\", \"lag_14y\", \"lag_15y\", \"lag_16y\", \"lag_17y\"]]\n",
    "\n",
    "[X_1], X_scaler1 = scaling([X_1.values], 'Invariant')\n",
    "[X_2], X_scaler2 = scaling([X_2.values], 'Invariant')\n",
    "[X_3], X_scaler3 = scaling([X_3.values], 'Invariant')\n",
    "[X_4], X_scaler4 = scaling([X_4.values], 'Invariant')\n",
    "[X_5], X_scaler5 = scaling([X_5.values], 'Invariant')\n",
    "\n",
    "[X_6], X_scaler6 = scaling([X_6.values], 'Invariant')\n",
    "[X_7], X_scaler7 = scaling([X_7.values], 'Invariant')\n",
    "[X_8], X_scaler8 = scaling([X_8.values], 'Invariant')\n",
    "[X_9], X_scaler9 = scaling([X_9.values], 'Invariant')\n",
    "[X_10], X_scaler10 = scaling([X_10.values], 'Invariant')\n",
    "\n",
    "X_test_1= X_scaler1.transform(X_test1.values)\n",
    "X_test_2= X_scaler2.transform(X_test2.values)\n",
    "X_test_3= X_scaler3.transform(X_test3.values)\n",
    "X_test_4= X_scaler4.transform(X_test4.values)\n",
    "X_test_5= X_scaler5.transform(X_test5.values)\n",
    "X_test_6= X_scaler6.transform(X_test6.values)\n",
    "X_test_7= X_scaler7.transform(X_test7.values)\n",
    "X_test_8= X_scaler8.transform(X_test8.values)\n",
    "X_test_9= X_scaler9.transform(X_test9.values)\n",
    "X_test_10= X_scaler10.transform(X_test10.values)\n",
    "\n",
    "[Y_train_scaled], Y_scaler = scaling([Y_1.values], 'Invariant')\n",
    "\n",
    "X_train_scaled=np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9, X_10), axis=1)\n",
    "X_test_scaled=np.concatenate((X_test_1, X_test_2, X_test_3, X_test_4, X_test_5, X_test_6, X_test_7, X_test_8, X_test_9, X_test_10), axis=1)\n",
    "\n",
    "alpha=LassoLarsIC(criterion='aic', max_iter=2500).fit(X_train_scaled, Y_train_scaled[:,:1].ravel()).alpha_\n",
    "\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                                       test_start_time: dt, test_end_time: dt):\n",
    "    \"\"\"\n",
    "    This method takes the raw information contained in the participat_df (i.e. explanatory variables and targets) and produces dataframes\n",
    "        train_X, train_y, test_X, test_y, test_df\n",
    "    What are the uses of these dataframes?\n",
    "        - The train_X and train_y dataframes can be used to train models.\n",
    "        - For the trained model, predictions can then be made using the test_X dataframe.\n",
    "        - Predictions made in the previous step can then be compared to actual/target values contained in the test_y dataframe.\n",
    "        - Finally, the test_df is used by other methods for plotting.\n",
    "    Thus, this method will be called repeatedly in the rolling_walk_forward_validation method/process.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    participant_df : pd.DataFrame\n",
    "        Pandas dataframe, contains the participant time series info (i.e. explanatory and target variables, the index will be te trading period).\n",
    "    date_time_column : str\n",
    "        This is the column in the participant_df which indicates the deliveryperiod.\n",
    "    train_start_time : dt\n",
    "        The train_X and train_y dataframes will cover the interval [train_start_time, train_end_time].\n",
    "    train_end_time : dt\n",
    "        See previous comment.\n",
    "    test_start_time : dt\n",
    "        The test_X and test_y dataframes will cover the 24 trading periods from [train_end_time, train_end_time + 24 hours].\n",
    "    test_end_time : dt\n",
    "        See previous comment.\n",
    "    columns_to_exclude: [str]\n",
    "        These are the columns participant_df which should be ignored i.e. columns we don't want to use as explanatory variables.\n",
    "    features_to_encode: [str]\n",
    "        These are the categorical columns for which we want to apply one hot encoding.\n",
    "    prefix_to_include: [str]\n",
    "        For the categorical columns to which we apply one hot encoding, this list helps inform the naming convention for the newly created columns.\n",
    "    targets: [str]\n",
    "        These are the columns that we are trying to predict.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of dataframes.\n",
    "                train_X, train_y, test_X, test_y,test_df\n",
    "    Details and use cases for these dataframes are described above.\n",
    "    \"\"\"\n",
    "\n",
    "    # These are the dataframes that will be returned from the method.\n",
    "    train_X = None\n",
    "    train_Y = None\n",
    "    test_X = None\n",
    "    test_Y = None\n",
    "    test_df = None\n",
    "    train_df = None\n",
    "\n",
    "    try:\n",
    "\n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "        #             return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        # Remove any rows with nan's etc (there shouldn't be any in the input).\n",
    "        participant_df = participant_df.dropna()\n",
    "\n",
    "        date_format = \"%m/%d/%Y %H:%M\"\n",
    "\n",
    "        # The train dataframe, it will be used later to create train_X and train_y.\n",
    "        train_df = None\n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)\n",
    "        train_df = participant_df[\n",
    "            (participant_df.index >= train_start_time_str) & (participant_df.index < train_end_time_str)].copy(\n",
    "            deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\n",
    "                \"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")\n",
    "        #             return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        # Create the test dataframe, it will be used later to create test_X and test_y\n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)\n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format)\n",
    "        test_df = participant_df[\n",
    "            (participant_df.index >= test_start_time_str) & (participant_df.index < test_end_time_str)].copy(\n",
    "            deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\n",
    "                \"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")\n",
    "        #             return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        X_1 = train_df[\n",
    "            [\"lag_-3x1\", \"lag_-4x1\", \"lag_-5x1\", \"lag_-6x1\", \"lag_-7x1\", \"lag_-8x1\", \"lag_-9x1\", \"lag_-10x1\", \"lag_-11x1\",\n",
    "             \"lag_-12x1\", \"lag_-13x1\", \"lag_-14x1\", \"lag_-15x1\", \"lag_-16x1\", \"lag_-17x1\", \"lag_-18x1\",\"lag_-19x1\", \"lag_-20x1\", \"lag_-21x1\", \"lag_-22x1\", \"lag_-23x1\", \"lag_-24x1\", \"lag_-25x1\", \"lag_-26x1\",\n",
    "             \"lag_-27x1\", \"lag_-28x1\", \"lag_-29x1\", \"lag_-30x1\", \"lag_-31x1\", \"lag_-32x1\", \"lag_-33x1\", \"lag_-34x1\",\"lag_-35x1\", \"lag_-36x1\", \"lag_-37x1\", \"lag_-38x1\", \"lag_-39x1\", \"lag_-40x1\", \"lag_-41x1\", \"lag_-42x1\",\n",
    "             \"lag_-43x1\", \"lag_-44x1\", \"lag_-45x1\", \"lag_-46x1\", \"lag_-47x1\", \"lag_-48x1\", \"lag_-49x1\", \"lag_-50x1\"]]\n",
    "\n",
    "\n",
    "        X_2 = train_df[\n",
    "            [\"lag_-3x2\", \"lag_-4x2\", \"lag_-5x2\", \"lag_-6x2\", \"lag_-7x2\", \"lag_-8x2\", \"lag_-9x2\", \"lag_-10x2\", \"lag_-11x2\",\n",
    "             \"lag_-12x2\", \"lag_-13x2\", \"lag_-14x2\", \"lag_-15x2\", \"lag_-16x2\", \"lag_-17x2\", \"lag_-18x2\",\"lag_-19x2\", \"lag_-20x2\", \"lag_-21x2\", \"lag_-22x2\", \"lag_-23x2\", \"lag_-24x2\", \"lag_-25x2\", \"lag_-26x2\",\n",
    "             \"lag_-27x2\", \"lag_-28x2\", \"lag_-29x2\", \"lag_-30x2\", \"lag_-31x2\", \"lag_-32x2\", \"lag_-33x2\", \"lag_-34x2\",\"lag_-35x2\", \"lag_-36x2\", \"lag_-37x2\", \"lag_-38x2\", \"lag_-39x2\", \"lag_-40x2\", \"lag_-41x2\", \"lag_-42x2\",\n",
    "             \"lag_-43x2\", \"lag_-44x2\", \"lag_-45x2\", \"lag_-46x2\", \"lag_-47x2\", \"lag_-48x2\", \"lag_-49x2\", \"lag_-50x2\"]]\n",
    "\n",
    "\n",
    "        X_3 = train_df[\n",
    "            [\"lag_-2x3\", \"lag_-3x3\", \"lag_-4x3\", \"lag_-5x3\", \"lag_-6x3\", \"lag_-7x3\", \"lag_-8x3\", \"lag_-9x3\", \"lag_-10x3\",\n",
    "             \"lag_-11x3\", \"lag_-12x3\", \"lag_-13x3\", \"lag_-14x3\", \"lag_-15x3\", \"lag_-16x3\", \"lag_-17x3\",\"lag_-18x3\", \"lag_-19x3\", \"lag_-20x3\", \"lag_-21x3\", \"lag_-22x3\", \"lag_-23x3\", \"lag_-24x3\", \"lag_-25x3\",\n",
    "             \"lag_-26x3\", \"lag_-27x3\", \"lag_-28x3\", \"lag_-29x3\", \"lag_-30x3\", \"lag_-31x3\", \"lag_-32x3\", \"lag_-33x3\",\"lag_-34x3\", \"lag_-35x3\", \"lag_-36x3\", \"lag_-37x3\", \"lag_-38x3\", \"lag_-39x3\", \"lag_-40x3\", \"lag_-41x3\",\n",
    "             \"lag_-42x3\", \"lag_-43x3\", \"lag_-44x3\", \"lag_-45x3\", \"lag_-46x3\", \"lag_-47x3\", \"lag_-48x3\", \"lag_-49x3\"]]\n",
    "\n",
    "\n",
    "        X_4 = train_df[\n",
    "            [\"lag_0x6\", \"lag_-1x6\", \"lag_-2x6\", \"lag_-3x6\", \"lag_-4x6\", \"lag_-5x6\", \"lag_-6x6\", \"lag_-7x6\", \"lag_-8x6\",\n",
    "             \"lag_-9x6\", \"lag_-10x6\", \"lag_-11x6\", \"lag_-12x6\", \"lag_-13x6\", \"lag_-14x6\", \"lag_-15x6\",\"lag_-16x6\", \"lag_-17x6\", \"lag_-18x6\", \"lag_-19x6\", \"lag_-20x6\", \"lag_-21x6\", \"lag_-22x6\", \"lag_-23x6\",\n",
    "             \"lag_-24x6\", \"lag_-25x6\", \"lag_-26x6\", \"lag_-27x6\", \"lag_-28x6\", \"lag_-29x6\", \"lag_-30x6\", \"lag_-31x6\",\"lag_-32x6\", \"lag_-33x6\", \"lag_-34x6\", \"lag_-35x6\", \"lag_-36x6\", \"lag_-37x6\", \"lag_-38x6\", \"lag_-39x6\",\n",
    "             \"lag_-40x6\", \"lag_-41x6\", \"lag_-42x6\", \"lag_-43x6\", \"lag_-44x6\", \"lag_-45x6\", \"lag_-46x6\", \"lag_-47x6\"]]\n",
    "\n",
    "\n",
    "        X_5 = train_df[\n",
    "            [\"lag_-2x12\", \"lag_-3x12\", \"lag_-4x12\", \"lag_-5x12\", \"lag_-6x12\", \"lag_-7x12\", \"lag_-8x12\", \"lag_-9x12\",\n",
    "             \"lag_-10x12\", \"lag_-11x12\", \"lag_-12x12\", \"lag_-13x12\", \"lag_-14x12\", \"lag_-15x12\", \"lag_-16x12\", \"lag_-17x12\",\"lag_-18x12\", \"lag_-19x12\", \"lag_-20x12\", \"lag_-21x12\", \"lag_-22x12\", \"lag_-23x12\", \"lag_-24x12\", \"lag_-25x12\",\n",
    "             \"lag_-26x12\", \"lag_-27x12\", \"lag_-28x12\", \"lag_-29x12\", \"lag_-30x12\", \"lag_-31x12\", \"lag_-32x12\", \"lag_-33x12\",\"lag_-34x12\", \"lag_-35x12\", \"lag_-36x12\", \"lag_-37x12\", \"lag_-38x12\", \"lag_-39x12\", \"lag_-40x12\", \"lag_-41x12\",\n",
    "             \"lag_-42x12\", \"lag_-43x12\", \"lag_-44x12\", \"lag_-45x12\", \"lag_-46x12\", \"lag_-47x12\", \"lag_-48x12\", \"lag_-49x12\"]]\n",
    "\n",
    "\n",
    "        X_6 = train_df[\n",
    "            [\"lag_2x7\", \"lag_3x7\", \"lag_4x7\", \"lag_5x7\", \"lag_6x7\", \"lag_7x7\", \"lag_8x7\", \"lag_9x7\", \"lag_10x7\", \"lag_11x7\",\n",
    "             \"lag_12x7\", \"lag_13x7\", \"lag_14x7\", \"lag_15x7\", \"lag_16x7\", \"lag_17x7\"]]\n",
    "        X_7 = train_df[\n",
    "            [\"lag_2x8\", \"lag_3x8\", \"lag_4x8\", \"lag_5x8\", \"lag_6x8\", \"lag_7x8\", \"lag_8x8\", \"lag_9x8\", \"lag_10x8\", \"lag_11x8\",\n",
    "             \"lag_12x8\", \"lag_13x8\", \"lag_14x8\", \"lag_15x8\", \"lag_16x8\", \"lag_17x8\"]]\n",
    "        X_8 = train_df[\n",
    "            [\"lag_2x9\", \"lag_3x9\", \"lag_4x9\", \"lag_5x9\", \"lag_6x9\", \"lag_7x9\", \"lag_8x9\", \"lag_9x9\", \"lag_10x9\", \"lag_11x9\",\n",
    "             \"lag_12x9\", \"lag_13x9\", \"lag_14x9\", \"lag_15x9\", \"lag_16x9\", \"lag_17x9\"]]\n",
    "        X_9 = train_df[\n",
    "            [\"lag_2x10\", \"lag_3x10\", \"lag_4x10\", \"lag_5x10\", \"lag_6x10\", \"lag_7x10\", \"lag_8x10\", \"lag_9x10\", \"lag_10x10\",\n",
    "             \"lag_11x10\", \"lag_12x10\", \"lag_13x10\", \"lag_14x10\", \"lag_15x10\", \"lag_16x10\", \"lag_17x10\"]]\n",
    "        X_10 = train_df[\n",
    "            [\"lag_2x11\", \"lag_3x11\", \"lag_4x11\", \"lag_5x11\", \"lag_6x11\", \"lag_7x11\", \"lag_8x11\", \"lag_9x11\", \"lag_10x11\",\n",
    "             \"lag_11x11\", \"lag_12x11\", \"lag_13x11\", \"lag_14x11\", \"lag_15x11\", \"lag_16x11\", \"lag_17x11\"]]\n",
    "\n",
    "        X_test1 = test_df[\n",
    "            [\"lag_-3x1\", \"lag_-4x1\", \"lag_-5x1\", \"lag_-6x1\", \"lag_-7x1\", \"lag_-8x1\", \"lag_-9x1\", \"lag_-10x1\", \"lag_-11x1\",\n",
    "             \"lag_-12x1\", \"lag_-13x1\", \"lag_-14x1\", \"lag_-15x1\", \"lag_-16x1\", \"lag_-17x1\", \"lag_-18x1\",\"lag_-19x1\", \"lag_-20x1\", \"lag_-21x1\", \"lag_-22x1\", \"lag_-23x1\", \"lag_-24x1\", \"lag_-25x1\", \"lag_-26x1\",\n",
    "             \"lag_-27x1\", \"lag_-28x1\", \"lag_-29x1\", \"lag_-30x1\", \"lag_-31x1\", \"lag_-32x1\", \"lag_-33x1\", \"lag_-34x1\",\"lag_-35x1\", \"lag_-36x1\", \"lag_-37x1\", \"lag_-38x1\", \"lag_-39x1\", \"lag_-40x1\", \"lag_-41x1\", \"lag_-42x1\",\n",
    "             \"lag_-43x1\", \"lag_-44x1\", \"lag_-45x1\", \"lag_-46x1\", \"lag_-47x1\", \"lag_-48x1\", \"lag_-49x1\", \"lag_-50x1\"]]\n",
    "\n",
    "\n",
    "        X_test2 = test_df[\n",
    "            [\"lag_-3x2\", \"lag_-4x2\", \"lag_-5x2\", \"lag_-6x2\", \"lag_-7x2\", \"lag_-8x2\", \"lag_-9x2\", \"lag_-10x2\", \"lag_-11x2\",\n",
    "             \"lag_-12x2\", \"lag_-13x2\", \"lag_-14x2\", \"lag_-15x2\", \"lag_-16x2\", \"lag_-17x2\", \"lag_-18x2\",\"lag_-19x2\", \"lag_-20x2\", \"lag_-21x2\", \"lag_-22x2\", \"lag_-23x2\", \"lag_-24x2\", \"lag_-25x2\", \"lag_-26x2\",\n",
    "             \"lag_-27x2\", \"lag_-28x2\", \"lag_-29x2\", \"lag_-30x2\", \"lag_-31x2\", \"lag_-32x2\", \"lag_-33x2\", \"lag_-34x2\",\"lag_-35x2\", \"lag_-36x2\", \"lag_-37x2\", \"lag_-38x2\", \"lag_-39x2\", \"lag_-40x2\", \"lag_-41x2\", \"lag_-42x2\",\n",
    "             \"lag_-43x2\", \"lag_-44x2\", \"lag_-45x2\", \"lag_-46x2\", \"lag_-47x2\", \"lag_-48x2\", \"lag_-49x2\", \"lag_-50x2\"]]\n",
    "\n",
    "\n",
    "        X_test3 = test_df[\n",
    "            [\"lag_-2x3\", \"lag_-3x3\", \"lag_-4x3\", \"lag_-5x3\", \"lag_-6x3\", \"lag_-7x3\", \"lag_-8x3\", \"lag_-9x3\", \"lag_-10x3\",\n",
    "             \"lag_-11x3\", \"lag_-12x3\", \"lag_-13x3\", \"lag_-14x3\", \"lag_-15x3\", \"lag_-16x3\", \"lag_-17x3\",\"lag_-18x3\", \"lag_-19x3\", \"lag_-20x3\", \"lag_-21x3\", \"lag_-22x3\", \"lag_-23x3\", \"lag_-24x3\", \"lag_-25x3\",\n",
    "             \"lag_-26x3\", \"lag_-27x3\", \"lag_-28x3\", \"lag_-29x3\", \"lag_-30x3\", \"lag_-31x3\", \"lag_-32x3\", \"lag_-33x3\",\"lag_-34x3\", \"lag_-35x3\", \"lag_-36x3\", \"lag_-37x3\", \"lag_-38x3\", \"lag_-39x3\", \"lag_-40x3\", \"lag_-41x3\",\n",
    "             \"lag_-42x3\", \"lag_-43x3\", \"lag_-44x3\", \"lag_-45x3\", \"lag_-46x3\", \"lag_-47x3\", \"lag_-48x3\", \"lag_-49x3\"]]\n",
    "\n",
    "\n",
    "        X_test4 = test_df[\n",
    "            [\"lag_0x6\", \"lag_-1x6\", \"lag_-2x6\", \"lag_-3x6\", \"lag_-4x6\", \"lag_-5x6\", \"lag_-6x6\", \"lag_-7x6\", \"lag_-8x6\",\n",
    "             \"lag_-9x6\", \"lag_-10x6\", \"lag_-11x6\", \"lag_-12x6\", \"lag_-13x6\", \"lag_-14x6\", \"lag_-15x6\",\"lag_-16x6\", \"lag_-17x6\", \"lag_-18x6\", \"lag_-19x6\", \"lag_-20x6\", \"lag_-21x6\", \"lag_-22x6\", \"lag_-23x6\",\n",
    "             \"lag_-24x6\", \"lag_-25x6\", \"lag_-26x6\", \"lag_-27x6\", \"lag_-28x6\", \"lag_-29x6\", \"lag_-30x6\", \"lag_-31x6\",\"lag_-32x6\", \"lag_-33x6\", \"lag_-34x6\", \"lag_-35x6\", \"lag_-36x6\", \"lag_-37x6\", \"lag_-38x6\", \"lag_-39x6\",\n",
    "             \"lag_-40x6\", \"lag_-41x6\", \"lag_-42x6\", \"lag_-43x6\", \"lag_-44x6\", \"lag_-45x6\", \"lag_-46x6\", \"lag_-47x6\"]]\n",
    "\n",
    "\n",
    "        X_test5 = test_df[\n",
    "            [\"lag_-2x12\", \"lag_-3x12\", \"lag_-4x12\", \"lag_-5x12\", \"lag_-6x12\", \"lag_-7x12\", \"lag_-8x12\", \"lag_-9x12\",\n",
    "             \"lag_-10x12\", \"lag_-11x12\", \"lag_-12x12\", \"lag_-13x12\", \"lag_-14x12\", \"lag_-15x12\", \"lag_-16x12\", \"lag_-17x12\",\"lag_-18x12\", \"lag_-19x12\", \"lag_-20x12\", \"lag_-21x12\", \"lag_-22x12\", \"lag_-23x12\", \"lag_-24x12\", \"lag_-25x12\",\n",
    "             \"lag_-26x12\", \"lag_-27x12\", \"lag_-28x12\", \"lag_-29x12\", \"lag_-30x12\", \"lag_-31x12\", \"lag_-32x12\", \"lag_-33x12\",\"lag_-34x12\", \"lag_-35x12\", \"lag_-36x12\", \"lag_-37x12\", \"lag_-38x12\", \"lag_-39x12\", \"lag_-40x12\", \"lag_-41x12\",\n",
    "             \"lag_-42x12\", \"lag_-43x12\", \"lag_-44x12\", \"lag_-45x12\", \"lag_-46x12\", \"lag_-47x12\", \"lag_-48x12\", \"lag_-49x12\"]]\n",
    "\n",
    "\n",
    "        X_test6 = test_df[\n",
    "            [\"lag_2x7\", \"lag_3x7\", \"lag_4x7\", \"lag_5x7\", \"lag_6x7\", \"lag_7x7\", \"lag_8x7\", \"lag_9x7\", \"lag_10x7\", \"lag_11x7\",\n",
    "             \"lag_12x7\", \"lag_13x7\", \"lag_14x7\", \"lag_15x7\", \"lag_16x7\", \"lag_17x7\"]]\n",
    "        X_test7 = test_df[\n",
    "            [\"lag_2x8\", \"lag_3x8\", \"lag_4x8\", \"lag_5x8\", \"lag_6x8\", \"lag_7x8\", \"lag_8x8\", \"lag_9x8\", \"lag_10x8\", \"lag_11x8\",\n",
    "             \"lag_12x8\", \"lag_13x8\", \"lag_14x8\", \"lag_15x8\", \"lag_16x8\", \"lag_17x8\"]]\n",
    "        X_test8 = test_df[\n",
    "            [\"lag_2x9\", \"lag_3x9\", \"lag_4x9\", \"lag_5x9\", \"lag_6x9\", \"lag_7x9\", \"lag_8x9\", \"lag_9x9\", \"lag_10x9\", \"lag_11x9\",\n",
    "             \"lag_12x9\", \"lag_13x9\", \"lag_14x9\", \"lag_15x9\", \"lag_16x9\", \"lag_17x9\"]]\n",
    "        X_test9 = test_df[\n",
    "            [\"lag_2x10\", \"lag_3x10\", \"lag_4x10\", \"lag_5x10\", \"lag_6x10\", \"lag_7x10\", \"lag_8x10\", \"lag_9x10\", \"lag_10x10\",\n",
    "             \"lag_11x10\", \"lag_12x10\", \"lag_13x10\", \"lag_14x10\", \"lag_15x10\", \"lag_16x10\", \"lag_17x10\"]]\n",
    "        X_test10 = test_df[\n",
    "            [\"lag_2x11\", \"lag_3x11\", \"lag_4x11\", \"lag_5x11\", \"lag_6x11\", \"lag_7x11\", \"lag_8x11\", \"lag_9x11\", \"lag_10x11\",\n",
    "             \"lag_11x11\", \"lag_12x11\", \"lag_13x11\", \"lag_14x11\", \"lag_15x11\", \"lag_16x11\", \"lag_17x11\"]]\n",
    "\n",
    "        Y_1 = train_df[\n",
    "            [\"lag_2y\", \"lag_3y\", \"lag_4y\", \"lag_5y\", \"lag_6y\", \"lag_7y\", \"lag_8y\", \"lag_9y\", \"lag_10y\", \"lag_11y\", \"lag_12y\",\n",
    "             \"lag_13y\", \"lag_14y\", \"lag_15y\", \"lag_16y\", \"lag_17y\"]]\n",
    "\n",
    "        [X_1], X_scaler1 = scaling([X_1.values], 'Invariant')\n",
    "        [X_2], X_scaler2 = scaling([X_2.values], 'Invariant')\n",
    "        [X_3], X_scaler3 = scaling([X_3.values], 'Invariant')\n",
    "        [X_4], X_scaler4 = scaling([X_4.values], 'Invariant')\n",
    "        [X_5], X_scaler5 = scaling([X_5.values], 'Invariant')\n",
    "\n",
    "        [X_6], X_scaler6 = scaling([X_6.values], 'Invariant')\n",
    "        [X_7], X_scaler7 = scaling([X_7.values], 'Invariant')\n",
    "        [X_8], X_scaler8 = scaling([X_8.values], 'Invariant')\n",
    "        [X_9], X_scaler9 = scaling([X_9.values], 'Invariant')\n",
    "        [X_10], X_scaler10 = scaling([X_10.values], 'Invariant')\n",
    "\n",
    "        X_test_1= X_scaler1.transform(X_test1.values)\n",
    "        X_test_2= X_scaler2.transform(X_test2.values)\n",
    "        X_test_3= X_scaler3.transform(X_test3.values)\n",
    "        X_test_4= X_scaler4.transform(X_test4.values)\n",
    "        X_test_5= X_scaler5.transform(X_test5.values)\n",
    "        X_test_6= X_scaler6.transform(X_test6.values)\n",
    "        X_test_7= X_scaler7.transform(X_test7.values)\n",
    "        X_test_8= X_scaler8.transform(X_test8.values)\n",
    "        X_test_9= X_scaler9.transform(X_test9.values)\n",
    "        X_test_10= X_scaler10.transform(X_test10.values)\n",
    "\n",
    "        [train_Y], Y_scaler = scaling([Y_1.values], 'Invariant')\n",
    "        Y_scaler_n = Y_scaler\n",
    "\n",
    "        train_X=np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9, X_10), axis=1)\n",
    "        test_X=np.concatenate((X_test_1, X_test_2, X_test_3, X_test_4, X_test_5, X_test_6, X_test_7, X_test_8, X_test_9, X_test_10), axis=1)\n",
    "        test_Y = test_df.iloc[:, 0:16]\n",
    "\n",
    "        return train_X, train_Y, test_X, test_Y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X, train_Y, test_X, test_Y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "\n",
    "def fit_multitarget_model(model, X_train, Y_train, X_test, Y_test, actuals_and_forecast_df, targets, Y_scaler_n):\n",
    "    \"\"\"\n",
    "    Fits the model to the training data.\n",
    "    Then uses the test data to produce a forecast.\n",
    "    Returns a dataframe containing the forecasts and actual values for each of the target variables.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        Model object i.e. randomforestregressor, linear model or other.\n",
    "    X_train : dataframe\n",
    "        The explanatory variables for the train/calibration set, numeric columns may already have been scaled.\n",
    "    Y_train : dataframe\n",
    "        The target variables for the train/calibration set. Might/Mighn't be scaled.\n",
    "    X_test : dataframe\n",
    "        The explanatory variables for the test set, columns may be scaled. It will comprise of 24 rows (1 row for each delivery period in the trading day).\n",
    "    Y_test : dataframe\n",
    "        The target variables for the test set i.e. what we would like to forecast. Similar to the previous bullet point, the dataframe will contain 24 rows.\n",
    "    actuals_and_forecast_df : dataframe\n",
    "        Initially the dataframe will only contain the actual values for each of the targets. At the end of the method it will also contain the forecast values.\n",
    "    targets : [str]\n",
    "        These are the items that we want to predict/forecast.\n",
    "    scale_target_variables: boolean\n",
    "       The target vector, do we want to scale it?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Returns a dataframe containing the forecasts and actual values for each of the target variables i.e. test set forecast and actuals.\n",
    "    \"\"\"\n",
    "    try:\n",
    "#         [ir], Y_scaler = scaling([Y_train], 'Invariant')\n",
    "        cols = Y.columns.values.tolist()\n",
    "\n",
    "        model.fit(X_train, Y_train)\n",
    "        model_test_predictions = None\n",
    "        #         model_test_predictions = model.predict(X_test)\n",
    "        model_test_predictions = pd.DataFrame(Y_scaler_n.inverse_transform(model.predict(X_test)))\n",
    "        print(model_test_predictions)\n",
    "        print(\"test number of observations: \" + str(len(Y_test)))\n",
    "        model_test_mse = mean_squared_error(Y_test, model_test_predictions)\n",
    "        model_test_rmse = round(np.sqrt(model_test_mse), 2)\n",
    "        model_test_mae = round(mean_absolute_error(Y_test, model_test_predictions), 2)\n",
    "        print(\"test rmse: \" + str(model_test_rmse))\n",
    "        print(\"test mae: \" + str(model_test_mae))\n",
    "\n",
    "        for i in range(0, len(cols)):\n",
    "            actuals_and_forecast_df[cols[i] + \"_Forecast\"] = model_test_predictions.iloc[:, i].tolist() if len(\n",
    "                cols) > 1 else model_test_predictions.tolist()\n",
    "            predictor_test_mse = mean_squared_error(Y_test[cols[i]], model_test_predictions.iloc[:, i]) if len(\n",
    "                cols) > 1 else mean_squared_error(Y_test[cols[i]], model_test_predictions.tolist())\n",
    "            predictor_test_rmse = round(np.sqrt(predictor_test_mse), 2)\n",
    "            predictor_test_mae = round(mean_absolute_error(Y_test[cols[i]], model_test_predictions.iloc[:, i]),\n",
    "                                       2) if len(cols) > 1 else round(\n",
    "                mean_absolute_error(Y_test[cols[i]], model_test_predictions.tolist()), 2)\n",
    "            print(cols[i] + \" test rmse: \" + str(predictor_test_rmse))\n",
    "            print(cols[i] + \" test mae: \" + str(predictor_test_mae))\n",
    "\n",
    "        Error_i = ([model_test_rmse, model_test_mae])\n",
    "        print(Error_i)\n",
    "        actuals_and_forecast_df = actuals_and_forecast_df.append(Error_i)\n",
    "\n",
    "        # return the test set, the target and forecast values.\n",
    "        #         actuals_and_forecast_df.sort_index(inplace=True)\n",
    "        #         test_columns = [\"SettlementPeriod\"]\n",
    "        #         for i in range(0,len(cols)):\n",
    "        #             test_columns.extend([cols[i]+\"_Forecast\",cols[i]])\n",
    "        #         actuals_and_forecast_df = actuals_and_forecast_df[test_columns]\n",
    "\n",
    "        return actuals_and_forecast_df\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rolling_walk_forward_validation(model, data, targets, start_time, end_time, training_days, path):\n",
    "    \"\"\"\n",
    "    This method implements the rolling walk forward validation process.\n",
    "    That is,\n",
    "        (a) fit the model on the train data\n",
    "        (b) use the fitted model on the test explanatory variables i.e. forecast 1 day ahead.\n",
    "        (c) Move the training and test datasets forward by 1 day and repeat.\n",
    "    The method will produce\n",
    "        (1) A csv containing the forecast and actual target values i.e. test set output over the horizon of interest.\n",
    "        (2) For each target variable, a graph of the actual and forecast values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: model\n",
    "        The model that will be used to train the data and produce the 1 day ahead forecasts\n",
    "    data: dataframe\n",
    "        Participant dataframe containing the explanatory and target variables.\n",
    "    explanatory_variables_of_interest : [str]\n",
    "        The columns in data that will be used as explanatory variables when fitting the model.\n",
    "        If there are categorical variables we want to use as explanatory variables, they are incorporated via the features_to_encode argument.\n",
    "    targets: [str]\n",
    "        The columns in data that we want to predict/forecast.\n",
    "    features_to_encode: [str]\n",
    "        If there are variables in data that we would like to apply one hot encoding to, we list them here. These one hot encoded vectors are then used as explanatory variables.\n",
    "    prefix_to_include: [str]\n",
    "        Just a string which will be used to name the columns if we apply one hot encoding (related to the features_to_encode argument).\n",
    "     start_time: dt\n",
    "       We will produce a forecast on unseen data for each trading period between [start_time, end_time].\n",
    "     end_time: dt\n",
    "       See previous point.\n",
    "    training_days: int\n",
    "       The number of training days (negative integer expected).\n",
    "    path, unit_name, scenario: str\n",
    "       The combination of path + unit_name + scenario indicate where the csv will be output to.\n",
    "    scale_explanatory_variables: boolean\n",
    "       The explanatory variables, do we want to scale them?\n",
    "    scale_target_variables: boolean\n",
    "       The target variables, do we want to scale them?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Output will include a  csv of the forecast/actual target values and a graph of the same.\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)\n",
    "        results = pd.DataFrame()\n",
    "\n",
    "        # Each time we\n",
    "        # (a) fit the model on the calibration/train data\n",
    "        # (b) apply it to the test data i.e. forecast 1 day ahead.\n",
    "        # Repeat.\n",
    "        date_format = \"%m/%d/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "\n",
    "        while start_time < end_time:\n",
    "\n",
    "            # Train interval\n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time\n",
    "\n",
    "            # Test interval, the test period is always the day ahead forecast\n",
    "            test_start_time = train_end_time + td(hours=8)\n",
    "            test_end_time = test_start_time + td(minutes=30)\n",
    "\n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "\n",
    "            # Generate the calibration and test dataframes.\n",
    "            train_X, train_Y, test_X, test_Y, test_df, train_df, Y_scaler_n = generate_train_and_test_dataframes(\n",
    "                participant_df=dat, train_start_time=train_start_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                train_end_time=train_end_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                test_start_time=test_start_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                test_end_time=test_end_time.strftime(\"%m/%d/%Y %H:%M\"))\n",
    "\n",
    "            if train_X is None or len(train_X) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(\n",
    "                    train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            if test_X is None or len(test_X) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(\n",
    "                    test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            # Fit the model to the train datasets, produce a forecast and return a dataframe containing the forecast/actuals.\n",
    "            actuals_and_forecast_df = fit_multitarget_model(model=model, Y_scaler_n=Y_scaler_n, X_train=train_X,\n",
    "                                                            Y_train=train_Y, X_test=test_X, Y_test=test_Y,\n",
    "                                                            actuals_and_forecast_df=test_df.iloc[:, 0:16],\n",
    "                                                            targets=Y.columns.values.tolist())\n",
    "\n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "\n",
    "            start_time = start_time + td(minutes=30)\n",
    "\n",
    "        results.to_csv(path + \".csv\", index=False)\n",
    "\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "rolling_walk_forward_validation(model=MultiOutputRegressor(Lasso(max_iter=2500, alpha=alpha)), \n",
    "                                data=dat, start_time='06/1/2020 00:00',end_time='09/1/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(),training_days=-300, path=\"/home/coconnor/BM_results_LEAR_1-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4fdc46",
   "metadata": {},
   "source": [
    "XGBoost library regular forecast BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1af7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Mar 25 20:02:46 2022\n",
    "\n",
    "@author: ciaran\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "from datetime import datetime\n",
    "# from pandas import Timedelta as td\n",
    "import traceback\n",
    "# from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "date_format=\"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "# dat = pd.read_csv(\"C:/Users/ciara/OneDrive/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "dat = pd.read_csv(\"/home/coconnor/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "# dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat=pd.DataFrame(dat)\n",
    "dat=dat.bfill(axis ='rows')\n",
    "dat=dat.ffill(axis ='rows')\n",
    "dat=dat._get_numeric_data()\n",
    "\n",
    "Y=dat.iloc[:, 0:16]\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                        test_start_time: dt, test_end_time: dt):\n",
    "    \"\"\"\n",
    "    This method takes the raw information contained in the participat_df (i.e. explanatory variables and targets) and produces dataframes\n",
    "        train_X, train_y, test_X, test_y, test_df\n",
    "    What are the uses of these dataframes?\n",
    "        - The train_X and train_y dataframes can be used to train models.\n",
    "        - For the trained model, predictions can then be made using the test_X dataframe.\n",
    "        - Predictions made in the previous step can then be compared to actual/target values contained in the test_y dataframe.\n",
    "        - Finally, the test_df is used by other methods for plotting.\n",
    "    Thus, this method will be called repeatedly in the rolling_walk_forward_validation method/process.\n",
    "         \n",
    "    Parameters\n",
    "    ----------\n",
    "    participant_df : pd.DataFrame\n",
    "        Pandas dataframe, contains the participant time series info (i.e. explanatory and target variables, the index will be te trading period).\n",
    "    date_time_column : str\n",
    "        This is the column in the participant_df which indicates the deliveryperiod.\n",
    "    train_start_time : dt\n",
    "        The train_X and train_y dataframes will cover the interval [train_start_time, train_end_time].\n",
    "    train_end_time : dt\n",
    "        See previous comment.\n",
    "    test_start_time : dt\n",
    "        The test_X and test_y dataframes will cover the 24 trading periods from [train_end_time, train_end_time + 24 hours].\n",
    "    test_end_time : dt\n",
    "        See previous comment.\n",
    "    columns_to_exclude: [str]\n",
    "        These are the columns participant_df which should be ignored i.e. columns we don't want to use as explanatory variables.\n",
    "    features_to_encode: [str]\n",
    "        These are the categorical columns for which we want to apply one hot encoding.\n",
    "    prefix_to_include: [str]\n",
    "        For the categorical columns to which we apply one hot encoding, this list helps inform the naming convention for the newly created columns.\n",
    "    targets: [str]\n",
    "        These are the columns that we are trying to predict.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of dataframes.\n",
    "                train_X, train_y, test_X, test_y,test_df\n",
    "    Details and use cases for these dataframes are described above.    \n",
    "    \"\"\"\n",
    "  \n",
    "    #These are the dataframes that will be returned from the method.\n",
    "    train_X = None\n",
    "    train_y = None \n",
    "    test_X = None \n",
    "    test_y = None \n",
    "    test_df = None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        #Remove any rows with nan's etc (there shouldn't be any in the input).        \n",
    "        participant_df = participant_df.dropna()\n",
    "\n",
    "        date_format=\"%m/%d/%Y %H:%M\"\n",
    "\n",
    "        #The train dataframe, it will be used later to create train_X and train_y.\n",
    "        train_df = None\n",
    "        \n",
    "#         train_start_time_str = train_start_time.strptime(date_format)\n",
    "#         train_end_time_str = train_end_time.strptime(date_format)\n",
    "        \n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)    \n",
    "        train_df = participant_df[(participant_df.index>=train_start_time_str) & (participant_df.index<train_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")            \n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        #Create the test dataframe, it will be used later to create test_X and test_y\n",
    "#         test_start_time_str = test_start_time.strftime(date_format)\n",
    "#         test_end_time_str = test_end_time.strftime(date_format)\n",
    "        \n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)    \n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format) \n",
    "        test_df = participant_df[(participant_df.index>=test_start_time_str) & (participant_df.index<test_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")            \n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        #The if statement handles situations where we want to scale explanatory variables (need to take care in case we one hot encoded explanatory variables)\n",
    "\n",
    "        train_X = train_df.iloc[:, 16:]\n",
    "        test_X = test_df.iloc[:, 16:]\n",
    "        train_y = train_df.iloc[:, 0:16]\n",
    "        test_y = test_df.iloc[:, 0:16]\n",
    "                                \n",
    "        return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X, train_y, test_X, test_y, test_df\n",
    "    \n",
    "    \n",
    "def fit_multitarget_model(model, X_train, Y_train, X_test, Y_test, actuals_and_forecast_df, targets):\n",
    "    \"\"\"\n",
    "    Fits the model to the training data.\n",
    "    Then uses the test data to produce a forecast.\n",
    "    Returns a dataframe containing the forecasts and actual values for each of the target variables.\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        Model object i.e. randomforestregressor, linear model or other.\n",
    "    X_train : dataframe\n",
    "        The explanatory variables for the train/calibration set, numeric columns may already have been scaled.\n",
    "    Y_train : dataframe\n",
    "        The target variables for the train/calibration set. Might/Mighn't be scaled.\n",
    "    X_test : dataframe\n",
    "        The explanatory variables for the test set, columns may be scaled. It will comprise of 24 rows (1 row for each delivery period in the trading day).\n",
    "    Y_test : dataframe\n",
    "        The target variables for the test set i.e. what we would like to forecast. Similar to the previous bullet point, the dataframe will contain 24 rows.\n",
    "    actuals_and_forecast_df : dataframe\n",
    "        Initially the dataframe will only contain the actual values for each of the targets. At the end of the method it will also contain the forecast values.\n",
    "    targets : [str]\n",
    "        These are the items that we want to predict/forecast.\n",
    "    scale_target_variables: boolean\n",
    "       The target vector, do we want to scale it?\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Returns a dataframe containing the forecasts and actual values for each of the target variables i.e. test set forecast and actuals.\n",
    "    \"\"\"   \n",
    "    try:\n",
    "        \n",
    "#         model.fit(X_train, Y_train) if len(targets) > 1 else model.fit(X_train, Y_train.values.ravel())  \n",
    "        model.fit(X_train, Y_train) \n",
    "\n",
    "        model_test_predictions=None  \n",
    "        model_train_predictions=None\n",
    "        model_train_predictions = model.predict(X_train)\n",
    "        model_test_predictions = model.predict(X_test)          \n",
    "                    \n",
    "        cols = Y_train.columns.values.tolist()   \n",
    "        \n",
    "        \n",
    "        print(\"train number of observations: \" + str(len(Y_train)))\n",
    "        model_train_mse = mean_squared_error(Y_train, model_train_predictions)\n",
    "        model_train_rmse = round(np.sqrt(model_train_mse),2)\n",
    "        model_train_mae = round(mean_absolute_error(Y_train, model_train_predictions),2)\n",
    "        print(\"train rmse: \" + str(model_train_rmse))\n",
    "        print(\"train mae: \" + str(model_train_mae))\n",
    "        \n",
    "        model_test_mse = mean_squared_error(Y_test, model_test_predictions)\n",
    "        model_test_rmse = round(np.sqrt(model_test_mse),2)\n",
    "        model_test_mae = round(mean_absolute_error(Y_test, model_test_predictions),2)\n",
    "        print(\"test rmse: \" + str(model_test_rmse))\n",
    "        print(\"test mae: \" + str(model_test_mae))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(0,len(cols)):    \n",
    "            predictor_train_mse = mean_squared_error(Y_train[cols[i]], model_train_predictions[:,i]) if len(cols) > 1 else mean_squared_error(Y_train[cols[i]], model_train_predictions.tolist()) \n",
    "            predictor_train_rmse = round(np.sqrt(predictor_train_mse),2)\n",
    "            predictor_train_mae = round(mean_absolute_error(Y_train[cols[i]], model_train_predictions[:,i]),2) if len(cols) > 1 else round(mean_absolute_error(Y_train[cols[i]], model_train_predictions.tolist()),2)\n",
    "            print(cols[i] + \" train rmse: \" + str(predictor_train_rmse))\n",
    "            print(cols[i] + \" train mae: \" + str(predictor_train_mae))\n",
    "            \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast\"] = model_test_predictions[:,i].tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "            predictor_test_mse = mean_squared_error(Y_test[cols[i]], model_test_predictions[:,i]) if len(cols) > 1 else mean_squared_error(Y_test[cols[i]], model_test_predictions.tolist())\n",
    "            predictor_test_rmse = round(np.sqrt(predictor_test_mse), 2)\n",
    "            predictor_test_mae = round(mean_absolute_error(Y_test[cols[i]], model_test_predictions[:,i]),2) if len(cols) > 1 else round(mean_absolute_error(Y_test[cols[i]], model_test_predictions.tolist()),2)\n",
    "            print(cols[i] + \" test rmse: \" + str(predictor_test_rmse))\n",
    "            print(cols[i] + \" test mae: \" + str(predictor_test_mae))\n",
    "            \n",
    "        Error_i= ([model_test_rmse, model_test_mae, model_train_rmse, model_train_mae])\n",
    "        print(Error_i)\n",
    "        actuals_and_forecast_df = actuals_and_forecast_df.append(Error_i)\n",
    "          \n",
    "        #return the test set, the target and forecast values.               \n",
    "#         actuals_and_forecast_df.sort_index(inplace=True)\n",
    "#         test_columns = [actuals_and_forecast_df.index]\n",
    "#         for i in range(0,len(cols)):\n",
    "#             test_columns.extend([cols[i]+\"_Forecast\",cols[i]])\n",
    "#         Error_i=Error_i   \n",
    "#         actuals_and_forecast_df = actuals_and_forecast_df[test_columns]\n",
    "#         #return the test set, the target and forecast values.               \n",
    "#         actuals_and_forecast_df.sort_index(inplace=True)\n",
    "#         test_columns = [\"SettlementPeriod\"]\n",
    "#         for i in range(0,len(cols)):\n",
    "#             test_columns.extend([cols[i]+\"_Forecast\",cols[i]])\n",
    "#         actuals_and_forecast_df = actuals_and_forecast_df[test_columns]\n",
    "           \n",
    "        return actuals_and_forecast_df\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "def rolling_walk_forward_validation(model, data, targets, start_time, end_time, training_days, path):\n",
    "    \"\"\"\n",
    "    This method implements the rolling walk forward validation process.\n",
    "    That is,  \n",
    "        (a) fit the model on the train data\n",
    "        (b) use the fitted model on the test explanatory variables i.e. forecast 1 day ahead.\n",
    "        (c) Move the training and test datasets forward by 1 day and repeat.\n",
    "    The method will produce\n",
    "        (1) A csv containing the forecast and actual target values i.e. test set output over the horizon of interest.\n",
    "        (2) For each target variable, a graph of the actual and forecast values.        \n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: model\n",
    "        The model that will be used to train the data and produce the 1 day ahead forecasts \n",
    "    data: dataframe\n",
    "        Participant dataframe containing the explanatory and target variables.\n",
    "    explanatory_variables_of_interest : [str]\n",
    "        The columns in data that will be used as explanatory variables when fitting the model. \n",
    "        If there are categorical variables we want to use as explanatory variables, they are incorporated via the features_to_encode argument.\n",
    "    targets: [str]\n",
    "        The columns in data that we want to predict/forecast.\n",
    "    features_to_encode: [str]\n",
    "        If there are variables in data that we would like to apply one hot encoding to, we list them here. These one hot encoded vectors are then used as explanatory variables.\n",
    "    prefix_to_include: [str]\n",
    "        Just a string which will be used to name the columns if we apply one hot encoding (related to the features_to_encode argument).\n",
    "     start_time: dt\n",
    "       We will produce a forecast on unseen data for each trading period between [start_time, end_time].\n",
    "     end_time: dt\n",
    "       See previous point.\n",
    "    training_days: int\n",
    "       The number of training days (negative integer expected).\n",
    "    path, unit_name, scenario: str\n",
    "       The combination of path + unit_name + scenario indicate where the csv will be output to.\n",
    "    scale_explanatory_variables: boolean\n",
    "       The explanatory variables, do we want to scale them?\n",
    "    scale_target_variables: boolean\n",
    "       The target variables, do we want to scale them?\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Output will include a  csv of the forecast/actual target values and a graph of the same.\n",
    "    \"\"\"   \n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)            \n",
    "        results = pd.DataFrame()\n",
    "            \n",
    "        #Each time we \n",
    "        # (a) fit the model on the calibration/train data\n",
    "        # (b) apply it to the test data i.e. forecast 1 day ahead.\n",
    "        #Repeat.\n",
    "        date_format=\"%m/%d/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "        \n",
    "        while start_time < end_time:\n",
    "            \n",
    "            #Train interval\n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time \n",
    "    \n",
    "            #Test interval, the test period is always the day ahead forecast\n",
    "            test_start_time = train_end_time + td(hours=8)\n",
    "            test_end_time = test_start_time + td(minutes=30)\n",
    "            \n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "    \n",
    "            #Generate the calibration and test dataframes.\n",
    "            train_X, train_y, test_X, test_y, test_df = generate_train_and_test_dataframes(participant_df=dat, train_start_time=train_start_time.strftime(\"%m/%d/%Y %H:%M\"), train_end_time=train_end_time.strftime(\"%m/%d/%Y %H:%M\"), \n",
    "                            test_start_time=test_start_time.strftime(\"%m/%d/%Y %H:%M\"), test_end_time=test_end_time.strftime(\"%m/%d/%Y %H:%M\"))\n",
    "            \n",
    "            if train_X is None or len(train_X) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "    \n",
    "            if test_X is None or len(test_X) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "            \n",
    "            #Fit the model to the train datasets, produce a forecast and return a dataframe containing the forecast/actuals.\n",
    "            actuals_and_forecast_df = fit_multitarget_model(model=model, X_train=train_X, Y_train=train_y,\n",
    "                                            X_test=test_X, Y_test=test_y, actuals_and_forecast_df=test_df,targets=Y.columns.values.tolist())\n",
    "    \n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "            print(results)\n",
    "            start_time = start_time + td(hours=8)\n",
    "            \n",
    "        results.to_csv(path  + \".csv\", index = False)\n",
    "        \n",
    "        #plot the output\n",
    "#         if results is not None and len(results)  > 0:\n",
    "#             for i in range(0,len(targets)):    \n",
    "#                 title_str = scenario + \", \" + targets[i] + \": forecast vs. actual\"\n",
    "#                 results.plot(y=[targets[i]+\"_Forecast\", targets[i]], x=date_time_column,style=['bs-', 'ro-'],title=title_str)            \n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "rolling_walk_forward_validation(model=MultiOutputRegressor(XGBRegressor(learning_rate = 0.05, max_depth = 2, n_estimators =  50)), \n",
    "                                data=dat, start_time='06/1/2020 00:00',end_time='09/1/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(),training_days=-210, path=\"/home/coconnor/BM_results_XGB_1-10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461240a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d255553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
