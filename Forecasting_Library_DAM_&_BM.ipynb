{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c79f7298",
   "metadata": {},
   "source": [
    "Found below are the libraries for generating quantile and regular forecasts in both the balancing and day ahead market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ac5217",
   "metadata": {},
   "source": [
    "Datasets access : https://drive.google.com/drive/u/0/folders/1GSJhwvhRZ5X5A0uJRZzkzCuJ8xu9kDcX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db2753e",
   "metadata": {},
   "source": [
    "Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea67aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ;\n",
    "# os.environ['HDF5_DISABLE_VERSION_CHECK']='2'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import traceback\n",
    "import importlib\n",
    "from pandas import concat\n",
    "from datetime import datetime\n",
    "from datetime import timedelta as td\n",
    "from functools import reduce\n",
    "from math import floor, sqrt\n",
    "from hyperopt import hp, fmin, tpe\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn import \n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from epftoolbox.models import evaluate_lear_in_test_dataset\n",
    "from sklearn.linear_model import LassoLarsIC, Lasso\n",
    "from epftoolbox.data import scaling, read_data\n",
    "from epftoolbox.models import LEAR, evaluate_lear_in_test_dataset\n",
    "from epftoolbox.evaluation import MAE, sMAPE\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.layers import concatenate, Flatten, Dense, BatchNormalization, Dropout\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import \n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, LSTM, concatenate\n",
    "from tensorflow.keras import optimizers, initializers\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.activations import *\n",
    "import tensorflow.keras.backend as K\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c7bc95",
   "metadata": {},
   "source": [
    "Pinball Loss Function for multiple quantile forecasts with both Multi-Headed (MH) and Single-Headed (SH) DNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e10486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qloss(qs, y_true, y_pred):\n",
    "    q = tf.constant(np.array([qs]), dtype=tf.float32)\n",
    "    e = y_true - y_pred\n",
    "    v = tf.maximum(q * e, (q - 1) * e)\n",
    "    return K.mean(v)\n",
    "\n",
    "\n",
    "loss_10th_p = lambda y_true, y_pred: qloss(0.1, y_true, y_pred)\n",
    "loss_30th_p = lambda y_true, y_pred: qloss(0.3, y_true, y_pred)\n",
    "loss_50th_p = lambda y_true, y_pred: qloss(0.5, y_true, y_pred)\n",
    "loss_70th_p = lambda y_true, y_pred: qloss(0.7, y_true, y_pred)\n",
    "loss_90th_p = lambda y_true, y_pred: qloss(0.9, y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194efc8",
   "metadata": {},
   "source": [
    " Load in BM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fafd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/coconnor/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be055c2b",
   "metadata": {},
   "source": [
    "Quantile Forecast: Multi-Headed, Multi-Input DNN-RNN for quantile forecast in the BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d0e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features and labels from the data\n",
    "Y = dat.iloc[:, 0:16]  # Selecting the first 16 columns as labels\n",
    "X = dat.iloc[:, 16:]   # Selecting the columns from index 16 onwards as features\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train = X.iloc[:7250, :]     # Training features\n",
    "Y_train = Y.iloc[:7250, :]     # Training labels\n",
    "X_test = X.iloc[7250:8739, :]  # Testing features\n",
    "Y_test = Y.iloc[7250:8739, :]  # Testing labels\n",
    "\n",
    "# Extracting specific lagged features for LSTM and FFNN models\n",
    "rnn_train_LSTM_1 = X_train.loc[:,\"lag_-3x1\":\"lag_-50x1\"]\n",
    "rnn_train_LSTM_2 = X_train.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "rnn_train_LSTM_3 = X_train.loc[:,\"lag_-2x3\":\"lag_-49x3\"]\n",
    "rnn_train_LSTM_4 = X_train.loc[:,\"lag_0x6\":\"lag_-47x6\"]\n",
    "rnn_train_LSTM_5 = X_train.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "\n",
    "rnn_test_LSTM_1 = X_test.loc[:,\"lag_-3x1\": \"lag_-50x1\"]\n",
    "rnn_test_LSTM_2 = X_test.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "rnn_test_LSTM_3 = X_test.loc[:,\"lag_-2x3\": \"lag_-49x3\"]\n",
    "rnn_test_LSTM_4 = X_test.loc[:,\"lag_0x6\" : \"lag_-47x6\"]\n",
    "rnn_test_LSTM_5 = X_test.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "\n",
    "rnn_train_ffnn_1 = X_train.loc[:,\"lag_2x7\": \"lag_17x7\"]\n",
    "rnn_train_ffnn_2 = X_train.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "rnn_train_ffnn_3 = X_train.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "rnn_train_ffnn_4 = X_train.loc[:,\"lag_2x10\": \"lag_17x10\"]\n",
    "rnn_train_ffnn_5 = X_train.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "\n",
    "rnn_test_ffnn_1 = X_test.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "rnn_test_ffnn_2 = X_test.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "rnn_test_ffnn_3 = X_test.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "rnn_test_ffnn_4 = X_test.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "rnn_test_ffnn_5 = X_test.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "\n",
    "rnn_Y = Y_train.loc[:,\"lag_2y\": \"lag_17y\"]\n",
    "\n",
    "# Scaling features and labels\n",
    "X_scaler_LSTM_1 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_2 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_3 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_4 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler_ffnn_1 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_2 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_3 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_4 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train_LSTM_1 = X_scaler_LSTM_1.fit_transform(rnn_train_LSTM_1)\n",
    "rnn_scaled_train_LSTM_2 = X_scaler_LSTM_2.fit_transform(rnn_train_LSTM_2)\n",
    "rnn_scaled_train_LSTM_3 = X_scaler_LSTM_3.fit_transform(rnn_train_LSTM_3)\n",
    "rnn_scaled_train_LSTM_4 = X_scaler_LSTM_4.fit_transform(rnn_train_LSTM_4)\n",
    "rnn_scaled_train_LSTM_5 = X_scaler_LSTM_5.fit_transform(rnn_train_LSTM_5)\n",
    "\n",
    "rnn_scaled_train_ffnn_1 = X_scaler_ffnn_1.fit_transform(rnn_train_ffnn_1)\n",
    "rnn_scaled_train_ffnn_2 = X_scaler_ffnn_2.fit_transform(rnn_train_ffnn_2)\n",
    "rnn_scaled_train_ffnn_3 = X_scaler_ffnn_3.fit_transform(rnn_train_ffnn_3)\n",
    "rnn_scaled_train_ffnn_4 = X_scaler_ffnn_4.fit_transform(rnn_train_ffnn_4)\n",
    "rnn_scaled_train_ffnn_5 = X_scaler_ffnn_5.fit_transform(rnn_train_ffnn_5)\n",
    "\n",
    "Y_train_Scaled = Y_scaler.fit_transform(Y_train)\n",
    "Y_test_scaled = Y_scaler.transform(Y_test)\n",
    "\n",
    "# Preparing data for LSTM and FFNN models\n",
    "X_train_Scaled_LSTM = np.hstack(\n",
    "    (rnn_scaled_train_LSTM_1, rnn_scaled_train_LSTM_2, rnn_scaled_train_LSTM_3,\n",
    "     rnn_scaled_train_LSTM_4, rnn_scaled_train_LSTM_5)\n",
    ").reshape(rnn_train_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "X_train_Scaled_ffnn = np.hstack(\n",
    "    (rnn_scaled_train_ffnn_1, rnn_scaled_train_ffnn_2, rnn_scaled_train_ffnn_3,\n",
    "     rnn_scaled_train_ffnn_4, rnn_scaled_train_ffnn_5)\n",
    ").reshape(rnn_train_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled_LSTM = np.hstack(\n",
    "    (X_scaler_LSTM_1.transform(rnn_test_LSTM_1), X_scaler_LSTM_2.transform(rnn_test_LSTM_2),\n",
    "     X_scaler_LSTM_3.transform(rnn_test_LSTM_3), X_scaler_LSTM_4.transform(rnn_test_LSTM_4),\n",
    "     X_scaler_LSTM_5.transform(rnn_test_LSTM_5))\n",
    ").reshape(rnn_test_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled_ffnn = np.hstack(\n",
    "    (X_scaler_ffnn_1.transform(rnn_test_ffnn_1), X_scaler_ffnn_2.transform(rnn_test_ffnn_2),\n",
    "     X_scaler_ffnn_3.transform(rnn_test_ffnn_3), X_scaler_ffnn_4.transform(rnn_test_ffnn_4),\n",
    "     X_scaler_ffnn_5.transform(rnn_test_ffnn_5))\n",
    ").reshape(rnn_test_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "i_shape_lstm = (X_train_Scaled_LSTM.shape[1], X_train_Scaled_LSTM.shape[2])\n",
    "i_shape_ffnn = (X_train_Scaled_ffnn.shape[1], X_train_Scaled_ffnn.shape[2])\n",
    "\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                                       test_start_time: dt, test_end_time: dt):\n",
    "    \"\"\"\n",
    "    This function generates training and testing dataframes for LSTM and FFNN models.\n",
    "\n",
    "    Parameters:\n",
    "    participant_df (pd.DataFrame): DataFrame containing participant data.\n",
    "    train_start_time (dt): Start time for training data.\n",
    "    train_end_time (dt): End time for training data.\n",
    "    test_start_time (dt): Start time for testing data.\n",
    "    test_end_time (dt): End time for testing data.\n",
    "\n",
    "    Returns:\n",
    "    train_X_LSTM (np.ndarray): Training features for LSTM model.\n",
    "    train_X_ffnn (np.ndarray): Training features for FFNN model.\n",
    "    train_y (np.ndarray): Training labels.\n",
    "    test_X_LSTM (np.ndarray): Testing features for LSTM model.\n",
    "    test_X_ffnn (np.ndarray): Testing features for FFNN model.\n",
    "    test_y (pd.DataFrame): Testing labels.\n",
    "    test_df (pd.DataFrame): DataFrame containing testing data.\n",
    "    train_df (pd.DataFrame): DataFrame containing training data.\n",
    "    Y_scaler_n (MinMaxScaler): Scaler for labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # These are the dataframes that will be returned from the method.\n",
    "    train_X_LSTM = None\n",
    "    train_X_ffnn = None\n",
    "\n",
    "    train_y = None\n",
    "    test_X_LSTM = None\n",
    "    test_X_ffnn = None\n",
    "    test_y = None\n",
    "    test_df = None\n",
    "    train_df = None\n",
    "\n",
    "    try:\n",
    "\n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "\n",
    "        original_columns = list(participant_df.columns)\n",
    "        participant_df = participant_df.dropna()\n",
    "        date_format = \"%m/%d/%Y %H:%M\"\n",
    "        \n",
    "        # Selecting data within specified time range for training and testing\n",
    "        train_df = None\n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)\n",
    "        train_df = participant_df[\n",
    "            (participant_df.index >= train_start_time_str) & (participant_df.index < train_end_time_str)].copy(\n",
    "            deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\n",
    "                \"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")\n",
    "\n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)\n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format)\n",
    "        test_df = participant_df[\n",
    "            (participant_df.index >= test_start_time_str) & (participant_df.index < test_end_time_str)].copy(\n",
    "            deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\n",
    "                \"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")\n",
    "            \n",
    "        # Extracting lagged features for LSTM and FFNN models\n",
    "        rnn_train_LSTM_1 = train_df.loc[:,\"lag_-3x1\":\"lag_-50x1\"]\n",
    "        rnn_train_LSTM_2 = train_df.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "        rnn_train_LSTM_3 = train_df.loc[:,\"lag_-2x3\":\"lag_-49x3\"]\n",
    "        rnn_train_LSTM_4 = train_df.loc[:,\"lag_0x6\":\"lag_-47x6\"]\n",
    "        rnn_train_LSTM_5 = train_df.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "\n",
    "        rnn_test_LSTM_1 = test_df.loc[:,\"lag_-3x1\": \"lag_-50x1\"]\n",
    "        rnn_test_LSTM_2 = test_df.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "        rnn_test_LSTM_3 = test_df.loc[:,\"lag_-2x3\": \"lag_-49x3\"]\n",
    "        rnn_test_LSTM_4 = test_df.loc[:,\"lag_0x6\" : \"lag_-47x6\"]\n",
    "        rnn_test_LSTM_5 = test_df.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "        \n",
    "        rnn_train_ffnn_1 = train_df.loc[:,\"lag_2x7\": \"lag_17x7\"]\n",
    "        rnn_train_ffnn_2 = train_df.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "        rnn_train_ffnn_3 = train_df.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "        rnn_train_ffnn_4 = train_df.loc[:,\"lag_2x10\": \"lag_17x10\"]\n",
    "        rnn_train_ffnn_5 = train_df.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "        \n",
    "        rnn_test_ffnn_1 = test_df.loc[:,\"lag_2x7\": \"lag_17x7\"]\n",
    "        rnn_test_ffnn_2 = test_df.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "        rnn_test_ffnn_3 = test_df.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "        rnn_test_ffnn_4 = test_df.loc[:,\"lag_2x10\": \"lag_17x10\"]\n",
    "        rnn_test_ffnn_5 = test_df.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "\n",
    "        rnn_Y = train_df.loc[:,\"lag_2y\": \"lag_17y\"]\n",
    "\n",
    "        X_scaler_LSTM_1 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_LSTM_2 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_LSTM_3 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_LSTM_4 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_LSTM_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "        X_scaler_ffnn_1 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_ffnn_2 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_ffnn_3 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_ffnn_4 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_ffnn_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "        rnn_scaled_train_LSTM_1 = X_scaler_LSTM_1.fit_transform(rnn_train_LSTM_1)\n",
    "        rnn_scaled_train_LSTM_2 = X_scaler_LSTM_2.fit_transform(rnn_train_LSTM_2)\n",
    "        rnn_scaled_train_LSTM_3 = X_scaler_LSTM_3.fit_transform(rnn_train_LSTM_3)\n",
    "        rnn_scaled_train_LSTM_4 = X_scaler_LSTM_4.fit_transform(rnn_train_LSTM_4)\n",
    "        rnn_scaled_train_LSTM_5 = X_scaler_LSTM_5.fit_transform(rnn_train_LSTM_5)\n",
    "\n",
    "        rnn_scaled_train_ffnn_1 = X_scaler_ffnn_1.fit_transform(rnn_train_ffnn_1)\n",
    "        rnn_scaled_train_ffnn_2 = X_scaler_ffnn_2.fit_transform(rnn_train_ffnn_2)\n",
    "        rnn_scaled_train_ffnn_3 = X_scaler_ffnn_3.fit_transform(rnn_train_ffnn_3)\n",
    "        rnn_scaled_train_ffnn_4 = X_scaler_ffnn_4.fit_transform(rnn_train_ffnn_4)\n",
    "        rnn_scaled_train_ffnn_5 = X_scaler_ffnn_5.fit_transform(rnn_train_ffnn_5)\n",
    "\n",
    "        train_y = Y_scaler.fit_transform(rnn_Y)\n",
    "        Y_scaler_n = Y_scaler.fit(rnn_Y)\n",
    "\n",
    "        train_X_LSTM = np.hstack(\n",
    "            (rnn_scaled_train_LSTM_1, rnn_scaled_train_LSTM_2, rnn_scaled_train_LSTM_3,\n",
    "             rnn_scaled_train_LSTM_4, rnn_scaled_train_LSTM_5)\n",
    "        ).reshape(rnn_train_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "        train_X_ffnn = np.hstack(\n",
    "            (rnn_scaled_train_ffnn_1, rnn_scaled_train_ffnn_2, rnn_scaled_train_ffnn_3,\n",
    "             rnn_scaled_train_ffnn_4, rnn_scaled_train_ffnn_5)\n",
    "        ).reshape(rnn_train_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "        test_X_LSTM = np.hstack(\n",
    "            (X_scaler_LSTM_1.transform(rnn_test_LSTM_1), X_scaler_LSTM_2.transform(rnn_test_LSTM_2),\n",
    "             X_scaler_LSTM_3.transform(rnn_test_LSTM_3), X_scaler_LSTM_4.transform(rnn_test_LSTM_4),\n",
    "             X_scaler_LSTM_5.transform(rnn_test_LSTM_5))\n",
    "        ).reshape(rnn_test_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "        test_X_ffnn = np.hstack(\n",
    "            (X_scaler_ffnn_1.transform(rnn_test_ffnn_1), X_scaler_ffnn_2.transform(rnn_test_ffnn_2),\n",
    "             X_scaler_ffnn_3.transform(rnn_test_ffnn_3), X_scaler_ffnn_4.transform(rnn_test_ffnn_4),\n",
    "             X_scaler_ffnn_5.transform(rnn_test_ffnn_5))\n",
    "        ).reshape(rnn_test_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "        test_y = test_df.iloc[:, 0:16]\n",
    "\n",
    "        return train_X_LSTM, train_X_ffnn, train_y, test_X_LSTM, test_X_ffnn, test_y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X_LSTM, train_X_ffnn, train_y, test_X_LSTM, test_X_ffnn, test_y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "\n",
    "def fit_multitarget_model(model, X_train_LSTM, X_train_ffnn, Y_train, X_test_LSTM, X_test_ffnn, Y_test,\n",
    "                          actuals_and_forecast_df, targets, Y_scaler_n):\n",
    "    \"\"\"\n",
    "    Fits the model to the training data.\n",
    "    Then uses the test data to produce a forecast.\n",
    "    Returns a dataframe containing the forecasts and actual values for each of the target variables.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        Model object i.e. randomforestregressor, linear model or other.\n",
    "    X_train : dataframe\n",
    "        The explanatory variables for the train/calibration set, numeric columns may already have been scaled.\n",
    "    Y_train : dataframe\n",
    "        The target variables for the train/calibration set. Might/Mighn't be scaled.\n",
    "    X_test : dataframe\n",
    "        The explanatory variables for the test set, columns may be scaled. It will comprise of 24 rows (1 row for each delivery period in the trading day).\n",
    "    Y_test : dataframe\n",
    "        The target variables for the test set i.e. what we would like to forecast. Similar to the previous bullet point, the dataframe will contain 24 rows.\n",
    "    actuals_and_forecast_df : dataframe\n",
    "        Initially the dataframe will only contain the actual values for each of the targets. At the end of the method it will also contain the forecast values.\n",
    "    targets : [str]\n",
    "        These are the items that we want to predict/forecast.\n",
    "    scale_target_variables: boolean\n",
    "       The target vector, do we want to scale it?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Returns a dataframe containing the forecasts and actual values for each of the target variables i.e. test set forecast and actuals.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "        Y_scaler = Y_scaler.fit(Y_train)\n",
    "        cols = Y.columns.values.tolist()\n",
    "        \n",
    "        es = EarlyStopping(monitor='val_loss', mode='min',  patience=20)\n",
    "        model.fit([X_train_LSTM, X_train_ffnn], Y_train, epochs=200, verbose=2,  callbacks=[es], validation_split=0.10)\n",
    "        model_test_predictions = None\n",
    "        model_test_predictions = pd.DataFrame(Y_scaler_n.inverse_transform(np.array(model.predict([X_test_LSTM, X_test_ffnn])).reshape(5, 16), columns=cols))\n",
    "            \n",
    "\n",
    "        print(model_test_predictions)\n",
    "        print(\"test number of observations: \" + str(len(Y_test)))\n",
    "\n",
    "        \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_10\"] = model_test_predictions.iloc[:1,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_30\"] = model_test_predictions.iloc[1:2,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_50\"] = model_test_predictions.iloc[2:3,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist()\n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_70\"] = model_test_predictions.iloc[3:4,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "           \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_90\"] = model_test_predictions.iloc[4:,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist()\n",
    "\n",
    "\n",
    "        return actuals_and_forecast_df\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def rolling_walk_forward_validation(model, data, targets, start_time, end_time, training_days, path):\n",
    "    \"\"\"\n",
    "    This method implements the rolling walk forward validation process.\n",
    "    That is,\n",
    "        (a) fit the model on the train data\n",
    "        (b) use the fitted model on the test explanatory variables i.e. forecast 1 day ahead.\n",
    "        (c) Move the training and test datasets forward by 1 day and repeat.\n",
    "    The method will produce\n",
    "        (1) A csv containing the forecast and actual target values i.e. test set output over the horizon of interest.\n",
    "        (2) For each target variable, a graph of the actual and forecast values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: model\n",
    "        The model that will be used to train the data and produce the 1 day ahead forecasts\n",
    "    data: dataframe\n",
    "        Participant dataframe containing the explanatory and target variables.\n",
    "    explanatory_variables_of_interest : [str]\n",
    "        The columns in data that will be used as explanatory variables when fitting the model.\n",
    "        If there are categorical variables we want to use as explanatory variables, they are incorporated via the features_to_encode argument.\n",
    "    targets: [str]\n",
    "        The columns in data that we want to predict/forecast.\n",
    "    features_to_encode: [str]\n",
    "        If there are variables in data that we would like to apply one hot encoding to, we list them here. These one hot encoded vectors are then used as explanatory variables.\n",
    "    prefix_to_include: [str]\n",
    "        Just a string which will be used to name the columns if we apply one hot encoding (related to the features_to_encode argument).\n",
    "     start_time: dt\n",
    "       We will produce a forecast on unseen data for each trading period between [start_time, end_time].\n",
    "     end_time: dt\n",
    "       See previous point.\n",
    "    training_days: int\n",
    "       The number of training days (negative integer expected).\n",
    "    path, unit_name, scenario: str\n",
    "       The combination of path + unit_name + scenario indicate where the csv will be output to.\n",
    "    scale_explanatory_variables: boolean\n",
    "       The explanatory variables, do we want to scale them?\n",
    "    scale_target_variables: boolean\n",
    "       The target variables, do we want to scale them?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Output will include a  csv of the forecast/actual target values and a graph of the same.\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)\n",
    "        results = pd.DataFrame()\n",
    "\n",
    "        # Each time we\n",
    "        # (a) fit the model on the calibration/train data\n",
    "        # (b) apply it to the test data i.e. forecast 1 day ahead for DAM, 8 hours ahead for BM.\n",
    "        # Repeat.\n",
    "        date_format = \"%m/%d/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "\n",
    "        while start_time < end_time:\n",
    "\n",
    "            # Train interval\n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time\n",
    "\n",
    "            # Test interval, the test period is always the day ahead forecast\n",
    "            test_start_time = train_end_time + td(hours=8)\n",
    "            test_end_time = test_start_time + td(minutes=30)\n",
    "\n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "\n",
    "            # Generate the calibration and test dataframes.\n",
    "            train_X_LSTM, train_X_ffnn, train_y, test_X_LSTM, test_X_ffnn, test_y, test_df, train_df, Y_scaler_n = generate_train_and_test_dataframes(\n",
    "                participant_df=dat, train_start_time=train_start_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                train_end_time=train_end_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                test_start_time=test_start_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                test_end_time=test_end_time.strftime(\"%m/%d/%Y %H:%M\"))\n",
    "\n",
    "            if train_X_LSTM is None or len(train_X_LSTM) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(\n",
    "                    train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            if test_X_LSTM is None or len(test_X_LSTM) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(\n",
    "                    test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            # Fit the model to the train datasets, produce a forecast and return a dataframe containing the forecast/actuals.\n",
    "            actuals_and_forecast_df = fit_multitarget_model(model=model, Y_scaler_n=Y_scaler_n,\n",
    "                                                            X_train_LSTM=train_X_LSTM, X_train_ffnn=train_X_ffnn,\n",
    "                                                            Y_train=train_y,\n",
    "                                                            X_test_LSTM=test_X_LSTM, X_test_ffnn=test_X_ffnn,\n",
    "                                                            Y_test=test_y,\n",
    "                                                            actuals_and_forecast_df=test_df.iloc[:, 0:16],\n",
    "                                                            targets=Y.columns.values.tolist())\n",
    "\n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "\n",
    "            start_time = start_time + td(hours=8)\n",
    "\n",
    "        results.to_csv(path + \".csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "def create_model():    \n",
    "    visible1 = Input(shape=(i_shape_lstm))\n",
    "    x1=visible1\n",
    "    \n",
    "    for i in range(1):\n",
    "        x1= LSTM(64, return_sequences= True, activation='sigmoid', input_shape=i_shape_lstm)(x1)\n",
    "    x1 = Dropout(0.222222)(x1)\n",
    "    \n",
    "    input_1 = Flatten()(x1)   \n",
    "    \n",
    "    \n",
    "    visible2 = Input(shape=(i_shape_ffnn))   \n",
    "    x2= visible2 \n",
    "    \n",
    "    for i in range(2):\n",
    "        x2 = Dense(64, 'tanh')(x2)\n",
    "    x2 = Dropout(0.088889)(x2)\n",
    "    \n",
    "    input_2 = Flatten()(x2) \n",
    "            \n",
    "    \n",
    "    merged = concatenate([input_1, input_2])\n",
    "    \n",
    "    a = Dense(128, 'tanh')(merged)\n",
    "    b = Dense(128, 'tanh')(merged)\n",
    "    c = Dense(128, 'tanh')(merged)\n",
    "    d = Dense(128, 'tanh')(merged)\n",
    "    e = Dense(128, 'tanh')(merged)\n",
    "        \n",
    "            \n",
    "    for i in range(1):\n",
    "            a = Dense(128, 'relu')(a) \n",
    "    output_1 =  Dense(16, name='out_10')(a)\n",
    "    \n",
    "    for i in range(1):\n",
    "            b = Dense(128, 'relu')(b) \n",
    "    output_2 =  Dense(16, name='out_30')(b)\n",
    "    \n",
    "    for i in range(1):\n",
    "            c = Dense(128, 'relu')(c) \n",
    "    output_3 =  Dense(16, name='out_50')(c)\n",
    "    \n",
    "    for i in range(1):\n",
    "            d = Dense(128, 'relu')(d) \n",
    "    output_4 =  Dense(16, name='out_70')(d)\n",
    "    \n",
    "    for i in range(1):\n",
    "            e = Dense(128, 'relu')(e) \n",
    "    output_5 =  Dense(16, name='out_90')(e)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[visible1, visible2], outputs=[output_1, output_2, output_3, output_4, output_5])    \n",
    "    opt = Adam(learning_rate= 0.000100)\n",
    "\n",
    "    model.compile(loss=[loss_10th_p, loss_30th_p, loss_50th_p, loss_70th_p, loss_90th_p], optimizer=opt)\n",
    "    return model\n",
    "\n",
    "mmo = KerasRegressor(build_fn=create_model, epochs=200, batch_size=32, verbose=2)\n",
    "\n",
    "rolling_walk_forward_validation(model=mmo, data=dat, start_time='6/1/2020 00:00', end_time='9/1/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(), training_days=-210,\n",
    "                                path=\"/home/coconnor/MH_Q_1-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a992432",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c02aab8",
   "metadata": {},
   "source": [
    "Point Forecast: Multi-Headed DNN-RNN for forecasting the BM price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0239cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_multitarget_model(model, X_train_LSTM, X_train_ffnn, Y_train, X_test_LSTM, X_test_ffnn, Y_test,\n",
    "                          actuals_and_forecast_df, targets, Y_scaler_n):\n",
    "\n",
    "    try:\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "        Y_scaler = Y_scaler.fit(Y_train)\n",
    "        cols = Y.columns.values.tolist()\n",
    "\n",
    "        model.fit([X_train_LSTM, X_train_ffnn], Y_train)\n",
    "        model_test_predictions = None\n",
    "        model_test_predictions = pd.DataFrame(\n",
    "            Y_scaler_n.inverse_transform(model.predict([X_test_LSTM, X_test_ffnn]).reshape(1, 16)), columns=cols,\n",
    "            index=Y_test.index)\n",
    "        model_test_mse = mean_squared_error(Y_test, model_test_predictions)\n",
    "        model_test_rmse = round(np.sqrt(model_test_mse), 2)\n",
    "        model_test_mae = round(mean_absolute_error(Y_test, model_test_predictions), 2)\n",
    "\n",
    "        for i in range(0, len(cols)):\n",
    "            actuals_and_forecast_df[cols[i] + \"_Forecast\"] = model_test_predictions.iloc[:, i].tolist() if len(\n",
    "                cols) > 1 else model_test_predictions.tolist()\n",
    "            predictor_test_mse = mean_squared_error(Y_test[cols[i]], model_test_predictions.iloc[:, i]) if len(\n",
    "                cols) > 1 else mean_squared_error(Y_test[cols[i]], model_test_predictions.tolist())\n",
    "            predictor_test_rmse = round(np.sqrt(predictor_test_mse), 2)\n",
    "            predictor_test_mae = round(mean_absolute_error(Y_test[cols[i]], model_test_predictions.iloc[:, i]),\n",
    "                                       2) if len(cols) > 1 else round(\n",
    "                mean_absolute_error(Y_test[cols[i]], model_test_predictions.tolist()), 2)\n",
    "\n",
    "        Error_i = ([model_test_rmse, model_test_mae])\n",
    "        actuals_and_forecast_df = actuals_and_forecast_df.append(Error_i)\n",
    "\n",
    "\n",
    "\n",
    "        return actuals_and_forecast_df\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def rolling_walk_forward_validation(model, data, targets, start_time, end_time, training_days, path):\n",
    "\n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)\n",
    "        results = pd.DataFrame()\n",
    "\n",
    "\n",
    "        date_format = \"%m/%d/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "\n",
    "        while start_time < end_time:\n",
    "\n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time\n",
    "\n",
    "            test_start_time = train_end_time + td(hours=8)\n",
    "            test_end_time = test_start_time + td(minutes=30)\n",
    "\n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "\n",
    "            train_X_LSTM, train_X_ffnn, train_y, test_X_LSTM, test_X_ffnn, test_y, test_df, train_df, Y_scaler_n = generate_train_and_test_dataframes(\n",
    "                participant_df=dat, train_start_time=train_start_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                train_end_time=train_end_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                test_start_time=test_start_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                test_end_time=test_end_time.strftime(\"%m/%d/%Y %H:%M\"))\n",
    "\n",
    "            if train_X_LSTM is None or len(train_X_LSTM) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(\n",
    "                    train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            if test_X_LSTM is None or len(test_X_LSTM) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(\n",
    "                    test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            actuals_and_forecast_df = fit_multitarget_model(model=model, Y_scaler_n=Y_scaler_n,\n",
    "                                                            X_train_LSTM=train_X_LSTM, X_train_ffnn=train_X_ffnn,\n",
    "                                                            Y_train=train_y,\n",
    "                                                            X_test_LSTM=test_X_LSTM, X_test_ffnn=test_X_ffnn,\n",
    "                                                            Y_test=test_y,\n",
    "                                                            actuals_and_forecast_df=test_df.iloc[:, 0:16],\n",
    "                                                            targets=Y.columns.values.tolist())\n",
    "\n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "\n",
    "            start_time = start_time + td(hours=8)\n",
    "\n",
    "        results.to_csv(path + \".csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "def create_model():    \n",
    "            visible1 = Input(shape=(i_shape_lstm))            \n",
    "            dense1 = LSTM(128, return_sequences= True, activation='tanh' , input_shape=i_shape_lstm)(visible1)\n",
    "            dense2 = LSTM(128, return_sequences= True, activation='tanh' , input_shape=i_shape_lstm)(dense1)\n",
    "            do_lstm = Dropout(0.044444, seed=123)(dense2)\n",
    "            dense3 = LSTM(128)(do_lstm)\n",
    "            flat1 = Flatten()(dense3)    \n",
    "    \n",
    "            visible2 = Input(shape=(i_shape_ffnn))        \n",
    "            dense5 = Dense(16, activation='relu')(visible2)        \n",
    "            dense6 = Dense(16, activation=LeakyReLU)(dense5)\n",
    "            do_ffnn = Dropout(0.200000, seed=123)(dense6)\n",
    "            dense7 = Dense(16, activation='relu')(do_ffnn)\n",
    "            flat2 = Flatten()(dense7)\n",
    "    \n",
    "            merged = concatenate([flat1, flat2])\n",
    "            dense_f = Dense(256, activation='relu')(merged)        \n",
    "            outputs = Dense(16)(dense_f)\n",
    "\n",
    "            model = Model(inputs=[visible1, visible2], outputs=outputs)    \n",
    "            opt = Adam(lr=0.004522)\n",
    "            model.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mean_absolute_error'])\n",
    "            return model\n",
    "        \n",
    "es = EarlyStopping(monitor='mean_absolute_error', mode='min', verbose=0, patience=20)\n",
    "mmo = KerasRegressor(build_fn=create_model, epochs=500, batch_size=48, verbose=2, callbacks=[es])\n",
    "\n",
    "rolling_walk_forward_validation(model=mmo, data=dat, start_time='9/1/2020 00:00', end_time='12/1/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(), training_days=-300,\n",
    "                                path=\"/home/coconnor/BM_results_MH_1-13\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3215004",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8082ba84",
   "metadata": {},
   "source": [
    "Point Forecast: SH DNN in the BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0297fa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[:7250,:]\n",
    "Y_train=Y.iloc[:7250,:]\n",
    "X_test=X.iloc[7250:8739,:]\n",
    "Y_test=Y.iloc[7250:8739,:]\n",
    "\n",
    "rnn_train1_a=X_train.loc[:,\"lag_-3x1\":\"lag_-18x1\"]\n",
    "rnn_train1_b=X_train.loc[:,\"lag_-19x1\":\"lag_-34x1\"]\n",
    "rnn_train1_c=X_train.loc[:,\"lag_-35x1\":\"lag_-50x1\"]        \n",
    "rnn_train2_a=X_train.loc[:,\"lag_-3x2\":\"lag_-18x2\"]\n",
    "rnn_train2_b=X_train.loc[:,\"lag_-19x2\":\"lag_-34x2\"]\n",
    "rnn_train2_c=X_train.loc[:,\"lag_-35x2\":\"lag_-50x2\"]        \n",
    "rnn_train3_a=X_train.loc[:,\"lag_-2x3\":\"lag_-17x3\"]\n",
    "rnn_train3_b=X_train.loc[:,\"lag_-18x3\":\"lag_-33x3\"]\n",
    "rnn_train3_c=X_train.loc[:,\"lag_-34x3\":\"lag_-49x3\"]      \n",
    "rnn_train4_a=X_train.loc[:,\"lag_0x6\":\"lag_-15x6\"]\n",
    "rnn_train4_b=X_train.loc[:,\"lag_-16x6\":\"lag_-31x6\"]\n",
    "rnn_train4_c=X_train.loc[:,\"lag_-32x6\":\"lag_-47x6\"]        \n",
    "rnn_train5_a=X_train.loc[:,\"lag_-2x12\":\"lag_-17x12\"]\n",
    "rnn_train5_b=X_train.loc[:,\"lag_-18x12\":\"lag_-33x12\"]\n",
    "rnn_train5_c=X_train.loc[:,\"lag_-34x12\":\"lag_-49x12\"]        \n",
    "rnn_train6=X_train.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "rnn_train7=X_train.loc[:,\"lag_2x8\":\"lag_17x8\"]\n",
    "rnn_train8=X_train.loc[:,\"lag_2x9\":\"lag_17x9\"]\n",
    "rnn_train9=X_train.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "rnn_train10=X_train.loc[:,\"lag_2x11\":\"lag_17x11\"]        \n",
    "        \n",
    "rnn_test1_a= X_test.loc[:,\"lag_-3x1\":\"lag_-18x1\"]\n",
    "rnn_test1_b= X_test.loc[:,\"lag_-19x1\":\"lag_-34x1\"]\n",
    "rnn_test1_c= X_test.loc[:,\"lag_-35x1\":\"lag_-50x1\"]\n",
    "rnn_test2_a= X_test.loc[:,\"lag_-3x2\":\"lag_-18x2\"]\n",
    "rnn_test2_b= X_test.loc[:,\"lag_-19x2\":\"lag_-34x2\"]\n",
    "rnn_test2_c= X_test.loc[:,\"lag_-35x2\":\"lag_-50x2\"]\n",
    "rnn_test3_a= X_test.loc[:,\"lag_-2x3\":\"lag_-17x3\"]\n",
    "rnn_test3_b= X_test.loc[:,\"lag_-18x3\":\"lag_-33x3\"]\n",
    "rnn_test3_c= X_test.loc[:,\"lag_-34x3\":\"lag_-49x3\"]\n",
    "rnn_test4_a= X_test.loc[:,\"lag_0x6\":\"lag_-15x6\"]\n",
    "rnn_test4_b= X_test.loc[:,\"lag_-16x6\":\"lag_-31x6\"]\n",
    "rnn_test4_c= X_test.loc[:,\"lag_-32x6\":\"lag_-47x6\"]\n",
    "rnn_test5_a= X_test.loc[:,\"lag_-2x12\":\"lag_-17x12\"]\n",
    "rnn_test5_b= X_test.loc[:,\"lag_-18x12\":\"lag_-33x12\"]\n",
    "rnn_test5_c= X_test.loc[:,\"lag_-34x12\":\"lag_-49x12\"]\n",
    "rnn_test6=X_test.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "rnn_test7=X_test.loc[:,\"lag_2x8\":\"lag_17x8\"]\n",
    "rnn_test8=X_test.loc[:,\"lag_2x9\":\"lag_17x9\"]\n",
    "rnn_test9=X_test.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "rnn_test10=X_test.loc[:,\"lag_2x11\":\"lag_17x11\"]\n",
    "\n",
    "rnn_Y=Y_train.loc[:,\"lag_2y\" : \"lag_17y\"]\n",
    "\n",
    "\n",
    "X_scaler1_a = preprocessing.MinMaxScaler()\n",
    "X_scaler1_b = preprocessing.MinMaxScaler()\n",
    "X_scaler1_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler2_a = preprocessing.MinMaxScaler()\n",
    "X_scaler2_b = preprocessing.MinMaxScaler()\n",
    "X_scaler2_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler3_a = preprocessing.MinMaxScaler()\n",
    "X_scaler3_b = preprocessing.MinMaxScaler()\n",
    "X_scaler3_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler4_a = preprocessing.MinMaxScaler()\n",
    "X_scaler4_b = preprocessing.MinMaxScaler()\n",
    "X_scaler4_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler5_a = preprocessing.MinMaxScaler()\n",
    "X_scaler5_b = preprocessing.MinMaxScaler()\n",
    "X_scaler5_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "\n",
    "X_scaler6 = preprocessing.MinMaxScaler()\n",
    "X_scaler7 = preprocessing.MinMaxScaler()\n",
    "X_scaler8 = preprocessing.MinMaxScaler()\n",
    "X_scaler9 = preprocessing.MinMaxScaler()\n",
    "X_scaler10 = preprocessing.MinMaxScaler()\n",
    "\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train1_a = X_scaler1_a.fit_transform(rnn_train1_a)\n",
    "rnn_scaled_train1_b = X_scaler1_b.fit_transform(rnn_train1_b)\n",
    "rnn_scaled_train1_c = X_scaler1_c.fit_transform(rnn_train1_c)\n",
    "\n",
    "rnn_scaled_train2_a = X_scaler2_a.fit_transform(rnn_train2_a)\n",
    "rnn_scaled_train2_b = X_scaler2_b.fit_transform(rnn_train2_b)\n",
    "rnn_scaled_train2_c = X_scaler2_c.fit_transform(rnn_train2_c)\n",
    "\n",
    "rnn_scaled_train3_a = X_scaler3_a.fit_transform(rnn_train3_a)\n",
    "rnn_scaled_train3_b = X_scaler3_b.fit_transform(rnn_train3_b)\n",
    "rnn_scaled_train3_c = X_scaler3_c.fit_transform(rnn_train3_c)\n",
    "\n",
    "rnn_scaled_train4_a = X_scaler4_a.fit_transform(rnn_train4_a)\n",
    "rnn_scaled_train4_b = X_scaler4_b.fit_transform(rnn_train4_b)\n",
    "rnn_scaled_train4_c = X_scaler4_c.fit_transform(rnn_train4_c)\n",
    "\n",
    "rnn_scaled_train5_a = X_scaler5_a.fit_transform(rnn_train5_a)\n",
    "rnn_scaled_train5_b = X_scaler5_b.fit_transform(rnn_train5_b)\n",
    "rnn_scaled_train5_c = X_scaler5_c.fit_transform(rnn_train5_c)\n",
    "\n",
    "rnn_scaled_train6 = X_scaler6.fit_transform(rnn_train6)\n",
    "rnn_scaled_train7 = X_scaler7.fit_transform(rnn_train7)\n",
    "rnn_scaled_train8 = X_scaler8.fit_transform(rnn_train8)\n",
    "rnn_scaled_train9 = X_scaler9.fit_transform(rnn_train9)\n",
    "rnn_scaled_train10 = X_scaler10.fit_transform(rnn_train10)\n",
    "\n",
    "Y_train_Scaled   = Y_scaler.fit_transform(Y_train)\n",
    "\n",
    "X_train_Scaled = np.hstack(\n",
    "    (rnn_scaled_train1_a, rnn_scaled_train1_b, rnn_scaled_train1_c, rnn_scaled_train2_a, rnn_scaled_train2_b, rnn_scaled_train2_c,\n",
    "     rnn_scaled_train3_a, rnn_scaled_train3_b, rnn_scaled_train3_c, rnn_scaled_train4_a, rnn_scaled_train4_b, rnn_scaled_train4_c,\n",
    "     rnn_scaled_train5_a, rnn_scaled_train5_b, rnn_scaled_train5_c,rnn_scaled_train6,rnn_scaled_train7, rnn_scaled_train8,\n",
    "     rnn_scaled_train9, rnn_scaled_train10)\n",
    ").reshape(rnn_train6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled = np.hstack(\n",
    "    (X_scaler1_a.transform(rnn_test1_a),X_scaler1_b.transform(rnn_test1_b),X_scaler1_c.transform(rnn_test1_c),\n",
    "     X_scaler2_a.transform(rnn_test2_a),X_scaler2_b.transform(rnn_test2_b),X_scaler2_c.transform(rnn_test2_c),\n",
    "     X_scaler3_a.transform(rnn_test3_a),X_scaler3_b.transform(rnn_test3_b),X_scaler3_c.transform(rnn_test3_c),\n",
    "     X_scaler4_a.transform(rnn_test4_a),X_scaler4_b.transform(rnn_test4_b),X_scaler4_c.transform(rnn_test4_c),\n",
    "     X_scaler5_a.transform(rnn_test5_a),X_scaler5_b.transform(rnn_test5_b),X_scaler5_c.transform(rnn_test5_c),\n",
    "     X_scaler6.transform(rnn_test6),X_scaler7.transform(rnn_test7),X_scaler8.transform(rnn_test8),\n",
    "     X_scaler9.transform(rnn_test9), X_scaler10.transform(rnn_test10))\n",
    ").reshape(rnn_test6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "\n",
    "i_shape=(X_train_Scaled.shape[1], X_train_Scaled.shape[2])\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                        test_start_time: dt, test_end_time: dt):\n",
    "\n",
    "  \n",
    "    train_X = None\n",
    "    train_y = None \n",
    "    test_X = None \n",
    "    test_y = None \n",
    "    test_df = None\n",
    "    train_df = None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "        \n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        participant_df = participant_df.dropna()\n",
    "        \n",
    "        date_format=\"%m/%d/%Y %H:%M\"\n",
    "      \n",
    "        train_df = None\n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)\n",
    "        train_df = participant_df[(participant_df.index>=train_start_time_str) & (participant_df.index<train_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")            \n",
    "\n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)    \n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format) \n",
    "        test_df = participant_df[(participant_df.index>=test_start_time_str) & (participant_df.index<test_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")            \n",
    "\n",
    "        \n",
    "        rnn_train1_a=train_df.loc[:,\"lag_-3x1\":\"lag_-18x1\"]\n",
    "        rnn_train1_b=train_df.loc[:,\"lag_-19x1\":\"lag_-34x1\"]\n",
    "        rnn_train1_c=train_df.loc[:,\"lag_-35x1\":\"lag_-50x1\"]\n",
    "        \n",
    "        rnn_train2_a=train_df.loc[:,\"lag_-3x2\":\"lag_-18x2\"]\n",
    "        rnn_train2_b=train_df.loc[:,\"lag_-19x2\":\"lag_-34x2\"]\n",
    "        rnn_train2_c=train_df.loc[:,\"lag_-35x2\":\"lag_-50x2\"]\n",
    "        \n",
    "        rnn_train3_a=train_df.loc[:,\"lag_-2x3\":\"lag_-17x3\"]\n",
    "        rnn_train3_b=train_df.loc[:,\"lag_-18x3\":\"lag_-33x3\"]\n",
    "        rnn_train3_c=train_df.loc[:,\"lag_-34x3\":\"lag_-49x3\"]\n",
    "        \n",
    "        rnn_train4_a=train_df.loc[:,\"lag_0x6\":\"lag_-15x6\"]\n",
    "        rnn_train4_b=train_df.loc[:,\"lag_-16x6\":\"lag_-31x6\"]\n",
    "        rnn_train4_c=train_df.loc[:,\"lag_-32x6\":\"lag_-47x6\"]\n",
    "        \n",
    "        rnn_train5_a=train_df.loc[:,\"lag_-2x12\":\"lag_-17x12\"]\n",
    "        rnn_train5_b=train_df.loc[:,\"lag_-18x12\":\"lag_-33x12\"]\n",
    "        rnn_train5_c=train_df.loc[:,\"lag_-34x12\":\"lag_-49x12\"]\n",
    "        \n",
    "        rnn_train6=train_df.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "        rnn_train7=train_df.loc[:,\"lag_2x8\":\"lag_17x8\"]\n",
    "        rnn_train8=train_df.loc[:,\"lag_2x9\":\"lag_17x9\"]\n",
    "        rnn_train9=train_df.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "        rnn_train10=train_df.loc[:,\"lag_2x11\":\"lag_17x11\"]\n",
    "\n",
    "        rnn_test1_a=test_df.loc[:,\"lag_-3x1\":\"lag_-18x1\"]\n",
    "        rnn_test1_b=test_df.loc[:,\"lag_-19x1\":\"lag_-34x1\"]\n",
    "        rnn_test1_c=test_df.loc[:,\"lag_-35x1\":\"lag_-50x1\"]\n",
    "\n",
    "        rnn_test2_a=test_df.loc[:,\"lag_-3x2\":\"lag_-18x2\"]\n",
    "        rnn_test2_b=test_df.loc[:,\"lag_-19x2\":\"lag_-34x2\"]\n",
    "        rnn_test2_c=test_df.loc[:,\"lag_-35x2\":\"lag_-50x2\"]\n",
    "\n",
    "        rnn_test3_a=test_df.loc[:,\"lag_-2x3\":\"lag_-17x3\"]\n",
    "        rnn_test3_b=test_df.loc[:,\"lag_-18x3\":\"lag_-33x3\"]\n",
    "        rnn_test3_c=test_df.loc[:,\"lag_-34x3\":\"lag_-49x3\"]\n",
    "\n",
    "        rnn_test4_a=test_df.loc[:,\"lag_0x6\":\"lag_-15x6\"]\n",
    "        rnn_test4_b=test_df.loc[:,\"lag_-16x6\":\"lag_-31x6\"]\n",
    "        rnn_test4_c=test_df.loc[:,\"lag_-32x6\":\"lag_-47x6\"]\n",
    "\n",
    "        rnn_test5_a=test_df.loc[:,\"lag_-2x12\":\"lag_-17x12\"]\n",
    "        rnn_test5_b=test_df.loc[:,\"lag_-18x12\":\"lag_-33x12\"]\n",
    "        rnn_test5_c=test_df.loc[:,\"lag_-34x12\":\"lag_-49x12\"]\n",
    "\n",
    "        rnn_test6=test_df.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "        rnn_test7=test_df.loc[:,\"lag_2x8\":\"lag_17x8\"]\n",
    "        rnn_test8=test_df.loc[:,\"lag_2x9\":\"lag_17x9\"]\n",
    "        rnn_test9=test_df.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "        rnn_test10=test_df.loc[:,\"lag_2x11\":\"lag_17x11\"]\n",
    "\n",
    "        rnn_Y=train_df.loc[:,\"lag_2y\" : \"lag_17y\"]\n",
    "        \n",
    "        \n",
    "        X_scaler1_a = preprocessing.MinMaxScaler()\n",
    "        X_scaler1_b = preprocessing.MinMaxScaler()\n",
    "        X_scaler1_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "        X_scaler2_a = preprocessing.MinMaxScaler()\n",
    "        X_scaler2_b = preprocessing.MinMaxScaler()\n",
    "        X_scaler2_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "        X_scaler3_a = preprocessing.MinMaxScaler()\n",
    "        X_scaler3_b = preprocessing.MinMaxScaler()\n",
    "        X_scaler3_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "        X_scaler4_a = preprocessing.MinMaxScaler()\n",
    "        X_scaler4_b = preprocessing.MinMaxScaler()\n",
    "        X_scaler4_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "        X_scaler5_a = preprocessing.MinMaxScaler()\n",
    "        X_scaler5_b = preprocessing.MinMaxScaler()\n",
    "        X_scaler5_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "\n",
    "        X_scaler6 = preprocessing.MinMaxScaler()\n",
    "        X_scaler7 = preprocessing.MinMaxScaler()\n",
    "        X_scaler8 = preprocessing.MinMaxScaler()\n",
    "        X_scaler9 = preprocessing.MinMaxScaler()\n",
    "        X_scaler10 = preprocessing.MinMaxScaler()\n",
    "\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "        rnn_scaled_train1_a = X_scaler1_a.fit_transform(rnn_train1_a)\n",
    "        rnn_scaled_train1_b = X_scaler1_b.fit_transform(rnn_train1_b)\n",
    "        rnn_scaled_train1_c = X_scaler1_c.fit_transform(rnn_train1_c)\n",
    "\n",
    "        rnn_scaled_train2_a = X_scaler2_a.fit_transform(rnn_train2_a)\n",
    "        rnn_scaled_train2_b = X_scaler2_b.fit_transform(rnn_train2_b)\n",
    "        rnn_scaled_train2_c = X_scaler2_c.fit_transform(rnn_train2_c)\n",
    "\n",
    "        rnn_scaled_train3_a = X_scaler3_a.fit_transform(rnn_train3_a)\n",
    "        rnn_scaled_train3_b = X_scaler3_b.fit_transform(rnn_train3_b)\n",
    "        rnn_scaled_train3_c = X_scaler3_c.fit_transform(rnn_train3_c)\n",
    "\n",
    "        rnn_scaled_train4_a = X_scaler4_a.fit_transform(rnn_train4_a)\n",
    "        rnn_scaled_train4_b = X_scaler4_b.fit_transform(rnn_train4_b)\n",
    "        rnn_scaled_train4_c = X_scaler4_c.fit_transform(rnn_train4_c)\n",
    "\n",
    "        rnn_scaled_train5_a = X_scaler5_a.fit_transform(rnn_train5_a)\n",
    "        rnn_scaled_train5_b = X_scaler5_b.fit_transform(rnn_train5_b)\n",
    "        rnn_scaled_train5_c = X_scaler5_c.fit_transform(rnn_train5_c)\n",
    "\n",
    "        rnn_scaled_train6 = X_scaler6.fit_transform(rnn_train6)\n",
    "        rnn_scaled_train7 = X_scaler7.fit_transform(rnn_train7)\n",
    "        rnn_scaled_train8 = X_scaler8.fit_transform(rnn_train8)\n",
    "        rnn_scaled_train9 = X_scaler9.fit_transform(rnn_train9)\n",
    "        rnn_scaled_train10 = X_scaler10.fit_transform(rnn_train10)\n",
    "\n",
    "        train_y   = Y_scaler.fit_transform(rnn_Y)\n",
    "        Y_scaler_n=Y_scaler.fit(rnn_Y)\n",
    "        train_X = np.hstack(\n",
    "            (rnn_scaled_train1_a, rnn_scaled_train1_b, rnn_scaled_train1_c, rnn_scaled_train2_a, rnn_scaled_train2_b, rnn_scaled_train2_c,\n",
    "             rnn_scaled_train3_a, rnn_scaled_train3_b, rnn_scaled_train3_c, rnn_scaled_train4_a, rnn_scaled_train4_b, rnn_scaled_train4_c,\n",
    "             rnn_scaled_train5_a, rnn_scaled_train5_b, rnn_scaled_train5_c,rnn_scaled_train6,rnn_scaled_train7, rnn_scaled_train8,\n",
    "             rnn_scaled_train9, rnn_scaled_train10)\n",
    "        ).reshape(rnn_train6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "\n",
    "        test_X = np.hstack(\n",
    "            (X_scaler1_a.transform(rnn_test1_a),X_scaler1_b.transform(rnn_test1_b),X_scaler1_c.transform(rnn_test1_c),\n",
    "             X_scaler2_a.transform(rnn_test2_a),X_scaler2_b.transform(rnn_test2_b),X_scaler2_c.transform(rnn_test2_c),\n",
    "             X_scaler3_a.transform(rnn_test3_a),X_scaler3_b.transform(rnn_test3_b),X_scaler3_c.transform(rnn_test3_c),\n",
    "             X_scaler4_a.transform(rnn_test4_a),X_scaler4_b.transform(rnn_test4_b),X_scaler4_c.transform(rnn_test4_c),\n",
    "             X_scaler5_a.transform(rnn_test5_a),X_scaler5_b.transform(rnn_test5_b),X_scaler5_c.transform(rnn_test5_c),\n",
    "             X_scaler6.transform(rnn_test6),X_scaler7.transform(rnn_test7),X_scaler8.transform(rnn_test8),\n",
    "             X_scaler9.transform(rnn_test9), X_scaler10.transform(rnn_test10))\n",
    "        ).reshape(rnn_test6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "        \n",
    "        test_y = test_df.iloc[:, 0:16]\n",
    "                                \n",
    "        return train_X, train_y, test_X, test_y, test_df, train_df, Y_scaler_n\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X, train_y, test_X, test_y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "\n",
    "def fit_multitarget_model(model, X_train, Y_train, X_test, Y_test, actuals_and_forecast_df, targets, Y_scaler_n):\n",
    " \n",
    "    try:\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "        Y_scaler = Y_scaler.fit(Y_train)\n",
    "        cols = Y.columns.values.tolist()                         \n",
    "\n",
    "        model.fit(X_train, Y_train) \n",
    "        model_test_predictions=None        \n",
    "        model_test_predictions = pd.DataFrame(Y_scaler_n.inverse_transform(model.predict(X_test).reshape(1,16)), columns=cols, index=Y_test.index)\n",
    "        model_test_mse = mean_squared_error(Y_test, model_test_predictions)\n",
    "        model_test_rmse = round(np.sqrt(model_test_mse),2)\n",
    "        model_test_mae = round(mean_absolute_error(Y_test, model_test_predictions),2)\n",
    "        \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast\"] = model_test_predictions.iloc[:,i].tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "            predictor_test_mse = mean_squared_error(Y_test[cols[i]], model_test_predictions.iloc[:,i]) if len(cols) > 1 else mean_squared_error(Y_test[cols[i]], model_test_predictions.tolist())\n",
    "            predictor_test_rmse = round(np.sqrt(predictor_test_mse), 2)\n",
    "            predictor_test_mae = round(mean_absolute_error(Y_test[cols[i]], model_test_predictions.iloc[:,i]),2) if len(cols) > 1 else round(mean_absolute_error(Y_test[cols[i]], model_test_predictions.tolist()),2)\n",
    "            \n",
    "        Error_i= ([model_test_rmse, model_test_mae])\n",
    "        actuals_and_forecast_df = actuals_and_forecast_df.append(Error_i)\n",
    "        \n",
    "           \n",
    "        return actuals_and_forecast_df\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "  \n",
    "    \n",
    "def rolling_walk_forward_validation(model, data, targets, start_time, end_time, training_days, path):\n",
    "  \n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)            \n",
    "        results = pd.DataFrame()\n",
    "            \n",
    "\n",
    "        date_format=\"%m/%d/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "        \n",
    "        while start_time < end_time:\n",
    "            \n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time\n",
    "    \n",
    "            test_start_time = train_end_time + td(hours=8)\n",
    "            test_end_time = test_start_time + td(minutes=30)\n",
    "            \n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "    \n",
    "            train_X, train_y, test_X, test_y, test_df, train_df, Y_scaler_n= generate_train_and_test_dataframes(participant_df=dat, train_start_time=train_start_time.strftime(\"%m/%d/%Y %H:%M\"), train_end_time=train_end_time.strftime(\"%m/%d/%Y %H:%M\"), \n",
    "                            test_start_time=test_start_time.strftime(\"%m/%d/%Y %H:%M\"), test_end_time=test_end_time.strftime(\"%m/%d/%Y %H:%M\"))\n",
    "            \n",
    "            \n",
    "            if train_X is None or len(train_X) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "    \n",
    "            if test_X is None or len(test_X) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "            \n",
    "            actuals_and_forecast_df = fit_multitarget_model(model=model,Y_scaler_n=Y_scaler_n, X_train=train_X, Y_train=train_y,\n",
    "                                            X_test=test_X, Y_test=test_y, actuals_and_forecast_df=test_df.iloc[:,0:16], targets=Y.columns.values.tolist())\n",
    "\n",
    "    \n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "        \n",
    "            start_time = start_time + td(minutes=30)\n",
    "            \n",
    "        results.to_csv(path  + \".csv\", index = False)\n",
    "        \n",
    "\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "\n",
    "def create_model():\n",
    "            nn = Sequential()\n",
    "            nn.add(Flatten(input_shape=i_shape))\n",
    "\n",
    "            for i in range(2):\n",
    "                nn.add(Dense(64, input_shape=i_shape, activation='tanh'))\n",
    "                nn.add(BatchNormalization())\n",
    "                \n",
    "            for i in range(1):\n",
    "                nn.add(Dense(128, activation='tanh'))\n",
    "                nn.add(BatchNormalization())\n",
    "   \n",
    "            for i in range(1):\n",
    "                nn.add(Dense(128, activation='relu'))\n",
    "                nn.add(BatchNormalization())\n",
    "           \n",
    "            nn.add(Dropout(0.133333, seed=123))\n",
    "            nn.add(BatchNormalization())\n",
    "            \n",
    "            for i in range(1):\n",
    "                nn.add(Dense(128, activation='relu'))\n",
    "                nn.add(BatchNormalization())\n",
    "                \n",
    "            nn.add(Dense(16, activation=LeakyReLU))\n",
    "            nn.add(Dense(16))\n",
    "            opt = Adam(lr = 0.004522)\n",
    "            nn.compile(loss='mean_squared_error', optimizer=opt, metrics=['mean_absolute_error'])\n",
    "            \n",
    "            return nn\n",
    "es = EarlyStopping(monitor='mean_absolute_error', mode='min', verbose=0, patience=20)\n",
    "mmo = KerasRegressor(build_fn=create_model, epochs=300, batch_size=16, verbose=2, callbacks=[es])\n",
    "\n",
    "\n",
    "rolling_walk_forward_validation(model=mmo, data=dat, start_time='6/1/2020 00:00', end_time='9/1/2020  10:00',       \n",
    "                                targets=Y.columns.values.tolist(), training_days=-210,\n",
    "                                path=\"/home/coconnor/BM_results_SH_1-10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463b2ac1",
   "metadata": {},
   "source": [
    "Quantile Forecast: SH DNN in the BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f57d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_multitarget_model(model, X_train, Y_train, X_test, Y_test, actuals_and_forecast_df, targets, Y_scaler_n):\n",
    "  \n",
    "    try:\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "        Y_scaler = Y_scaler.fit(Y_train)\n",
    "        cols = Y.columns.values.tolist()                         \n",
    "\n",
    "        model.fit(X_train, Y_train, validation_split=0.1) \n",
    "        model_test_predictions=None        \n",
    "\n",
    "        model_test_predictions = pd.DataFrame(Y_scaler_n.inverse_transform(model.predict(X_test).reshape(5,16)), columns=cols)\n",
    "        print(model_test_predictions)\n",
    "        print(\"test number of observations: \" + str(len(Y_test)))\n",
    "\n",
    "        \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_10\"] = model_test_predictions.iloc[:1,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_30\"] = model_test_predictions.iloc[1:2,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_50\"] = model_test_predictions.iloc[2:3,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist()\n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_70\"] = model_test_predictions.iloc[3:4,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "           \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_90\"] = model_test_predictions.iloc[4:,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist()\n",
    "            \n",
    "            \n",
    "        return actuals_and_forecast_df\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "  \n",
    "    \n",
    "def rolling_walk_forward_validation(model, data, targets, start_time, end_time, training_days, path):\n",
    " \n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)            \n",
    "        results = pd.DataFrame()\n",
    "            \n",
    "\n",
    "        date_format=\"%m/%d/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "        \n",
    "        while start_time < end_time:\n",
    "            \n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time\n",
    "    \n",
    "            test_start_time = train_end_time + td(hours=8)\n",
    "            test_end_time = test_start_time + td(minutes=30)\n",
    "            \n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "    \n",
    "            train_X, train_y, test_X, test_y, test_df, train_df, Y_scaler_n= generate_train_and_test_dataframes(participant_df=dat, train_start_time=train_start_time.strftime(\"%m/%d/%Y %H:%M\"), train_end_time=train_end_time.strftime(\"%m/%d/%Y %H:%M\"), \n",
    "                            test_start_time=test_start_time.strftime(\"%m/%d/%Y %H:%M\"), test_end_time=test_end_time.strftime(\"%m/%d/%Y %H:%M\"))\n",
    "            \n",
    "            \n",
    "            if train_X is None or len(train_X) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "    \n",
    "            if test_X is None or len(test_X) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "            \n",
    "            actuals_and_forecast_df = fit_multitarget_model(model=model,Y_scaler_n=Y_scaler_n, X_train=train_X, Y_train=train_y,\n",
    "                                            X_test=test_X, Y_test=test_y, actuals_and_forecast_df=test_df.iloc[:,0:16], targets=Y.columns.values.tolist())\n",
    "\n",
    "    \n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "        \n",
    "            start_time = start_time + td(hours=8)\n",
    "            \n",
    "        results.to_csv(path  + \".csv\", index = False)\n",
    "        \n",
    "\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "\n",
    "def create_model():    \n",
    "    net_input=Input(shape=i_shape)    \n",
    "    \n",
    "    x = Flatten(input_shape=i_shape)(net_input)\n",
    "    \n",
    "    for i in range(2):\n",
    "        x = Dense(192, 'sigmoid')(x)\n",
    "    \n",
    "    for i in range(3):\n",
    "        x = Dense(128, 'relu')(x)\n",
    "    x = Dropout(0.400000)(x)\n",
    "    \n",
    "    a = Dense(64, 'sigmoid')(x)\n",
    "    b = Dense(64, 'sigmoid')(x)\n",
    "    c = Dense(64, 'sigmoid')(x)\n",
    "    d = Dense(64, 'sigmoid')(x)\n",
    "    e = Dense(64, 'sigmoid')(x)  \n",
    "    \n",
    "    \n",
    "    for i in range(1):\n",
    "            a = Dense(64, 'relu')(a) \n",
    "    output_1 =  Dense(16, name='out_10')(a)\n",
    "    \n",
    "    for i in range(1):\n",
    "            b = Dense(64, 'relu')(b) \n",
    "    output_2 =  Dense(16, name='out_30')(b)\n",
    "    \n",
    "    for i in range(1):\n",
    "            c = Dense(64, 'relu')(c) \n",
    "    output_3 =  Dense(16, name='out_50')(c)\n",
    "    \n",
    "    for i in range(1):\n",
    "            d = Dense(64, 'relu')(d) \n",
    "    output_4 =  Dense(16, name='out_70')(d)\n",
    "    \n",
    "    for i in range(1):\n",
    "            e = Dense(64, 'relu')(e) \n",
    "    output_5 =  Dense(16, name='out_90')(e)    \n",
    "    \n",
    "    opt = Adam(learning_rate=0.000100)\n",
    "    model=Model(inputs=net_input, outputs=[output_1, output_2, output_3, output_4, output_5])    \n",
    "    model.compile(loss=[loss_10th_p, loss_30th_p, loss_50th_p, loss_70th_p, loss_90th_p], optimizer=opt)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=40)\n",
    "mmo = KerasRegressor(build_fn=create_model, epochs=200, batch_size=16, verbose=2, callbacks=[es])\n",
    "\n",
    "\n",
    "rolling_walk_forward_validation(model=mmo, data=dat, start_time='6/1/2020 00:00', end_time='9/1/2020  00:00',       \n",
    "                                targets=Y.columns.values.tolist(), training_days=-210,\n",
    "                                path=\"/home/coconnor/SH_Q_1-3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a6492",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d36fc6",
   "metadata": {},
   "source": [
    "Quantile Forecast: light gradient boosting machine for the BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26501288",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=dat.iloc[:, 0:16]\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                        test_start_time: dt, test_end_time: dt):\n",
    "\n",
    "    train_X = None\n",
    "    train_y = None \n",
    "    test_X = None \n",
    "    test_y = None \n",
    "    test_df = None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        participant_df = participant_df.dropna()\n",
    "\n",
    "        date_format=\"%m/%d/%Y %H:%M\"\n",
    "\n",
    "        train_df = None\n",
    "        \n",
    "\n",
    "        \n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)    \n",
    "        train_df = participant_df[(participant_df.index>=train_start_time_str) & (participant_df.index<train_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")            \n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "\n",
    "        \n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)    \n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format) \n",
    "        test_df = participant_df[(participant_df.index>=test_start_time_str) & (participant_df.index<test_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")            \n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "\n",
    "        train_X = train_df.iloc[:, 16:]\n",
    "        test_X = test_df.iloc[:, 16:]\n",
    "        train_y = train_df.iloc[:, 0:16]\n",
    "        test_y = test_df.iloc[:, 0:16]\n",
    "                                \n",
    "        return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X, train_y, test_X, test_y, test_df\n",
    "    \n",
    "    \n",
    "def fit_multitarget_model(model_1, model_2, model_3, model_4, model_5, X_train, Y_train, X_test, Y_test, actuals_and_forecast_df, targets):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        model_1.fit(X_train, Y_train) \n",
    "        model_2.fit(X_train, Y_train) \n",
    "        model_3.fit(X_train, Y_train) \n",
    "        model_4.fit(X_train, Y_train) \n",
    "        model_5.fit(X_train, Y_train) \n",
    "\n",
    "        model_test_predictions_1=None  \n",
    "        model_test_predictions_3=None  \n",
    "        model_test_predictions_5=None  \n",
    "        model_test_predictions_7=None  \n",
    "        model_test_predictions_9=None  \n",
    "        model_test_predictions_1 = model_1.predict(X_test)     \n",
    "        model_test_predictions_3 = model_2.predict(X_test) \n",
    "        model_test_predictions_5 = model_3.predict(X_test)     \n",
    "        model_test_predictions_7 = model_4.predict(X_test)     \n",
    "        model_test_predictions_9 = model_5.predict(X_test)          \n",
    "                    \n",
    "        cols = Y_train.columns.values.tolist()  \n",
    "        \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_10\"] = model_test_predictions_1[:,i].tolist() if len(cols) > 1 else model_test_predictions_1.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_30\"] = model_test_predictions_3[:,i].tolist() if len(cols) > 1 else model_test_predictions_3.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_50\"] = model_test_predictions_5[:,i].tolist() if len(cols) > 1 else model_test_predictions_5.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_70\"] = model_test_predictions_7[:,i].tolist() if len(cols) > 1 else model_test_predictions_7.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_90\"] = model_test_predictions_9[:,i].tolist() if len(cols) > 1 else model_test_predictions_9.tolist() \n",
    "          \n",
    "           \n",
    "        return actuals_and_forecast_df\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "def rolling_walk_forward_validation(model_1, model_2, model_3, model_4, model_5, data, targets, start_time, end_time, training_days, path):\n",
    "\n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)            \n",
    "        results = pd.DataFrame()\n",
    "            \n",
    "        date_format=\"%m/%d/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "        \n",
    "        while start_time < end_time:\n",
    "            \n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time \n",
    "    \n",
    "            test_start_time = train_end_time + td(hours=8)\n",
    "            test_end_time = test_start_time + td(minutes=30)\n",
    "            \n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "    \n",
    "            train_X, train_y, test_X, test_y, test_df = generate_train_and_test_dataframes(participant_df=dat, train_start_time=train_start_time.strftime(\"%m/%d/%Y %H:%M\"), train_end_time=train_end_time.strftime(\"%m/%d/%Y %H:%M\"), \n",
    "                            test_start_time=test_start_time.strftime(\"%m/%d/%Y %H:%M\"), test_end_time=test_end_time.strftime(\"%m/%d/%Y %H:%M\"))\n",
    "            \n",
    "            if train_X is None or len(train_X) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "    \n",
    "            if test_X is None or len(test_X) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "            \n",
    "            actuals_and_forecast_df = fit_multitarget_model(model_1=model_1,model_2=model_2,model_3=model_3, model_4=model_4,model_5=model_5, X_train=train_X, Y_train=train_y,\n",
    "                                            X_test=test_X, Y_test=test_y, actuals_and_forecast_df=test_df,targets=Y.columns.values.tolist())\n",
    "    \n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "            print(results)\n",
    "            start_time = start_time + td(hours=8)\n",
    "            \n",
    "        results.to_csv(path  + \".csv\", index = False)\n",
    "        \n",
    "          \n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "\n",
    "\n",
    "rolling_walk_forward_validation(model_1=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.1, learning_rate = 0.05, num_leaves=10,  max_depth = 4, n_estimators =  100)),\n",
    "                                model_2=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.3, learning_rate = 0.05, num_leaves=10,  max_depth = 4, n_estimators =  100)),\n",
    "                                model_3=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.5, learning_rate = 0.05, num_leaves=10,  max_depth = 4, n_estimators =  100)),\n",
    "                                model_4=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.7, learning_rate = 0.05, num_leaves=10,  max_depth = 4, n_estimators =  100)),\n",
    "                                model_5=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.9, learning_rate = 0.05, num_leaves=10,  max_depth = 4, n_estimators =  100)),\n",
    "                                data=dat, start_time='06/1/2020 00:00',end_time='09/1/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(),training_days=-210, path=\"/home/coconnor/lgbm_Q_1-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735a417d",
   "metadata": {},
   "source": [
    "Point Forecast: RF, XGB for DAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc5d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=dat.iloc[:, 0:16]\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                        test_start_time: dt, test_end_time: dt):\n",
    "    train_X = None\n",
    "    train_y = None \n",
    "    test_X = None \n",
    "    test_y = None \n",
    "    test_df = None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        participant_df = participant_df.dropna()\n",
    "\n",
    "        date_format=\"%m/%d/%Y %H:%M\"\n",
    "        train_df = None\n",
    "        \n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)    \n",
    "        train_df = participant_df[(participant_df.index>=train_start_time_str) & (participant_df.index<train_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")            \n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        \n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)    \n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format) \n",
    "        test_df = participant_df[(participant_df.index>=test_start_time_str) & (participant_df.index<test_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")            \n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "\n",
    "        train_X = train_df.iloc[:, 16:]\n",
    "        test_X = test_df.iloc[:, 16:]\n",
    "        train_y = train_df.iloc[:, 0:16]\n",
    "        test_y = test_df.iloc[:, 0:16]\n",
    "                                \n",
    "        return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X, train_y, test_X, test_y, test_df\n",
    "    \n",
    "    \n",
    "def fit_multitarget_model(model, X_train, Y_train, X_test, Y_test, actuals_and_forecast_df, targets): \n",
    "    try:        \n",
    "        model.fit(X_train, Y_train) \n",
    "\n",
    "        model_test_predictions=None  \n",
    "        model_train_predictions=None\n",
    "        model_train_predictions = model.predict(X_train)\n",
    "        model_test_predictions = model.predict(X_test)          \n",
    "                    \n",
    "        cols = Y_train.columns.values.tolist()           \n",
    "        \n",
    "        model_train_mse = mean_squared_error(Y_train, model_train_predictions)\n",
    "        model_train_rmse = round(np.sqrt(model_train_mse),2)\n",
    "        model_train_mae = round(mean_absolute_error(Y_train, model_train_predictions),2)\n",
    "        \n",
    "        model_test_mse = mean_squared_error(Y_test, model_test_predictions)\n",
    "        model_test_rmse = round(np.sqrt(model_test_mse),2)\n",
    "        model_test_mae = round(mean_absolute_error(Y_test, model_test_predictions),2)\n",
    "       \n",
    "        for i in range(0,len(cols)):    \n",
    "            predictor_train_mse = mean_squared_error(Y_train[cols[i]], model_train_predictions[:,i]) if len(cols) > 1 else mean_squared_error(Y_train[cols[i]], model_train_predictions.tolist()) \n",
    "            predictor_train_rmse = round(np.sqrt(predictor_train_mse),2)\n",
    "            predictor_train_mae = round(mean_absolute_error(Y_train[cols[i]], model_train_predictions[:,i]),2) if len(cols) > 1 else round(mean_absolute_error(Y_train[cols[i]], model_train_predictions.tolist()),2)            \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast\"] = model_test_predictions[:,i].tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "            predictor_test_mse = mean_squared_error(Y_test[cols[i]], model_test_predictions[:,i]) if len(cols) > 1 else mean_squared_error(Y_test[cols[i]], model_test_predictions.tolist())\n",
    "            predictor_test_rmse = round(np.sqrt(predictor_test_mse), 2)\n",
    "            predictor_test_mae = round(mean_absolute_error(Y_test[cols[i]], model_test_predictions[:,i]),2) if len(cols) > 1 else round(mean_absolute_error(Y_test[cols[i]], model_test_predictions.tolist()),2)\n",
    "            \n",
    "        Error_i= ([model_test_rmse, model_test_mae, model_train_rmse, model_train_mae])\n",
    "        actuals_and_forecast_df = actuals_and_forecast_df.append(Error_i)\n",
    "          \n",
    "        return actuals_and_forecast_df\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "def rolling_walk_forward_validation(model, data, targets, start_time, end_time, training_days, path):  \n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)            \n",
    "        results = pd.DataFrame()\n",
    "            \n",
    "        date_format=\"%m/%d/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "        \n",
    "        while start_time < end_time:\n",
    "            \n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time \n",
    "    \n",
    "            test_start_time = train_end_time + td(hours=8)\n",
    "            test_end_time = test_start_time + td(minutes=30)\n",
    "            \n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "    \n",
    "            train_X, train_y, test_X, test_y, test_df = generate_train_and_test_dataframes(participant_df=dat, train_start_time=train_start_time.strftime(\"%m/%d/%Y %H:%M\"), train_end_time=train_end_time.strftime(\"%m/%d/%Y %H:%M\"), \n",
    "                            test_start_time=test_start_time.strftime(\"%m/%d/%Y %H:%M\"), test_end_time=test_end_time.strftime(\"%m/%d/%Y %H:%M\"))\n",
    "            \n",
    "            if train_X is None or len(train_X) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "    \n",
    "            if test_X is None or len(test_X) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "            \n",
    "            actuals_and_forecast_df = fit_multitarget_model(model=model, X_train=train_X, Y_train=train_y,\n",
    "                                            X_test=test_X, Y_test=test_y, actuals_and_forecast_df=test_df,targets=Y.columns.values.tolist())\n",
    "    \n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "            start_time = start_time + td(hours=8)\n",
    "            \n",
    "        results.to_csv(path  + \".csv\", index = False)\n",
    "        \n",
    "    \n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "        \n",
    "rolling_walk_forward_validation(model=MultiOutputRegressor(XGBRegressor(learning_rate = 0.05, max_depth = 2, n_estimators =  50)), \n",
    "                                data=dat, start_time='06/1/2020 00:00',end_time='09/1/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(),training_days=-210,\n",
    "                                path=\"/home/coconnor/BM_results_XGB_1-10\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2cef99",
   "metadata": {},
   "source": [
    "Point Forecast: LEAR for BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af870c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_train1_a=X_train.loc[:,\"lag_-3x1\":\"lag_-18x1\"]\n",
    "\n",
    "X_1 = X_train.loc[:,\"lag_-3x1\":\"lag_-50x1\"]\n",
    "X_2 = X_train.loc[:,\"lag_-3x2\":\"lag_-50x2\"]\n",
    "X_3 = X_train.loc[\"lag_-2x3\":\"lag_-49x3\"]\n",
    "X_4 = X_train.loc[:,\"lag_0x6\":\"lag_-47x6\"]\n",
    "X_5 = X_train.loc[:,\"lag_-2x12\":\"lag_-49x12\"]\n",
    "X_6 = X_train.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "X_7 = X_train.loc[:,\"lag_2x8\":\"lag_17x8\"]\n",
    "X_8 = X_train.loc[:,\"lag_2x9\":\"lag_17x9\"]\n",
    "X_9 = X_train.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "X_10 = X_train.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "\n",
    "X_test1 = X_test.loc[:,\"lag_-3x1\":\"lag_-50x1\"]\n",
    "X_test2 = X_test.loc[:,\"lag_-3x2\":\"lag_-50x2\"]\n",
    "X_test3 = X_test.loc[:,\"lag_-2x3\":\"lag_-49x3\"]\n",
    "X_test4 = X_test.loc[:,\"lag_0x6\":\"lag_-47x6\"]\n",
    "X_test5 = X_test.loc[:,\"lag_-2x12\":\"lag_-49x12\"]\n",
    "X_test6 = X_test.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "X_test7 = X_test.loc[:,\"lag_2x8\":\"lag_17x8\"]\n",
    "X_test8 = X_test.loc[:,\"lag_2x9\":\"lag_17x9\"]\n",
    "X_test9 = X_test.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "X_test10 = X_test.loc[:,\"lag_2x11\":\"lag_17x11\"]\n",
    "Y_1 = Y_train.loc[:,\"lag_2y\": \"lag_17y\"]\n",
    "\n",
    "[X_1], X_scaler1 = scaling([X_1.values], 'Invariant')\n",
    "[X_2], X_scaler2 = scaling([X_2.values], 'Invariant')\n",
    "[X_3], X_scaler3 = scaling([X_3.values], 'Invariant')\n",
    "[X_4], X_scaler4 = scaling([X_4.values], 'Invariant')\n",
    "[X_5], X_scaler5 = scaling([X_5.values], 'Invariant')\n",
    "\n",
    "[X_6], X_scaler6 = scaling([X_6.values], 'Invariant')\n",
    "[X_7], X_scaler7 = scaling([X_7.values], 'Invariant')\n",
    "[X_8], X_scaler8 = scaling([X_8.values], 'Invariant')\n",
    "[X_9], X_scaler9 = scaling([X_9.values], 'Invariant')\n",
    "[X_10], X_scaler10 = scaling([X_10.values], 'Invariant')\n",
    "\n",
    "X_test_1= X_scaler1.transform(X_test1.values)\n",
    "X_test_2= X_scaler2.transform(X_test2.values)\n",
    "X_test_3= X_scaler3.transform(X_test3.values)\n",
    "X_test_4= X_scaler4.transform(X_test4.values)\n",
    "X_test_5= X_scaler5.transform(X_test5.values)\n",
    "X_test_6= X_scaler6.transform(X_test6.values)\n",
    "X_test_7= X_scaler7.transform(X_test7.values)\n",
    "X_test_8= X_scaler8.transform(X_test8.values)\n",
    "X_test_9= X_scaler9.transform(X_test9.values)\n",
    "X_test_10= X_scaler10.transform(X_test10.values)\n",
    "\n",
    "[Y_train_scaled], Y_scaler = scaling([Y_1.values], 'Invariant')\n",
    "\n",
    "X_train_scaled=np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9, X_10), axis=1)\n",
    "X_test_scaled=np.concatenate((X_test_1, X_test_2, X_test_3, X_test_4, X_test_5, X_test_6, X_test_7, X_test_8, X_test_9, X_test_10), axis=1)\n",
    "\n",
    "alpha=LassoLarsIC(criterion='aic', max_iter=2500).fit(X_train_scaled, Y_train_scaled[:,:1].ravel()).alpha_\n",
    "\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                                       test_start_time: dt, test_end_time: dt):\n",
    "\n",
    "\n",
    "    train_X = None\n",
    "    train_Y = None\n",
    "    test_X = None\n",
    "    test_Y = None\n",
    "    test_df = None\n",
    "    train_df = None\n",
    "\n",
    "    try:\n",
    "\n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "        #             return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        participant_df = participant_df.dropna()\n",
    "\n",
    "        date_format = \"%m/%d/%Y %H:%M\"\n",
    "\n",
    "        train_df = None\n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)\n",
    "        train_df = participant_df[\n",
    "            (participant_df.index >= train_start_time_str) & (participant_df.index < train_end_time_str)].copy(\n",
    "            deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\n",
    "                \"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")\n",
    "\n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)\n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format)\n",
    "        test_df = participant_df[\n",
    "            (participant_df.index >= test_start_time_str) & (participant_df.index < test_end_time_str)].copy(\n",
    "            deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\n",
    "                \"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")\n",
    "\n",
    "        X_1 = train_df.loc[:,\"lag_-3x1\":\"lag_-50x1\"]\n",
    "        X_2 = train_df.loc[:,\"lag_-3x2\":\"lag_-50x2\"]\n",
    "        X_3 = train_df.loc[\"lag_-2x3\":\"lag_-49x3\"]\n",
    "        X_4 = train_df.loc[:,\"lag_0x6\":\"lag_-47x6\"]\n",
    "        X_5 = train_df.loc[:,\"lag_-2x12\":\"lag_-49x12\"]\n",
    "        X_6 = train_df.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "        X_7 = train_df.loc[:,\"lag_2x8\":\"lag_17x8\"]\n",
    "        X_8 = train_df.loc[:,\"lag_2x9\":\"lag_17x9\"]\n",
    "        X_9 = train_df.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "        X_10 = train_df.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "\n",
    "        X_test1 = test_df.loc[:,\"lag_-3x1\":\"lag_-50x1\"]\n",
    "        X_test2 = test_df.loc[:,\"lag_-3x2\":\"lag_-50x2\"]\n",
    "        X_test3 = test_df.loc[:,\"lag_-2x3\":\"lag_-49x3\"]\n",
    "        X_test4 = test_df.loc[:,\"lag_0x6\":\"lag_-47x6\"]\n",
    "        X_test5 = test_df.loc[:,\"lag_-2x12\":\"lag_-49x12\"]\n",
    "        X_test6 = test_df.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "        X_test7 = test_df.loc[:,\"lag_2x8\":\"lag_17x8\"]\n",
    "        X_test8 = test_df.loc[:,\"lag_2x9\":\"lag_17x9\"]\n",
    "        X_test9 = test_df.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "        X_test10 = test_df.loc[:,\"lag_2x11\":\"lag_17x11\"]\n",
    "        Y_1 = train_df.loc[:,\"lag_2y\": \"lag_17y\"]\n",
    "\n",
    "        [X_1], X_scaler1 = scaling([X_1.values], 'Invariant')\n",
    "        [X_2], X_scaler2 = scaling([X_2.values], 'Invariant')\n",
    "        [X_3], X_scaler3 = scaling([X_3.values], 'Invariant')\n",
    "        [X_4], X_scaler4 = scaling([X_4.values], 'Invariant')\n",
    "        [X_5], X_scaler5 = scaling([X_5.values], 'Invariant')\n",
    "\n",
    "        [X_6], X_scaler6 = scaling([X_6.values], 'Invariant')\n",
    "        [X_7], X_scaler7 = scaling([X_7.values], 'Invariant')\n",
    "        [X_8], X_scaler8 = scaling([X_8.values], 'Invariant')\n",
    "        [X_9], X_scaler9 = scaling([X_9.values], 'Invariant')\n",
    "        [X_10], X_scaler10 = scaling([X_10.values], 'Invariant')\n",
    "\n",
    "        X_test_1= X_scaler1.transform(X_test1.values)\n",
    "        X_test_2= X_scaler2.transform(X_test2.values)\n",
    "        X_test_3= X_scaler3.transform(X_test3.values)\n",
    "        X_test_4= X_scaler4.transform(X_test4.values)\n",
    "        X_test_5= X_scaler5.transform(X_test5.values)\n",
    "        X_test_6= X_scaler6.transform(X_test6.values)\n",
    "        X_test_7= X_scaler7.transform(X_test7.values)\n",
    "        X_test_8= X_scaler8.transform(X_test8.values)\n",
    "        X_test_9= X_scaler9.transform(X_test9.values)\n",
    "        X_test_10= X_scaler10.transform(X_test10.values)\n",
    "\n",
    "        [train_Y], Y_scaler = scaling([Y_1.values], 'Invariant')\n",
    "        Y_scaler_n = Y_scaler\n",
    "\n",
    "        train_X=np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9, X_10), axis=1)\n",
    "        test_X=np.concatenate((X_test_1, X_test_2, X_test_3, X_test_4, X_test_5, X_test_6, X_test_7, X_test_8, X_test_9, X_test_10), axis=1)\n",
    "        test_Y = test_df.iloc[:, 0:16]\n",
    "\n",
    "        return train_X, train_Y, test_X, test_Y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X, train_Y, test_X, test_Y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "\n",
    "def fit_multitarget_model(model, X_train, Y_train, X_test, Y_test, actuals_and_forecast_df, targets, Y_scaler_n):\n",
    "    try:\n",
    "        cols = Y.columns.values.tolist()\n",
    "\n",
    "        model.fit(X_train, Y_train)\n",
    "        model_test_predictions = None\n",
    "        model_test_predictions = pd.DataFrame(Y_scaler_n.inverse_transform(model.predict(X_test)))\n",
    "        model_test_mse = mean_squared_error(Y_test, model_test_predictions)\n",
    "        model_test_rmse = round(np.sqrt(model_test_mse), 2)\n",
    "        model_test_mae = round(mean_absolute_error(Y_test, model_test_predictions), 2)\n",
    "\n",
    "        for i in range(0, len(cols)):\n",
    "            actuals_and_forecast_df[cols[i] + \"_Forecast\"] = model_test_predictions.iloc[:, i].tolist() if len(\n",
    "                cols) > 1 else model_test_predictions.tolist()\n",
    "            predictor_test_mse = mean_squared_error(Y_test[cols[i]], model_test_predictions.iloc[:, i]) if len(\n",
    "                cols) > 1 else mean_squared_error(Y_test[cols[i]], model_test_predictions.tolist())\n",
    "            predictor_test_rmse = round(np.sqrt(predictor_test_mse), 2)\n",
    "            predictor_test_mae = round(mean_absolute_error(Y_test[cols[i]], model_test_predictions.iloc[:, i]),\n",
    "                                       2) if len(cols) > 1 else round(\n",
    "                mean_absolute_error(Y_test[cols[i]], model_test_predictions.tolist()), 2)\n",
    "\n",
    "        Error_i = ([model_test_rmse, model_test_mae])\n",
    "        actuals_and_forecast_df = actuals_and_forecast_df.append(Error_i)\n",
    "\n",
    "        return actuals_and_forecast_df\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rolling_walk_forward_validation(model, data, targets, start_time, end_time, training_days, path):\n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)\n",
    "        results = pd.DataFrame()\n",
    "\n",
    "        date_format = \"%m/%d/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "\n",
    "        while start_time < end_time:\n",
    "\n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time\n",
    "\n",
    "            test_start_time = train_end_time + td(hours=8)\n",
    "            test_end_time = test_start_time + td(minutes=30)\n",
    "\n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "\n",
    "            train_X, train_Y, test_X, test_Y, test_df, train_df, Y_scaler_n = generate_train_and_test_dataframes(\n",
    "                participant_df=dat, train_start_time=train_start_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                train_end_time=train_end_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                test_start_time=test_start_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                test_end_time=test_end_time.strftime(\"%m/%d/%Y %H:%M\"))\n",
    "\n",
    "            if train_X is None or len(train_X) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(\n",
    "                    train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            if test_X is None or len(test_X) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(\n",
    "                    test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            actuals_and_forecast_df = fit_multitarget_model(model=model, Y_scaler_n=Y_scaler_n, X_train=train_X,\n",
    "                                                            Y_train=train_Y, X_test=test_X, Y_test=test_Y,\n",
    "                                                            actuals_and_forecast_df=test_df.iloc[:, 0:16],\n",
    "                                                            targets=Y.columns.values.tolist())\n",
    "\n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "\n",
    "            start_time = start_time + td(minutes=30)\n",
    "\n",
    "        results.to_csv(path + \".csv\", index=False)\n",
    "\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "rolling_walk_forward_validation(model=MultiOutputRegressor(Lasso(max_iter=2500, alpha=alpha)), \n",
    "                                data=dat, start_time='06/1/2020 00:00',end_time='09/1/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(),training_days=-300,\n",
    "                                path=\"/home/coconnor/BM_results_LEAR_1-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0347168",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c04e2c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab82923e",
   "metadata": {},
   "source": [
    " Import DAM dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76540226",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_format = \"%d/%m/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/coconnor/DAM_VAR_1-3.csv\", index_col=\"DeliveryPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8bd255",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230bf3b4",
   "metadata": {},
   "source": [
    "Quantile Forecast: Single headed Deep neural network in the DAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e25ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dat.iloc[:, 0:24]\n",
    "X = dat.iloc[:, 24:]\n",
    "X_train = X.iloc[:12360, :]\n",
    "Y_train = Y.iloc[:12360, :]\n",
    "X_test = X.iloc[12360:13104, :]\n",
    "Y_test = Y.iloc[12360:13104, :]\n",
    "\n",
    "\n",
    "\n",
    "rnn_train_1 = X_train.loc[:, \"EURPrices-24\":\"EURPrices-167\"]\n",
    "rnn_train_2 = X_train.loc[:, \"WF\": \"WF-143\"]\n",
    "rnn_train_3 = X_train.loc[:, \"DF\": \"DF-143\"]\n",
    "\n",
    "rnn_test_1 = X_test.loc[:, \"EURPrices-24\":\"EURPrices-167\"]\n",
    "rnn_test_2 = X_test.loc[:, \"WF\": \"WF-143\"]\n",
    "rnn_test_3 = X_test.loc[:, \"DF\": \"DF-143\"]\n",
    "\n",
    "rnn_Y = Y_train.loc[:, \"EURPrices\":\"EURPrices+23\"]\n",
    "rnn_test_Y = Y_test.loc[:, \"EURPrices\":\"EURPrices+23\"]\n",
    "\n",
    "X_scaler1 = preprocessing.MinMaxScaler()\n",
    "X_scaler2 = preprocessing.MinMaxScaler()\n",
    "X_scaler3 = preprocessing.MinMaxScaler()\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train_1 = X_scaler1.fit_transform(rnn_train_1)\n",
    "rnn_scaled_train_2 = X_scaler2.fit_transform(rnn_train_2)\n",
    "rnn_scaled_train_3 = X_scaler3.fit_transform(rnn_train_3)\n",
    "\n",
    "Y_train_Scaled = Y_scaler.fit_transform(rnn_Y)\n",
    "Y_test_scaled = Y_scaler.transform(rnn_test_Y)\n",
    "\n",
    "X_train_Scaled = np.hstack(\n",
    "    (rnn_scaled_train_1, rnn_scaled_train_2, rnn_scaled_train_3)\n",
    ").reshape(rnn_train_1.shape[0], 3, 144).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled = np.hstack(\n",
    "    (X_scaler1.transform(rnn_test_1), X_scaler2.transform(rnn_test_2), X_scaler3.transform(rnn_test_3))\n",
    ").reshape(rnn_test_1.shape[0], 3, 144).transpose(0, 2, 1)\n",
    "\n",
    "i_shape = (X_train_Scaled.shape[1], X_train_Scaled.shape[2])\n",
    "\n",
    "\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                                       test_start_time: dt, test_end_time: dt):\n",
    "\n",
    "\n",
    "    # These are the dataframes that will be returned from the method.\n",
    "    train_X = None\n",
    "    train_y = None\n",
    "    test_X = None\n",
    "    test_y = None\n",
    "    test_df = None\n",
    "    train_df = None\n",
    "\n",
    "    try:\n",
    "\n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "        #             return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        # Remove any rows with nan's etc (there shouldn't be any in the input).\n",
    "        participant_df = participant_df.dropna()\n",
    "\n",
    "        date_format = \"%d/%m/%Y %H:%M\"\n",
    "\n",
    "        # The train dataframe, it will be used later to create train_X and train_y.\n",
    "        train_df = None\n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)\n",
    "        train_df = participant_df[\n",
    "            (participant_df.index >= train_start_time_str) & (participant_df.index < train_end_time_str)].copy(\n",
    "            deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\n",
    "                \"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")\n",
    "        #             return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        # Create the test dataframe, it will be used later to create test_X and test_y\n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)\n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format)\n",
    "        test_df = participant_df[\n",
    "            (participant_df.index >= test_start_time_str) & (participant_df.index < test_end_time_str)].copy(\n",
    "            deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\n",
    "                \"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")\n",
    "        #             return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        rnn_train_1 = train_df.loc[:, \"EURPrices-24\":\"EURPrices-167\"]\n",
    "        rnn_train_2 = train_df.loc[:, \"WF\": \"WF-143\"]\n",
    "        rnn_train_3 = train_df.loc[:, \"DF\": \"DF-143\"]\n",
    "\n",
    "        rnn_test_1 = test_df.loc[:, \"EURPrices-24\":\"EURPrices-167\"]\n",
    "        rnn_test_2 = test_df.loc[:, \"WF\": \"WF-143\"]\n",
    "        rnn_test_3 = test_df.loc[:, \"DF\": \"DF-143\"]\n",
    "\n",
    "        rnn_Y = train_df.loc[:, \"EURPrices\":\"EURPrices+23\"]\n",
    "        rnn_test_Y = test_df.loc[:, \"EURPrices\":\"EURPrices+23\"]\n",
    "\n",
    "        X_scaler1 = preprocessing.MinMaxScaler()\n",
    "        X_scaler2 = preprocessing.MinMaxScaler()\n",
    "        X_scaler3 = preprocessing.MinMaxScaler()\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "        rnn_scaled_train_1 = X_scaler1.fit_transform(rnn_train_1)\n",
    "        rnn_scaled_train_2 = X_scaler2.fit_transform(rnn_train_2)\n",
    "        rnn_scaled_train_3 = X_scaler3.fit_transform(rnn_train_3)\n",
    "\n",
    "        train_y = Y_scaler.fit_transform(rnn_Y)\n",
    "        Y_scaler_n = Y_scaler.fit(rnn_Y)\n",
    "\n",
    "        Y_test_scaled = Y_scaler.transform(rnn_test_Y)\n",
    "\n",
    "        train_X = np.hstack(\n",
    "            (rnn_scaled_train_1, rnn_scaled_train_2, rnn_scaled_train_3)\n",
    "        ).reshape(rnn_train_1.shape[0], 3, 144).transpose(0, 2, 1)\n",
    "\n",
    "        test_X = np.hstack(\n",
    "            (X_scaler1.transform(rnn_test_1), X_scaler2.transform(rnn_test_2), X_scaler3.transform(rnn_test_3))\n",
    "        ).reshape(rnn_test_1.shape[0], 3, 144).transpose(0, 2, 1)\n",
    "\n",
    "        test_y = test_df.iloc[:, 0:24]\n",
    "\n",
    "        return train_X, train_y, test_X, test_y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X, train_y, test_X, test_y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "\n",
    "def fit_multitarget_model(model, X_train, Y_train, X_test, Y_test, actuals_and_forecast_df, targets, Y_scaler_n):\n",
    "    try:\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "        Y_scaler = Y_scaler.fit(Y_train)\n",
    "        cols = Y.columns.values.tolist()\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min',  patience=20)\n",
    "\n",
    "        model.fit(X_train, Y_train, epochs=200, verbose=2,  callbacks=[es], validation_split=0.1)\n",
    "        model_test_predictions=None\n",
    "        model_test_predictions = pd.DataFrame(Y_scaler_n.inverse_transform(np.array(model.predict(X_test).reshape(5,24))), columns=cols)\n",
    "\n",
    "        print(model_test_predictions)\n",
    "        print(\"test number of observations: \" + str(len(Y_test)))\n",
    "\n",
    "        for i in range(0, len(cols)):\n",
    "            actuals_and_forecast_df[cols[i] + \"_Forecast_10\"] = model_test_predictions.iloc[:1, i].T.tolist() if len(\n",
    "                cols) > 1 else model_test_predictions.tolist()\n",
    "\n",
    "        for i in range(0, len(cols)):\n",
    "            actuals_and_forecast_df[cols[i] + \"_Forecast_30\"] = model_test_predictions.iloc[1:2, i].T.tolist() if len(\n",
    "                cols) > 1 else model_test_predictions.tolist()\n",
    "\n",
    "        for i in range(0, len(cols)):\n",
    "            actuals_and_forecast_df[cols[i] + \"_Forecast_50\"] = model_test_predictions.iloc[2:3, i].T.tolist() if len(\n",
    "                cols) > 1 else model_test_predictions.tolist()\n",
    "\n",
    "        for i in range(0, len(cols)):\n",
    "            actuals_and_forecast_df[cols[i] + \"_Forecast_70\"] = model_test_predictions.iloc[3:4, i].T.tolist() if len(\n",
    "                cols) > 1 else model_test_predictions.tolist()\n",
    "\n",
    "        for i in range(0, len(cols)):\n",
    "            actuals_and_forecast_df[cols[i] + \"_Forecast_90\"] = model_test_predictions.iloc[4:, i].T.tolist() if len(\n",
    "                cols) > 1 else model_test_predictions.tolist()\n",
    "\n",
    "        return actuals_and_forecast_df\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def rolling_walk_forward_validation(model, data, targets, start_time, end_time, training_days, path):\n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)\n",
    "        results = pd.DataFrame()\n",
    "\n",
    "        date_format = \"%d/%m/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "\n",
    "        while start_time < end_time:\n",
    "\n",
    "            # Train interval\n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time\n",
    "\n",
    "            # Test interval, the test period is always the day ahead forecast\n",
    "            test_start_time = train_end_time + td(hours=24)\n",
    "            test_end_time = test_start_time + td(hours=1)\n",
    "\n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "\n",
    "            # Generate the calibration and test dataframes.\n",
    "            train_X, train_y, test_X, test_y, test_df, train_df, Y_scaler_n = generate_train_and_test_dataframes(\n",
    "                participant_df=dat, train_start_time=train_start_time.strftime(\"%d/%m/%Y %H:%M\"),\n",
    "                train_end_time=train_end_time.strftime(\"%d/%m/%Y %H:%M\"),\n",
    "                test_start_time=test_start_time.strftime(\"%d/%m/%Y %H:%M\"),\n",
    "                test_end_time=test_end_time.strftime(\"%d/%m/%Y %H:%M\"))\n",
    "\n",
    "            if train_X is None or len(train_X) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(\n",
    "                    train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            if test_X is None or len(test_X) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(\n",
    "                    test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            # Fit the model to the train datasets, produce a forecast and return a dataframe containing the forecast/actuals.\n",
    "            actuals_and_forecast_df = fit_multitarget_model(model=model, Y_scaler_n=Y_scaler_n, X_train=train_X,\n",
    "                                                            Y_train=train_y,\n",
    "                                                            X_test=test_X, Y_test=test_y,\n",
    "                                                            actuals_and_forecast_df=test_df.iloc[:, 0:24],\n",
    "                                                            targets=Y.columns.values.tolist())\n",
    "\n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "            start_time = start_time + td(hours=24)\n",
    "\n",
    "        results.to_csv(path + \".csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    net_input = Input(shape=i_shape)\n",
    "\n",
    "    x = Flatten(input_shape=i_shape)(net_input)\n",
    "\n",
    "    for i in range(3):\n",
    "        x = Dense(144, 'sigmoid')(x)\n",
    "\n",
    "    for i in range(3):\n",
    "        x = Dense(240, 'tanh')(x)\n",
    "    x = Dropout(0.311111)(x)\n",
    "\n",
    "    a = Dense(24, 'tanh')(x)\n",
    "    b = Dense(24, 'tanh')(x)\n",
    "    c = Dense(24, 'tanh')(x)\n",
    "    d = Dense(24, 'tanh')(x)\n",
    "    e = Dense(24, 'tanh')(x)\n",
    "\n",
    "    for i in range(1):\n",
    "        a = Dense(48, 'relu')(a)\n",
    "    output_1 = Dense(24, name='out_10')(a)\n",
    "\n",
    "    for i in range(1):\n",
    "        b = Dense(48, 'relu')(b)\n",
    "    output_2 = Dense(24, name='out_30')(b)\n",
    "\n",
    "    for i in range(1):\n",
    "        c = Dense(48, 'relu')(c)\n",
    "    output_3 = Dense(24, name='out_50')(c)\n",
    "\n",
    "    for i in range(1):\n",
    "        d = Dense(48, 'relu')(d)\n",
    "    output_4 = Dense(24, name='out_70')(d)\n",
    "\n",
    "    for i in range(1):\n",
    "        e = Dense(48, 'relu')(e)\n",
    "    output_5 = Dense(24, name='out_90')(e)\n",
    "\n",
    "    opt = Adam(learning_rate=0.000100)\n",
    "    model = Model(inputs=net_input, outputs=[output_1, output_2, output_3, output_4, output_5])\n",
    "    model.compile(loss=[loss_10th_p, loss_30th_p, loss_50th_p, loss_70th_p, loss_90th_p], optimizer=opt)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "mmo = KerasRegressor(build_fn=create_model, epochs=200, batch_size=16, verbose=2)\n",
    "\n",
    "rolling_walk_forward_validation(model=mmo, data=dat, start_time='1/6/2020 00:00', end_time='1/9/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(), training_days=-210,\n",
    "                                path=\"/home/coconnor/SH_Q_DAM_1-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6dbd0",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1ff09e",
   "metadata": {},
   "source": [
    "Quantile Forecast: light gradient boosting library for the DAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8713fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dat.iloc[:, 0:24]\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                        test_start_time: dt, test_end_time: dt):\n",
    "\n",
    "    train_X = None\n",
    "    train_y = None \n",
    "    test_X = None \n",
    "    test_y = None \n",
    "    test_df = None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        participant_df = participant_df.dropna()\n",
    "\n",
    "        date_format = \"%d/%m/%Y %H:%M\"\n",
    "\n",
    "        train_df = None\n",
    "        \n",
    "\n",
    "        \n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)    \n",
    "        train_df = participant_df[(participant_df.index>=train_start_time_str) & (participant_df.index<train_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")            \n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "\n",
    "        \n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)    \n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format) \n",
    "        test_df = participant_df[(participant_df.index>=test_start_time_str) & (participant_df.index<test_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")            \n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "\n",
    "        train_X = train_df.iloc[:, 24:]\n",
    "        test_X = test_df.iloc[:, 24:]\n",
    "        train_y = train_df.iloc[:, 0:24]\n",
    "        test_y = test_df.iloc[:, 0:24]\n",
    "                                \n",
    "        return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X, train_y, test_X, test_y, test_df\n",
    "    \n",
    "    \n",
    "def fit_multitarget_model(model_1, model_2, model_3, model_4, model_5, X_train, Y_train, X_test, Y_test, actuals_and_forecast_df, targets):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        model_1.fit(X_train, Y_train) \n",
    "        model_2.fit(X_train, Y_train) \n",
    "        model_3.fit(X_train, Y_train) \n",
    "        model_4.fit(X_train, Y_train) \n",
    "        model_5.fit(X_train, Y_train) \n",
    "\n",
    "\n",
    "        model_test_predictions_1=None  \n",
    "        model_test_predictions_3=None  \n",
    "        model_test_predictions_5=None  \n",
    "        model_test_predictions_7=None  \n",
    "        model_test_predictions_9=None  \n",
    "        model_test_predictions_1 = model_1.predict(X_test)     \n",
    "        model_test_predictions_3 = model_2.predict(X_test) \n",
    "        model_test_predictions_5 = model_3.predict(X_test)     \n",
    "        model_test_predictions_7 = model_4.predict(X_test)     \n",
    "        model_test_predictions_9 = model_5.predict(X_test)          \n",
    "                    \n",
    "        cols = Y_train.columns.values.tolist()   \n",
    "        \n",
    "        \n",
    "        print(\"train number of observations: \" + str(len(Y_train)))\n",
    "\n",
    "        \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_10\"] = model_test_predictions_1[:,i].tolist() if len(cols) > 1 else model_test_predictions_1.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_30\"] = model_test_predictions_3[:,i].tolist() if len(cols) > 1 else model_test_predictions_3.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_50\"] = model_test_predictions_5[:,i].tolist() if len(cols) > 1 else model_test_predictions_5.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_70\"] = model_test_predictions_7[:,i].tolist() if len(cols) > 1 else model_test_predictions_7.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_90\"] = model_test_predictions_9[:,i].tolist() if len(cols) > 1 else model_test_predictions_9.tolist() \n",
    "          \n",
    "           \n",
    "        return actuals_and_forecast_df\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "def rolling_walk_forward_validation(model_1, model_2, model_3, model_4, model_5, data, targets, start_time, end_time, training_days, path):\n",
    "\n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)            \n",
    "        results = pd.DataFrame()\n",
    "            \n",
    "\n",
    "        date_format = \"%d/%m/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "        \n",
    "        while start_time < end_time:\n",
    "            \n",
    "            #Train interval\n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time \n",
    "    \n",
    "            #Test interval, the test period is always the day ahead forecast\n",
    "            test_start_time = train_end_time + td(hours=24)\n",
    "            test_end_time = test_start_time + td(hours=1)\n",
    "            \n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "    \n",
    "            #Generate the calibration and test dataframes.\n",
    "            train_X, train_y, test_X, test_y, test_df = generate_train_and_test_dataframes(participant_df=dat, train_start_time=train_start_time.strftime(\"%d/%m/%Y %H:%M\"), train_end_time=train_end_time.strftime(\"%d/%m/%Y %H:%M\"), \n",
    "                            test_start_time=test_start_time.strftime(\"%d/%m/%Y %H:%M\"), test_end_time=test_end_time.strftime(\"%d/%m/%Y %H:%M\"))\n",
    "            \n",
    "            if train_X is None or len(train_X) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "    \n",
    "            if test_X is None or len(test_X) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "            \n",
    "            actuals_and_forecast_df = fit_multitarget_model(model_1=model_1,model_2=model_2,model_3=model_3, model_4=model_4,model_5=model_5, X_train=train_X, Y_train=train_y,\n",
    "                                            X_test=test_X, Y_test=test_y, actuals_and_forecast_df=test_df,targets=Y.columns.values.tolist())\n",
    "    \n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "            print(results)\n",
    "            start_time = start_time + td(hours=24)\n",
    "            \n",
    "        results.to_csv(path  + \".csv\", index = False)\n",
    "        \n",
    "          \n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "\n",
    "\n",
    "rolling_walk_forward_validation(model_1=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.1, learning_rate = 0.01, num_leaves=20,  max_depth = 18, n_estimators =  60000)),\n",
    "                                model_2=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.3, learning_rate = 0.01, num_leaves=20,  max_depth = 18, n_estimators =  60000)),\n",
    "                                model_3=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.5, learning_rate = 0.01, num_leaves=20,  max_depth = 18, n_estimators =  60000)),\n",
    "                                model_4=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.7, learning_rate = 0.01, num_leaves=20,  max_depth = 18, n_estimators =  60000)),\n",
    "                                model_5=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.9, learning_rate = 0.01, num_leaves=20,  max_depth = 18, n_estimators =  60000)),\n",
    "                                data=dat, start_time='1/6/2020 00:00', end_time='1/9/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(),training_days=-210, path=\"/home/coconnor/lgbm_Q_DAM_1-3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d08def",
   "metadata": {},
   "source": [
    "Quantile Forecast: random forest & KNN library for the DAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_multitarget_model(model, X_train, Y_train, X_test, Y_test, actuals_and_forecast_df, targets):\n",
    "  \n",
    "    try:\n",
    "        \n",
    "        model.fit(X_train, Y_train) \n",
    "\n",
    "        model_test_predictions=None  \n",
    "        model_test_predictions = model.predict(X_test).reshape(5,24)                            \n",
    "        cols = Y_train.columns.values.tolist()   \n",
    "        \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_10\"] = model_test_predictions[0:1,i].tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_30\"] = model_test_predictions[1:2,i].tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_50\"] = model_test_predictions[2:3,i].tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_70\"] = model_test_predictions[3:4,i].tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_90\"] = model_test_predictions[4:,i].tolist() if len(cols) > 1 else model_test_predictions.tolist()\n",
    "            \n",
    "        print(\"train number of observations: \" + str(len(Y_train)))\n",
    "        \n",
    "        \n",
    "        \n",
    "           \n",
    "        return actuals_and_forecast_df\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "def rolling_walk_forward_validation(model, data, targets, start_time, end_time, training_days, path):\n",
    " \n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)            \n",
    "        results = pd.DataFrame()\n",
    "            \n",
    "\n",
    "        date_format=\"%d/%m/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "        \n",
    "        while start_time < end_time:\n",
    "            \n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time \n",
    "    \n",
    "            test_start_time = train_end_time + td(hours=24)\n",
    "            test_end_time = test_start_time + td(hours=1)\n",
    "            \n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "    \n",
    "            train_X, train_y, test_X, test_y, test_df = generate_train_and_test_dataframes(participant_df=dat, train_start_time=train_start_time.strftime(\"%d/%m/%Y %H:%M\"), train_end_time=train_end_time.strftime(\"%d/%m/%Y %H:%M\"), \n",
    "                            test_start_time=test_start_time.strftime(\"%d/%m/%Y %H:%M\"), test_end_time=test_end_time.strftime(\"%d/%m/%Y %H:%M\"))\n",
    "            \n",
    "            if train_X is None or len(train_X) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "    \n",
    "            if test_X is None or len(test_X) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "            \n",
    "            actuals_and_forecast_df = fit_multitarget_model(model=model, X_train=train_X, Y_train=train_y,\n",
    "                                            X_test=test_X, Y_test=test_y, actuals_and_forecast_df=test_df,targets=Y.columns.values.tolist())\n",
    "    \n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "            print(results)\n",
    "            start_time = start_time + td(hours=24)\n",
    "            \n",
    "        results.to_csv(path  + \".csv\", index = False)\n",
    "        \n",
    "          \n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "rolling_walk_forward_validation(model = MultiOutputRegressor(RandomForestQuantileRegressor(q=[0.10, 0.30, 0.50, 0.70, 0.90], max_depth=2 , min_samples_leaf=2 , n_estimators=100 , min_samples_split=2 )),\n",
    "                                data=dat, start_time='1/6/2020 00:00', end_time='1/9/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(),training_days=-210, path=\"/home/coconnor/rf_Q_DAM_1-3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
