{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a8337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Models Search Maximum training Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pandas import concat\n",
    "from math import sqrt\n",
    "from sklearn import preprocessing\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.utils import check_array\n",
    "# from epftoolbox.models import LEAR\n",
    "from epftoolbox.models import evaluate_lear_in_test_dataset\n",
    "from epftoolbox.evaluation import sMAPE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.robust import mad\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LassoLarsIC, Lasso\n",
    "from epftoolbox.data import scaling\n",
    "from epftoolbox.data import read_data\n",
    "from epftoolbox.evaluation import MAE, sMAPE\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "#months1-7(210)\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[:8738,:]\n",
    "Y_train=Y.iloc[:8738,:]\n",
    "X_test=X.iloc[8738:8739,:]\n",
    "Y_test=Y.iloc[8738:8739,:]\n",
    "\n",
    "#months1-10(300)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:13154,:]\n",
    "# Y_train=Y.iloc[:13154,:]\n",
    "# X_test=X.iloc[13154:13155,:]\n",
    "# Y_test=Y.iloc[13154:13155,:]\n",
    "\n",
    "#months1-13(390)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:17522,:]\n",
    "# Y_train=Y.iloc[:17522,:]\n",
    "# X_test=X.iloc[17522:17523,:]\n",
    "# Y_test=Y.iloc[17522:17523,:]\n",
    "\n",
    "#months1-16(450)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:21842,:]\n",
    "# Y_train=Y.iloc[:21842,:]\n",
    "# X_test=X.iloc[21842:21843,:]\n",
    "# Y_test=Y.iloc[21842:21843,:]\n",
    "\n",
    "def hyperParameterTuning(X_train, Y_train):\n",
    "       param_tuning = {'bootstrap': [True],\n",
    "                   'max_depth': [40, 60, 80],\n",
    "                   'max_features': [40, 60, 80],\n",
    "                   'min_samples_leaf' : [20, 40, 60],\n",
    "                   'min_samples_split' : [2, 4, 6],\n",
    "                   'n_estimators': [800, 1600, 2400]\n",
    "                  }\n",
    " \n",
    "       RF_Regressor = RandomForestRegressor()\n",
    "\n",
    "       gsearch = GridSearchCV(estimator = RF_Regressor,\n",
    "                          param_grid = param_tuning,                        \n",
    "                           scoring = 'neg_mean_absolute_error', #MAE\n",
    "#                            scoring = 'neg_mean_squared_error',  #MSE\n",
    "                          cv = 2,\n",
    "                          n_jobs = -1,\n",
    "                          verbose = 3)\n",
    "\n",
    "       gsearch.fit(X_train,Y_train)\n",
    "       return gsearch.best_params_\n",
    "\n",
    "hyperParameterTuning(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18a8421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Models Search 90 days Training Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pandas import concat\n",
    "from math import sqrt\n",
    "from sklearn import preprocessing\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.utils import check_array\n",
    "# from epftoolbox.models import LEAR\n",
    "from epftoolbox.models import evaluate_lear_in_test_dataset\n",
    "from epftoolbox.evaluation import sMAPE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.robust import mad\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LassoLarsIC, Lasso\n",
    "from epftoolbox.data import scaling\n",
    "from epftoolbox.data import read_data\n",
    "from epftoolbox.evaluation import MAE, sMAPE\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "#months 5-7(90 days)\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[4322:8738,:]\n",
    "Y_train=Y.iloc[4322:8738,:]\n",
    "\n",
    "#months 8-10(90 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[8738:13154,:]\n",
    "# Y_train=Y.iloc[8738:13154,:]\n",
    "\n",
    "#months 11-13(90 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[13154:17522,:]\n",
    "# Y_train=Y.iloc[13154:17522,:]\n",
    "\n",
    "#months 14-16(90 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[17522:21842,:]\n",
    "# Y_train=Y.iloc[17522:21842,:]\n",
    "\n",
    "def hyperParameterTuning(X_train, Y_train):\n",
    "       param_tuning = {'bootstrap': [True],\n",
    "                   'max_depth': [40, 60, 80],\n",
    "                   'max_features': [40, 60, 80],\n",
    "                   'min_samples_leaf' : [20, 40, 60],\n",
    "                   'min_samples_split' : [2, 4, 6],\n",
    "                   'n_estimators': [800, 1600, 2400]\n",
    "                  }\n",
    " \n",
    "       RF_Regressor = RandomForestRegressor()\n",
    "\n",
    "       gsearch = GridSearchCV(estimator = RF_Regressor,\n",
    "                          param_grid = param_tuning,                        \n",
    "                           scoring = 'neg_mean_absolute_error', #MAE\n",
    "#                            scoring = 'neg_mean_squared_error',  #MSE\n",
    "                          cv = 2,\n",
    "                          n_jobs = -1,\n",
    "                          verbose = 3)\n",
    "\n",
    "       gsearch.fit(X_train,Y_train)\n",
    "       return gsearch.best_params_\n",
    "\n",
    "hyperParameterTuning(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b06785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Models Search 60 days Training Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pandas import concat\n",
    "from math import sqrt\n",
    "from sklearn import preprocessing\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.utils import check_array\n",
    "# from epftoolbox.models import LEAR\n",
    "from epftoolbox.models import evaluate_lear_in_test_dataset\n",
    "from epftoolbox.evaluation import sMAPE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.robust import mad\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LassoLarsIC, Lasso\n",
    "from epftoolbox.data import scaling\n",
    "from epftoolbox.data import read_data\n",
    "from epftoolbox.evaluation import MAE, sMAPE\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "#months 6-7(60 days)\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[5810:8738,:]\n",
    "Y_train=Y.iloc[5810:8738,:]\n",
    "\n",
    "#months 9-10(60 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[10178:13154,:]\n",
    "# Y_train=Y.iloc[10178:13154,:]\n",
    "\n",
    "#months 12-13(60 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[14594:17522,:]\n",
    "# Y_train=Y.iloc[14594:17522,:]\n",
    "\n",
    "#months 15-16(60 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[19010:21842,:]\n",
    "# Y_train=Y.iloc[19010:21842,:]\n",
    "\n",
    "def hyperParameterTuning(X_train, Y_train):\n",
    "       param_tuning = {'bootstrap': [True],\n",
    "                   'max_depth': [40, 60, 80],\n",
    "                   'max_features': [40, 60, 80],\n",
    "                   'min_samples_leaf' : [20, 40, 60],\n",
    "                   'min_samples_split' : [2, 4, 6],\n",
    "                   'n_estimators': [800, 1600, 2400]\n",
    "                  }\n",
    " \n",
    "       RF_Regressor = RandomForestRegressor()\n",
    "\n",
    "       gsearch = GridSearchCV(estimator = RF_Regressor,\n",
    "                          param_grid = param_tuning,                        \n",
    "                           scoring = 'neg_mean_absolute_error', #MAE\n",
    "#                            scoring = 'neg_mean_squared_error',  #MSE\n",
    "                          cv = 2,\n",
    "                          n_jobs = -1,\n",
    "                          verbose = 3)\n",
    "\n",
    "       gsearch.fit(X_train,Y_train)\n",
    "       return gsearch.best_params_\n",
    "\n",
    "hyperParameterTuning(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8fc2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Models Search 30 days Training Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pandas import concat\n",
    "from math import sqrt\n",
    "from sklearn import preprocessing\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.utils import check_array\n",
    "# from epftoolbox.models import LEAR\n",
    "from epftoolbox.models import evaluate_lear_in_test_dataset\n",
    "from epftoolbox.evaluation import sMAPE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.robust import mad\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LassoLarsIC, Lasso\n",
    "from epftoolbox.data import scaling\n",
    "from epftoolbox.data import read_data\n",
    "from epftoolbox.evaluation import MAE, sMAPE\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "#months 5-7(30 days)\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[7250:8738,:]\n",
    "Y_train=Y.iloc[7250:8738,:]\n",
    "\n",
    "#months 8-10(30 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[11666:13154,:]\n",
    "# Y_train=Y.iloc[11666:13154,:]\n",
    "\n",
    "#months 11-13(30 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[16082:17522,:]\n",
    "# Y_train=Y.iloc[16082:17522,:]\n",
    "\n",
    "#months 14-16(30 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[20498:21842,:]\n",
    "# Y_train=Y.iloc[20498:21842,:]\n",
    "\n",
    "def hyperParameterTuning(X_train, Y_train):\n",
    "       param_tuning = {'bootstrap': [True],\n",
    "                   'max_depth': [40, 60, 80],\n",
    "                   'max_features': [40, 60, 80],\n",
    "                   'min_samples_leaf' : [20, 40, 60],\n",
    "                   'min_samples_split' : [2, 4, 6],\n",
    "                   'n_estimators': [800, 1600, 2400]\n",
    "                  }\n",
    " \n",
    "       RF_Regressor = RandomForestRegressor()\n",
    "\n",
    "       gsearch = GridSearchCV(estimator = RF_Regressor,\n",
    "                          param_grid = param_tuning,                        \n",
    "                           scoring = 'neg_mean_absolute_error', #MAE\n",
    "#                            scoring = 'neg_mean_squared_error',  #MSE\n",
    "                          cv = 2,\n",
    "                          n_jobs = -1,\n",
    "                          verbose = 3)\n",
    "\n",
    "       gsearch.fit(X_train,Y_train)\n",
    "       return gsearch.best_params_\n",
    "\n",
    "hyperParameterTuning(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87e6fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf9dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extreme Gradient Boosting Models Search Maximum training Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pandas import concat\n",
    "from math import sqrt\n",
    "from sklearn import preprocessing\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.utils import check_array\n",
    "from epftoolbox.models import evaluate_lear_in_test_dataset\n",
    "from epftoolbox.evaluation import sMAPE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.robust import mad\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LassoLarsIC, Lasso\n",
    "from epftoolbox.data import scaling\n",
    "from epftoolbox.data import read_data\n",
    "from epftoolbox.evaluation import MAE, sMAPE\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "#months1-7(210)\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[:8738,:]\n",
    "Y_train=Y.iloc[:8738,:]\n",
    "X_test=X.iloc[8738:8739,:]\n",
    "Y_test=Y.iloc[8738:8739,:]\n",
    "\n",
    "#months1-10(300)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:13154,:]\n",
    "# Y_train=Y.iloc[:13154,:]\n",
    "# X_test=X.iloc[13154:13155,:]\n",
    "# Y_test=Y.iloc[13154:13155,:]\n",
    "\n",
    "#months1-13(390)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:17522,:]\n",
    "# Y_train=Y.iloc[:17522,:]\n",
    "# X_test=X.iloc[17522:17523,:]\n",
    "# Y_test=Y.iloc[17522:17523,:]\n",
    "\n",
    "#months1-16(450)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:21842,:]\n",
    "# Y_train=Y.iloc[:21842,:]\n",
    "# X_test=X.iloc[21842:21843,:]\n",
    "# Y_test=Y.iloc[21842:21843,:]\n",
    "\n",
    "def hyperParameterTuning(X_train, Y_train):\n",
    "       param_tuning = {'max_depth': [1, 2, 4],\n",
    "                       'learning_rate': [.01, 0.03, 0.05, 0.08, 0.1],\n",
    "                       'n_estimators': [25, 50, 75, 100, 200]\n",
    "                  }\n",
    " \n",
    "       RF_Regressor = XGBRegressor()\n",
    "    \n",
    "       gsearch = GridSearchCV(estimator = RF_Regressor,\n",
    "                          param_grid = param_tuning,                        \n",
    "                           scoring = 'neg_mean_absolute_error', #MAE\n",
    "#                            scoring = 'neg_mean_squared_error',  #MSE\n",
    "                          cv = 2,\n",
    "                          n_jobs = -1,\n",
    "                          verbose = 0)\n",
    "\n",
    "       gsearch.fit(X_train,Y_train)\n",
    "       return gsearch.best_params_\n",
    "\n",
    "hyperParameterTuning(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fec4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extreme Gradient Boosting Models Search 90 days Training Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pandas import concat\n",
    "from math import sqrt\n",
    "from sklearn import preprocessing\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.utils import check_array\n",
    "from epftoolbox.models import evaluate_lear_in_test_dataset\n",
    "from epftoolbox.evaluation import sMAPE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.robust import mad\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LassoLarsIC, Lasso\n",
    "from epftoolbox.data import scaling\n",
    "from epftoolbox.data import read_data\n",
    "from epftoolbox.evaluation import MAE, sMAPE\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "#months 5-7(90 days)\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[4322:8738,:]\n",
    "Y_train=Y.iloc[4322:8738,:]\n",
    "\n",
    "#months 8-10(90 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[8738:13154,:]\n",
    "# Y_train=Y.iloc[8738:13154,:]\n",
    "\n",
    "#months 11-13(90 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[13154:17522,:]\n",
    "# Y_train=Y.iloc[13154:17522,:]\n",
    "\n",
    "#months 14-16(90 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[17522:21842,:]\n",
    "# Y_train=Y.iloc[17522:21842,:]\n",
    "\n",
    "def hyperParameterTuning(X_train, Y_train):\n",
    "       param_tuning = {'max_depth': [1, 2, 4],\n",
    "                       'learning_rate': [.01, 0.03, 0.05, 0.08, 0.1],\n",
    "                       'n_estimators': [25, 50, 75, 100, 200]\n",
    "                  }\n",
    " \n",
    "       RF_Regressor = XGBRegressor()\n",
    "    \n",
    "       gsearch = GridSearchCV(estimator = RF_Regressor,\n",
    "                          param_grid = param_tuning,                        \n",
    "                           scoring = 'neg_mean_absolute_error', #MAE\n",
    "#                            scoring = 'neg_mean_squared_error',  #MSE\n",
    "                          cv = 2,\n",
    "                          n_jobs = -1,\n",
    "                          verbose = 0)\n",
    "\n",
    "       gsearch.fit(X_train,Y_train)\n",
    "       return gsearch.best_params_\n",
    "\n",
    "hyperParameterTuning(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c239d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extreme Gradient Boosting Models Search 60 days Training Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pandas import concat\n",
    "from math import sqrt\n",
    "from sklearn import preprocessing\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.utils import check_array\n",
    "from epftoolbox.models import evaluate_lear_in_test_dataset\n",
    "from epftoolbox.evaluation import sMAPE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.robust import mad\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LassoLarsIC, Lasso\n",
    "from epftoolbox.data import scaling\n",
    "from epftoolbox.data import read_data\n",
    "from epftoolbox.evaluation import MAE, sMAPE\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "#months 6-7(60 days)\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[5810:8738,:]\n",
    "Y_train=Y.iloc[5810:8738,:]\n",
    "\n",
    "#months 9-10(60 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[10178:13154,:]\n",
    "# Y_train=Y.iloc[10178:13154,:]\n",
    "\n",
    "#months 12-13(60 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[14594:17522,:]\n",
    "# Y_train=Y.iloc[14594:17522,:]\n",
    "\n",
    "#months 15-16(60 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[19010:21842,:]\n",
    "# Y_train=Y.iloc[19010:21842,:]\n",
    "\n",
    "def hyperParameterTuning(X_train, Y_train):\n",
    "       param_tuning = {'max_depth': [1, 2, 4],\n",
    "                       'learning_rate': [.01, 0.03, 0.05, 0.08, 0.1],\n",
    "                       'n_estimators': [25, 50, 75, 100, 200]\n",
    "                  }\n",
    " \n",
    "       RF_Regressor = XGBRegressor()\n",
    "    \n",
    "       gsearch = GridSearchCV(estimator = RF_Regressor,\n",
    "                          param_grid = param_tuning,                        \n",
    "                           scoring = 'neg_mean_absolute_error', #MAE\n",
    "#                            scoring = 'neg_mean_squared_error',  #MSE\n",
    "                          cv = 2,\n",
    "                          n_jobs = -1,\n",
    "                          verbose = 0)\n",
    "\n",
    "       gsearch.fit(X_train,Y_train)\n",
    "       return gsearch.best_params_\n",
    "\n",
    "hyperParameterTuning(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extreme Gradient Boosting Models Search 30 days Training Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pandas import concat\n",
    "from math import sqrt\n",
    "from sklearn import preprocessing\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.utils import check_array\n",
    "from epftoolbox.models import evaluate_lear_in_test_dataset\n",
    "from epftoolbox.evaluation import sMAPE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.robust import mad\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LassoLarsIC, Lasso\n",
    "from epftoolbox.data import scaling\n",
    "from epftoolbox.data import read_data\n",
    "from epftoolbox.evaluation import MAE, sMAPE\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "#months 5-7(30 days)\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[7250:8738,:]\n",
    "Y_train=Y.iloc[7250:8738,:]\n",
    "\n",
    "#months 8-10(30 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[11666:13154,:]\n",
    "# Y_train=Y.iloc[11666:13154,:]\n",
    "\n",
    "#months 11-13(30 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[16082:17522,:]\n",
    "# Y_train=Y.iloc[16082:17522,:]\n",
    "\n",
    "#months 14-16(30 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[20498:21842,:]\n",
    "# Y_train=Y.iloc[20498:21842,:]\n",
    "\n",
    "def hyperParameterTuning(X_train, Y_train):\n",
    "       param_tuning = {'max_depth': [1, 2, 4],\n",
    "                       'learning_rate': [.01, 0.03, 0.05, 0.08, 0.1],\n",
    "                       'n_estimators': [25, 50, 75, 100, 200]\n",
    "                  }\n",
    " \n",
    "       RF_Regressor = XGBRegressor()\n",
    "    \n",
    "       gsearch = GridSearchCV(estimator = RF_Regressor,\n",
    "                          param_grid = param_tuning,                        \n",
    "                           scoring = 'neg_mean_absolute_error', #MAE\n",
    "#                            scoring = 'neg_mean_squared_error',  #MSE\n",
    "                          cv = 2,\n",
    "                          n_jobs = -1,\n",
    "                          verbose = 0)\n",
    "\n",
    "       gsearch.fit(X_train,Y_train)\n",
    "       return gsearch.best_params_\n",
    "\n",
    "hyperParameterTuning(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce47afc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Regression Models Search Maximum training Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pandas import concat\n",
    "from math import sqrt\n",
    "from sklearn import preprocessing\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.utils import check_array\n",
    "from epftoolbox.models import evaluate_lear_in_test_dataset\n",
    "from epftoolbox.evaluation import sMAPE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.robust import mad\n",
    "import os\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "#months1-7(210)\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[:8738,:]\n",
    "Y_train=Y.iloc[:8738,:]\n",
    "X_test=X.iloc[8738:8739,:]\n",
    "Y_test=Y.iloc[8738:8739,:]\n",
    "\n",
    "#months1-10(300)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:13154,:]\n",
    "# Y_train=Y.iloc[:13154,:]\n",
    "# X_test=X.iloc[13154:13155,:]\n",
    "# Y_test=Y.iloc[13154:13155,:]\n",
    "\n",
    "#months1-13(390)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:17522,:]\n",
    "# Y_train=Y.iloc[:17522,:]\n",
    "# X_test=X.iloc[17522:17523,:]\n",
    "# Y_test=Y.iloc[17522:17523,:]\n",
    "\n",
    "#months1-16(450)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:21842,:]\n",
    "# Y_train=Y.iloc[:21842,:]\n",
    "# X_test=X.iloc[21842:21843,:]\n",
    "# Y_test=Y.iloc[21842:21843,:]\n",
    "\n",
    "def hyperParameterTuning(X_train, Y_train):\n",
    "       param_tuning = { 'C': [0.01, 0.1],\n",
    "                        'gamma': [0.0001, 0.001, 0.005],\n",
    "                        'epsilon': [0.001, 0.1, 0.3],\n",
    "                        'kernel': [\"rbf\"]\n",
    "                      }\n",
    " \n",
    "       RF_Regressor = SVR()\n",
    "\n",
    "       gsearch = GridSearchCV(estimator = RF_Regressor,\n",
    "                          param_grid = param_tuning,                        \n",
    "                           scoring = 'neg_mean_absolute_error', #MAE\n",
    "#                            scoring = 'neg_mean_squared_error',  #MSE\n",
    "                          cv = 2,\n",
    "                          n_jobs = -1,\n",
    "                          verbose = 0)\n",
    "\n",
    "       gsearch.fit(X_train,Y_train)\n",
    "       return gsearch.best_params_\n",
    "\n",
    "hyperParameterTuning(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9897c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Regression Models Search 90 days Training Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pandas import concat\n",
    "from math import sqrt\n",
    "from sklearn import preprocessing\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.utils import check_array\n",
    "from epftoolbox.models import evaluate_lear_in_test_dataset\n",
    "from epftoolbox.evaluation import sMAPE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.robust import mad\n",
    "import os\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "#months 5-7(90 days)\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[4322:8738,:]\n",
    "Y_train=Y.iloc[4322:8738,:]\n",
    "\n",
    "#months 8-10(90 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[8738:13154,:]\n",
    "# Y_train=Y.iloc[8738:13154,:]\n",
    "\n",
    "#months 11-13(90 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[13154:17522,:]\n",
    "# Y_train=Y.iloc[13154:17522,:]\n",
    "\n",
    "#months 14-16(90 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[17522:21842,:]\n",
    "# Y_train=Y.iloc[17522:21842,:]\n",
    "\n",
    "def hyperParameterTuning(X_train, Y_train):\n",
    "       param_tuning = { 'C': [0.01, 0.1],\n",
    "                        'gamma': [0.0001, 0.001, 0.005],\n",
    "                        'epsilon': [0.001, 0.1, 0.3],\n",
    "                        'kernel': [\"rbf\"]\n",
    "                      }\n",
    " \n",
    "       RF_Regressor = SVR()\n",
    "\n",
    "       gsearch = GridSearchCV(estimator = RF_Regressor,\n",
    "                          param_grid = param_tuning,                        \n",
    "                           scoring = 'neg_mean_absolute_error', #MAE\n",
    "#                            scoring = 'neg_mean_squared_error',  #MSE\n",
    "                          cv = 2,\n",
    "                          n_jobs = -1,\n",
    "                          verbose = 0)\n",
    "\n",
    "       gsearch.fit(X_train,Y_train)\n",
    "       return gsearch.best_params_\n",
    "\n",
    "hyperParameterTuning(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb53c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Regression Models Search 60 days Training Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pandas import concat\n",
    "from math import sqrt\n",
    "from sklearn import preprocessing\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.utils import check_array\n",
    "from epftoolbox.models import evaluate_lear_in_test_dataset\n",
    "from epftoolbox.evaluation import sMAPE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.robust import mad\n",
    "import os\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "#months 6-7(60 days)\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[5810:8738,:]\n",
    "Y_train=Y.iloc[5810:8738,:]\n",
    "\n",
    "#months 9-10(60 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[10178:13154,:]\n",
    "# Y_train=Y.iloc[10178:13154,:]\n",
    "\n",
    "#months 12-13(60 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[14594:17522,:]\n",
    "# Y_train=Y.iloc[14594:17522,:]\n",
    "\n",
    "#months 15-16(60 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[19010:21842,:]\n",
    "# Y_train=Y.iloc[19010:21842,:]\n",
    "\n",
    "def hyperParameterTuning(X_train, Y_train):\n",
    "       param_tuning = { 'C': [0.01, 0.1],\n",
    "                        'gamma': [0.0001, 0.001, 0.005],\n",
    "                        'epsilon': [0.001, 0.1, 0.3],\n",
    "                        'kernel': [\"rbf\"]\n",
    "                      }\n",
    " \n",
    "       RF_Regressor = SVR()\n",
    "\n",
    "       gsearch = GridSearchCV(estimator = RF_Regressor,\n",
    "                          param_grid = param_tuning,                        \n",
    "                           scoring = 'neg_mean_absolute_error', #MAE\n",
    "#                            scoring = 'neg_mean_squared_error',  #MSE\n",
    "                          cv = 2,\n",
    "                          n_jobs = -1,\n",
    "                          verbose = 0)\n",
    "\n",
    "       gsearch.fit(X_train,Y_train)\n",
    "       return gsearch.best_params_\n",
    "\n",
    "hyperParameterTuning(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42418fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Regression Models Search 30 days Training Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pandas import concat\n",
    "from math import sqrt\n",
    "from sklearn import preprocessing\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.utils import check_array\n",
    "from epftoolbox.models import evaluate_lear_in_test_dataset\n",
    "from epftoolbox.evaluation import sMAPE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.robust import mad\n",
    "import os\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "#months 5-7(30 days)\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[7250:8738,:]\n",
    "Y_train=Y.iloc[7250:8738,:]\n",
    "\n",
    "#months 8-10(30 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[11666:13154,:]\n",
    "# Y_train=Y.iloc[11666:13154,:]\n",
    "\n",
    "#months 11-13(30 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[16082:17522,:]\n",
    "# Y_train=Y.iloc[16082:17522,:]\n",
    "\n",
    "#months 14-16(30 days)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[20498:21842,:]\n",
    "# Y_train=Y.iloc[20498:21842,:]\n",
    "\n",
    "def hyperParameterTuning(X_train, Y_train):\n",
    "       param_tuning = { 'C': [0.01, 0.1],\n",
    "                        'gamma': [0.0001, 0.001, 0.005],\n",
    "                        'epsilon': [0.001, 0.1, 0.3],\n",
    "                        'kernel': [\"rbf\"]\n",
    "                      }\n",
    " \n",
    "       RF_Regressor = SVR()\n",
    "\n",
    "       gsearch = GridSearchCV(estimator = RF_Regressor,\n",
    "                          param_grid = param_tuning,                        \n",
    "                           scoring = 'neg_mean_absolute_error', #MAE\n",
    "#                            scoring = 'neg_mean_squared_error',  #MSE\n",
    "                          cv = 2,\n",
    "                          n_jobs = -1,\n",
    "                          verbose = 0)\n",
    "\n",
    "       gsearch.fit(X_train,Y_train)\n",
    "       return gsearch.best_params_\n",
    "\n",
    "hyperParameterTuning(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a62ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc71f0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single Headed DNN Models Search Maximum training Data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import concat\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from math import floor\n",
    "from bayes_opt import BayesianOptimization\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "#tensorflow &keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "from keras.activations import *\n",
    "\n",
    "from keras.layers import LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "\n",
    "#sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import talos \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "date_format=\"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat=pd.DataFrame(dat)\n",
    "dat=dat.bfill(axis ='rows')\n",
    "dat=dat.ffill(axis ='rows')\n",
    "dat=dat._get_numeric_data()\n",
    "\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[:7250,:]\n",
    "Y_train=Y.iloc[:7250,:]\n",
    "X_test=X.iloc[7250:8739,:]\n",
    "Y_test=Y.iloc[7250:8739,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:11666,:]\n",
    "# Y_train=Y.iloc[:11666,:]\n",
    "# X_test=X.iloc[11666:13155,:]\n",
    "# Y_test=Y.iloc[11666:13155,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:16082,:]\n",
    "# Y_train=Y.iloc[:16082,:]\n",
    "# X_test=X.iloc[16082:17523,:]\n",
    "# Y_test=Y.iloc[16082:17523,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:20498,:]\n",
    "# Y_train=Y.iloc[:20498,:]\n",
    "# X_test=X.iloc[20498:21843,:]\n",
    "# Y_test=Y.iloc[20498:21843,:]\n",
    "\n",
    "rnn_train1_a=X_train[[\"lag_-3x1\", \"lag_-4x1\",\"lag_-5x1\",\"lag_-6x1\",\"lag_-7x1\", \"lag_-8x1\",\"lag_-9x1\",\"lag_-10x1\",\"lag_-11x1\", \"lag_-12x1\",\"lag_-13x1\",\"lag_-14x1\",\"lag_-15x1\", \"lag_-16x1\",\"lag_-17x1\",\"lag_-18x1\"]]\n",
    "rnn_train1_b=X_train[[\"lag_-19x1\", \"lag_-20x1\",\"lag_-21x1\",\"lag_-22x1\",\"lag_-23x1\", \"lag_-24x1\",\"lag_-25x1\",\"lag_-26x1\",\"lag_-27x1\", \"lag_-28x1\",\"lag_-29x1\",\"lag_-30x1\",\"lag_-31x1\", \"lag_-32x1\",\"lag_-33x1\",\"lag_-34x1\"]]\n",
    "rnn_train1_c=X_train[[\"lag_-35x1\", \"lag_-36x1\",\"lag_-37x1\",\"lag_-38x1\",\"lag_-39x1\", \"lag_-40x1\",\"lag_-41x1\",\"lag_-42x1\",\"lag_-43x1\", \"lag_-44x1\",\"lag_-45x1\",\"lag_-46x1\",\"lag_-47x1\", \"lag_-48x1\",\"lag_-49x1\",\"lag_-50x1\"]]\n",
    "\n",
    "rnn_train2_a=X_train[[\"lag_-3x2\", \"lag_-4x2\",\"lag_-5x2\",\"lag_-6x2\",\"lag_-7x2\", \"lag_-8x2\",\"lag_-9x2\",\"lag_-10x2\",\"lag_-11x2\", \"lag_-12x2\",\"lag_-13x2\",\"lag_-14x2\",\"lag_-15x2\", \"lag_-16x2\",\"lag_-17x2\",\"lag_-18x2\"]]\n",
    "rnn_train2_b=X_train[[\"lag_-19x2\", \"lag_-20x2\",\"lag_-21x2\",\"lag_-22x2\",\"lag_-23x2\", \"lag_-24x2\",\"lag_-25x2\",\"lag_-26x2\",\"lag_-27x2\", \"lag_-28x2\",\"lag_-29x2\",\"lag_-30x2\",\"lag_-31x2\", \"lag_-32x2\",\"lag_-33x2\",\"lag_-34x2\"]]\n",
    "rnn_train2_c=X_train[[\"lag_-35x2\", \"lag_-36x2\",\"lag_-37x2\",\"lag_-38x2\",\"lag_-39x2\", \"lag_-40x2\",\"lag_-41x2\",\"lag_-42x2\",\"lag_-43x2\", \"lag_-44x2\",\"lag_-45x2\",\"lag_-46x2\",\"lag_-47x2\", \"lag_-48x2\",\"lag_-49x2\",\"lag_-50x2\"]]\n",
    "\n",
    "rnn_train3_a=X_train[[\"lag_-2x3\",\"lag_-3x3\", \"lag_-4x3\",\"lag_-5x3\",\"lag_-6x3\",\"lag_-7x3\", \"lag_-8x3\",\"lag_-9x3\",\"lag_-10x3\",\"lag_-11x3\", \"lag_-12x3\",\"lag_-13x3\",\"lag_-14x3\",\"lag_-15x3\", \"lag_-16x3\",\"lag_-17x3\"]]\n",
    "rnn_train3_b=X_train[[\"lag_-18x3\",\"lag_-19x3\", \"lag_-20x3\",\"lag_-21x3\",\"lag_-22x3\",\"lag_-23x3\", \"lag_-24x3\",\"lag_-25x3\",\"lag_-26x3\",\"lag_-27x3\", \"lag_-28x3\",\"lag_-29x3\",\"lag_-30x3\",\"lag_-31x3\", \"lag_-32x3\",\"lag_-33x3\"]]\n",
    "rnn_train3_c=X_train[[\"lag_-34x3\",\"lag_-35x3\", \"lag_-36x3\",\"lag_-37x3\",\"lag_-38x3\",\"lag_-39x3\", \"lag_-40x3\",\"lag_-41x3\",\"lag_-42x3\",\"lag_-43x3\", \"lag_-44x3\",\"lag_-45x3\",\"lag_-46x3\",\"lag_-47x3\", \"lag_-48x3\",\"lag_-49x3\"]]\n",
    "\n",
    "rnn_train4_a=X_train[[\"lag_0x6\",\"lag_-1x6\",\"lag_-2x6\",\"lag_-3x6\", \"lag_-4x6\",\"lag_-5x6\",\"lag_-6x6\",\"lag_-7x6\", \"lag_-8x6\",\"lag_-9x6\",\"lag_-10x6\",\"lag_-11x6\", \"lag_-12x6\",\"lag_-13x6\",\"lag_-14x6\",\"lag_-15x6\"]]\n",
    "rnn_train4_b=X_train[[\"lag_-16x6\",\"lag_-17x6\",\"lag_-18x6\",\"lag_-19x6\", \"lag_-20x6\",\"lag_-21x6\",\"lag_-22x6\",\"lag_-23x6\", \"lag_-24x6\",\"lag_-25x6\",\"lag_-26x6\",\"lag_-27x6\", \"lag_-28x6\",\"lag_-29x6\",\"lag_-30x6\",\"lag_-31x6\"]]\n",
    "rnn_train4_c=X_train[[\"lag_-32x6\",\"lag_-33x6\",\"lag_-34x6\",\"lag_-35x6\", \"lag_-36x6\",\"lag_-37x6\",\"lag_-38x6\",\"lag_-39x6\", \"lag_-40x6\",\"lag_-41x6\",\"lag_-42x6\",\"lag_-43x6\", \"lag_-44x6\",\"lag_-45x6\",\"lag_-46x6\",\"lag_-47x6\"]]\n",
    "\n",
    "rnn_train5_a=X_train[[\"lag_-2x12\",\"lag_-3x12\", \"lag_-4x12\",\"lag_-5x12\",\"lag_-6x12\",\"lag_-7x12\", \"lag_-8x12\",\"lag_-9x12\",\"lag_-10x12\",\"lag_-11x12\", \"lag_-12x12\",\"lag_-13x12\",\"lag_-14x12\",\"lag_-15x12\", \"lag_-16x12\",\"lag_-17x12\"]]\n",
    "rnn_train5_b=X_train[[\"lag_-18x12\",\"lag_-19x12\", \"lag_-20x12\",\"lag_-21x12\",\"lag_-22x12\",\"lag_-23x12\", \"lag_-24x12\",\"lag_-25x12\",\"lag_-26x12\",\"lag_-27x12\", \"lag_-28x12\",\"lag_-29x12\",\"lag_-30x12\",\"lag_-31x12\", \"lag_-32x12\",\"lag_-33x12\"]]\n",
    "rnn_train5_c=X_train[[\"lag_-34x12\",\"lag_-35x12\", \"lag_-36x12\",\"lag_-37x12\",\"lag_-38x12\",\"lag_-39x12\", \"lag_-40x12\",\"lag_-41x12\",\"lag_-42x12\",\"lag_-43x12\", \"lag_-44x12\",\"lag_-45x12\",\"lag_-46x12\",\"lag_-47x12\", \"lag_-48x12\",\"lag_-49x12\"]]\n",
    "\n",
    "rnn_train6=X_train[[\"lag_2x7\",\"lag_3x7\",\"lag_4x7\",\"lag_5x7\",\"lag_6x7\",\"lag_7x7\",\"lag_8x7\",\"lag_9x7\",\"lag_10x7\",\"lag_11x7\",\"lag_12x7\",\"lag_13x7\",\"lag_14x7\",\"lag_15x7\",\"lag_16x7\",\"lag_17x7\"]]\n",
    "rnn_train7=X_train[[\"lag_2x8\",\"lag_3x8\",\"lag_4x8\",\"lag_5x8\",\"lag_6x8\",\"lag_7x8\",\"lag_8x8\",\"lag_9x8\",\"lag_10x8\",\"lag_11x8\",\"lag_12x8\",\"lag_13x8\",\"lag_14x8\",\"lag_15x8\",\"lag_16x8\",\"lag_17x8\"]]\n",
    "rnn_train8=X_train[[\"lag_2x9\",\"lag_3x9\",\"lag_4x9\",\"lag_5x9\",\"lag_6x9\",\"lag_7x9\",\"lag_8x9\",\"lag_9x9\",\"lag_10x9\",\"lag_11x9\",\"lag_12x9\",\"lag_13x9\",\"lag_14x9\",\"lag_15x9\",\"lag_16x9\",\"lag_17x9\"]]\n",
    "rnn_train9=X_train[[\"lag_2x10\",\"lag_3x10\",\"lag_4x10\",\"lag_5x10\",\"lag_6x10\",\"lag_7x10\",\"lag_8x10\",\"lag_9x10\",\"lag_10x10\",\"lag_11x10\",\"lag_12x10\",\"lag_13x10\",\"lag_14x10\",\"lag_15x10\",\"lag_16x10\",\"lag_17x10\"]]\n",
    "rnn_train10=X_train[[\"lag_2x11\",\"lag_3x11\",\"lag_4x11\",\"lag_5x11\",\"lag_6x11\",\"lag_7x11\",\"lag_8x11\",\"lag_9x11\",\"lag_10x11\",\"lag_11x11\",\"lag_12x11\",\"lag_13x11\",\"lag_14x11\",\"lag_15x11\",\"lag_16x11\",\"lag_17x11\"]]\n",
    "\n",
    "\n",
    "\n",
    "rnn_test1_a=X_test[[\"lag_-3x1\", \"lag_-4x1\",\"lag_-5x1\",\"lag_-6x1\",\"lag_-7x1\", \"lag_-8x1\",\"lag_-9x1\",\"lag_-10x1\",\"lag_-11x1\", \"lag_-12x1\",\"lag_-13x1\",\"lag_-14x1\",\"lag_-15x1\", \"lag_-16x1\",\"lag_-17x1\",\"lag_-18x1\"]]\n",
    "rnn_test1_b=X_test[[\"lag_-19x1\", \"lag_-20x1\",\"lag_-21x1\",\"lag_-22x1\",\"lag_-23x1\", \"lag_-24x1\",\"lag_-25x1\",\"lag_-26x1\",\"lag_-27x1\", \"lag_-28x1\",\"lag_-29x1\",\"lag_-30x1\",\"lag_-31x1\", \"lag_-32x1\",\"lag_-33x1\",\"lag_-34x1\"]]\n",
    "rnn_test1_c=X_test[[\"lag_-35x1\", \"lag_-36x1\",\"lag_-37x1\",\"lag_-38x1\",\"lag_-39x1\", \"lag_-40x1\",\"lag_-41x1\",\"lag_-42x1\",\"lag_-43x1\", \"lag_-44x1\",\"lag_-45x1\",\"lag_-46x1\",\"lag_-47x1\", \"lag_-48x1\",\"lag_-49x1\",\"lag_-50x1\"]]\n",
    "\n",
    "rnn_test2_a=X_test[[\"lag_-3x2\", \"lag_-4x2\",\"lag_-5x2\",\"lag_-6x2\",\"lag_-7x2\", \"lag_-8x2\",\"lag_-9x2\",\"lag_-10x2\",\"lag_-11x2\", \"lag_-12x2\",\"lag_-13x2\",\"lag_-14x2\",\"lag_-15x2\", \"lag_-16x2\",\"lag_-17x2\",\"lag_-18x2\"]]\n",
    "rnn_test2_b=X_test[[\"lag_-19x2\", \"lag_-20x2\",\"lag_-21x2\",\"lag_-22x2\",\"lag_-23x2\", \"lag_-24x2\",\"lag_-25x2\",\"lag_-26x2\",\"lag_-27x2\", \"lag_-28x2\",\"lag_-29x2\",\"lag_-30x2\",\"lag_-31x2\", \"lag_-32x2\",\"lag_-33x2\",\"lag_-34x2\"]]\n",
    "rnn_test2_c=X_test[[\"lag_-35x2\", \"lag_-36x2\",\"lag_-37x2\",\"lag_-38x2\",\"lag_-39x2\", \"lag_-40x2\",\"lag_-41x2\",\"lag_-42x2\",\"lag_-43x2\", \"lag_-44x2\",\"lag_-45x2\",\"lag_-46x2\",\"lag_-47x2\", \"lag_-48x2\",\"lag_-49x2\",\"lag_-50x2\"]]\n",
    "\n",
    "rnn_test3_a=X_test[[\"lag_-2x3\",\"lag_-3x3\", \"lag_-4x3\",\"lag_-5x3\",\"lag_-6x3\",\"lag_-7x3\", \"lag_-8x3\",\"lag_-9x3\",\"lag_-10x3\",\"lag_-11x3\", \"lag_-12x3\",\"lag_-13x3\",\"lag_-14x3\",\"lag_-15x3\", \"lag_-16x3\",\"lag_-17x3\"]]\n",
    "rnn_test3_b=X_test[[\"lag_-18x3\",\"lag_-19x3\", \"lag_-20x3\",\"lag_-21x3\",\"lag_-22x3\",\"lag_-23x3\", \"lag_-24x3\",\"lag_-25x3\",\"lag_-26x3\",\"lag_-27x3\", \"lag_-28x3\",\"lag_-29x3\",\"lag_-30x3\",\"lag_-31x3\", \"lag_-32x3\",\"lag_-33x3\"]]\n",
    "rnn_test3_c=X_test[[\"lag_-34x3\",\"lag_-35x3\", \"lag_-36x3\",\"lag_-37x3\",\"lag_-38x3\",\"lag_-39x3\", \"lag_-40x3\",\"lag_-41x3\",\"lag_-42x3\",\"lag_-43x3\", \"lag_-44x3\",\"lag_-45x3\",\"lag_-46x3\",\"lag_-47x3\", \"lag_-48x3\",\"lag_-49x3\"]]\n",
    "\n",
    "rnn_test4_a=X_test[[\"lag_0x6\",\"lag_-1x6\",\"lag_-2x6\",\"lag_-3x6\", \"lag_-4x6\",\"lag_-5x6\",\"lag_-6x6\",\"lag_-7x6\", \"lag_-8x6\",\"lag_-9x6\",\"lag_-10x6\",\"lag_-11x6\", \"lag_-12x6\",\"lag_-13x6\",\"lag_-14x6\",\"lag_-15x6\"]]\n",
    "rnn_test4_b=X_test[[\"lag_-16x6\",\"lag_-17x6\",\"lag_-18x6\",\"lag_-19x6\", \"lag_-20x6\",\"lag_-21x6\",\"lag_-22x6\",\"lag_-23x6\", \"lag_-24x6\",\"lag_-25x6\",\"lag_-26x6\",\"lag_-27x6\", \"lag_-28x6\",\"lag_-29x6\",\"lag_-30x6\",\"lag_-31x6\"]]\n",
    "rnn_test4_c=X_test[[\"lag_-32x6\",\"lag_-33x6\",\"lag_-34x6\",\"lag_-35x6\", \"lag_-36x6\",\"lag_-37x6\",\"lag_-38x6\",\"lag_-39x6\", \"lag_-40x6\",\"lag_-41x6\",\"lag_-42x6\",\"lag_-43x6\", \"lag_-44x6\",\"lag_-45x6\",\"lag_-46x6\",\"lag_-47x6\"]]\n",
    "\n",
    "rnn_test5_a=X_test[[\"lag_-2x12\",\"lag_-3x12\", \"lag_-4x12\",\"lag_-5x12\",\"lag_-6x12\",\"lag_-7x12\", \"lag_-8x12\",\"lag_-9x12\",\"lag_-10x12\",\"lag_-11x12\", \"lag_-12x12\",\"lag_-13x12\",\"lag_-14x12\",\"lag_-15x12\", \"lag_-16x12\",\"lag_-17x12\"]]\n",
    "rnn_test5_b=X_test[[\"lag_-18x12\",\"lag_-19x12\", \"lag_-20x12\",\"lag_-21x12\",\"lag_-22x12\",\"lag_-23x12\", \"lag_-24x12\",\"lag_-25x12\",\"lag_-26x12\",\"lag_-27x12\", \"lag_-28x12\",\"lag_-29x12\",\"lag_-30x12\",\"lag_-31x12\", \"lag_-32x12\",\"lag_-33x12\"]]\n",
    "rnn_test5_c=X_test[[\"lag_-34x12\",\"lag_-35x12\", \"lag_-36x12\",\"lag_-37x12\",\"lag_-38x12\",\"lag_-39x12\", \"lag_-40x12\",\"lag_-41x12\",\"lag_-42x12\",\"lag_-43x12\", \"lag_-44x12\",\"lag_-45x12\",\"lag_-46x12\",\"lag_-47x12\", \"lag_-48x12\",\"lag_-49x12\"]]\n",
    "\n",
    "rnn_test6=X_test[[\"lag_2x7\",\"lag_3x7\",\"lag_4x7\",\"lag_5x7\",\"lag_6x7\",\"lag_7x7\",\"lag_8x7\",\"lag_9x7\",\"lag_10x7\",\"lag_11x7\",\"lag_12x7\",\"lag_13x7\",\"lag_14x7\",\"lag_15x7\",\"lag_16x7\",\"lag_17x7\"]]\n",
    "rnn_test7=X_test[[\"lag_2x8\",\"lag_3x8\",\"lag_4x8\",\"lag_5x8\",\"lag_6x8\",\"lag_7x8\",\"lag_8x8\",\"lag_9x8\",\"lag_10x8\",\"lag_11x8\",\"lag_12x8\",\"lag_13x8\",\"lag_14x8\",\"lag_15x8\",\"lag_16x8\",\"lag_17x8\"]]\n",
    "rnn_test8=X_test[[\"lag_2x9\",\"lag_3x9\",\"lag_4x9\",\"lag_5x9\",\"lag_6x9\",\"lag_7x9\",\"lag_8x9\",\"lag_9x9\",\"lag_10x9\",\"lag_11x9\",\"lag_12x9\",\"lag_13x9\",\"lag_14x9\",\"lag_15x9\",\"lag_16x9\",\"lag_17x9\"]]\n",
    "rnn_test9=X_test[[\"lag_2x10\",\"lag_3x10\",\"lag_4x10\",\"lag_5x10\",\"lag_6x10\",\"lag_7x10\",\"lag_8x10\",\"lag_9x10\",\"lag_10x10\",\"lag_11x10\",\"lag_12x10\",\"lag_13x10\",\"lag_14x10\",\"lag_15x10\",\"lag_16x10\",\"lag_17x10\"]]\n",
    "rnn_test10=X_test[[\"lag_2x11\",\"lag_3x11\",\"lag_4x11\",\"lag_5x11\",\"lag_6x11\",\"lag_7x11\",\"lag_8x11\",\"lag_9x11\",\"lag_10x11\",\"lag_11x11\",\"lag_12x11\",\"lag_13x11\",\"lag_14x11\",\"lag_15x11\",\"lag_16x11\",\"lag_17x11\"]]\n",
    "\n",
    "rnn_Y=Y_train[[\"lag_2y\", \"lag_3y\", \"lag_4y\", \"lag_5y\", \"lag_6y\", \"lag_7y\", \"lag_8y\", \"lag_9y\", \"lag_10y\", \"lag_11y\", \"lag_12y\", \"lag_13y\", \"lag_14y\", \"lag_15y\", \"lag_16y\", \"lag_17y\"]]\n",
    "\n",
    "X_scaler1_a = preprocessing.MinMaxScaler()\n",
    "X_scaler1_b = preprocessing.MinMaxScaler()\n",
    "X_scaler1_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler2_a = preprocessing.MinMaxScaler()\n",
    "X_scaler2_b = preprocessing.MinMaxScaler()\n",
    "X_scaler2_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler3_a = preprocessing.MinMaxScaler()\n",
    "X_scaler3_b = preprocessing.MinMaxScaler()\n",
    "X_scaler3_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler4_a = preprocessing.MinMaxScaler()\n",
    "X_scaler4_b = preprocessing.MinMaxScaler()\n",
    "X_scaler4_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler5_a = preprocessing.MinMaxScaler()\n",
    "X_scaler5_b = preprocessing.MinMaxScaler()\n",
    "X_scaler5_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "\n",
    "X_scaler6 = preprocessing.MinMaxScaler()\n",
    "X_scaler7 = preprocessing.MinMaxScaler()\n",
    "X_scaler8 = preprocessing.MinMaxScaler()\n",
    "X_scaler9 = preprocessing.MinMaxScaler()\n",
    "X_scaler10 = preprocessing.MinMaxScaler()\n",
    "\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train1_a = X_scaler1_a.fit_transform(rnn_train1_a)\n",
    "rnn_scaled_train1_b = X_scaler1_b.fit_transform(rnn_train1_b)\n",
    "rnn_scaled_train1_c = X_scaler1_c.fit_transform(rnn_train1_c)\n",
    "\n",
    "rnn_scaled_train2_a = X_scaler2_a.fit_transform(rnn_train2_a)\n",
    "rnn_scaled_train2_b = X_scaler2_b.fit_transform(rnn_train2_b)\n",
    "rnn_scaled_train2_c = X_scaler2_c.fit_transform(rnn_train2_c)\n",
    "\n",
    "rnn_scaled_train3_a = X_scaler3_a.fit_transform(rnn_train3_a)\n",
    "rnn_scaled_train3_b = X_scaler3_b.fit_transform(rnn_train3_b)\n",
    "rnn_scaled_train3_c = X_scaler3_c.fit_transform(rnn_train3_c)\n",
    "\n",
    "rnn_scaled_train4_a = X_scaler4_a.fit_transform(rnn_train4_a)\n",
    "rnn_scaled_train4_b = X_scaler4_b.fit_transform(rnn_train4_b)\n",
    "rnn_scaled_train4_c = X_scaler4_c.fit_transform(rnn_train4_c)\n",
    "\n",
    "rnn_scaled_train5_a = X_scaler5_a.fit_transform(rnn_train5_a)\n",
    "rnn_scaled_train5_b = X_scaler5_b.fit_transform(rnn_train5_b)\n",
    "rnn_scaled_train5_c = X_scaler5_c.fit_transform(rnn_train5_c)\n",
    "\n",
    "rnn_scaled_train6 = X_scaler6.fit_transform(rnn_train6)\n",
    "rnn_scaled_train7 = X_scaler7.fit_transform(rnn_train7)\n",
    "rnn_scaled_train8 = X_scaler8.fit_transform(rnn_train8)\n",
    "rnn_scaled_train9 = X_scaler9.fit_transform(rnn_train9)\n",
    "rnn_scaled_train10 = X_scaler10.fit_transform(rnn_train10)\n",
    "\n",
    "Y_train_Scaled   = Y_scaler.fit_transform(Y_train)\n",
    "Y_test_scaled = Y_scaler.transform(Y_test)\n",
    "\n",
    "X_train_Scaled = np.hstack(\n",
    "    (rnn_scaled_train1_a, rnn_scaled_train1_b, rnn_scaled_train1_c, rnn_scaled_train2_a, rnn_scaled_train2_b, rnn_scaled_train2_c,\n",
    "     rnn_scaled_train3_a, rnn_scaled_train3_b, rnn_scaled_train3_c, rnn_scaled_train4_a, rnn_scaled_train4_b, rnn_scaled_train4_c,\n",
    "     rnn_scaled_train5_a, rnn_scaled_train5_b, rnn_scaled_train5_c,rnn_scaled_train6,rnn_scaled_train7, rnn_scaled_train8,\n",
    "     rnn_scaled_train9, rnn_scaled_train10)\n",
    ").reshape(rnn_train6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled = np.hstack(\n",
    "    (X_scaler1_a.transform(rnn_test1_a),X_scaler1_b.transform(rnn_test1_b),X_scaler1_c.transform(rnn_test1_c),\n",
    "     X_scaler2_a.transform(rnn_test2_a),X_scaler2_b.transform(rnn_test2_b),X_scaler2_c.transform(rnn_test2_c),\n",
    "     X_scaler3_a.transform(rnn_test3_a),X_scaler3_b.transform(rnn_test3_b),X_scaler3_c.transform(rnn_test3_c),\n",
    "     X_scaler4_a.transform(rnn_test4_a),X_scaler4_b.transform(rnn_test4_b),X_scaler4_c.transform(rnn_test4_c),\n",
    "     X_scaler5_a.transform(rnn_test5_a),X_scaler5_b.transform(rnn_test5_b),X_scaler5_c.transform(rnn_test5_c),\n",
    "     X_scaler6.transform(rnn_test6),X_scaler7.transform(rnn_test7),X_scaler8.transform(rnn_test8),\n",
    "     X_scaler9.transform(rnn_test9), X_scaler10.transform(rnn_test10))\n",
    ").reshape(rnn_test6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "\n",
    "\n",
    "score_acc = make_scorer(mean_absolute_error)\n",
    "mse = make_scorer(MSE, greater_is_better=False)\n",
    "i_shape=(X_train_Scaled.shape[1], X_train_Scaled.shape[2])\n",
    "      \n",
    "pp ={\n",
    "    'neurons_0': list(range(16, 256, 16)),\n",
    "    'activation_0':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "    'neurons_1': list(range(16, 256, 16)),\n",
    "    'activation_1':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "    'neurons_2': list(range(16, 256, 16)),\n",
    "    'activation_2':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "    'neurons_3': list(range(16, 256, 16)),\n",
    "    'activation_3':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "    'neurons_4': list(range(16, 256, 16)),\n",
    "    'activation_4':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "\n",
    "    'learning_rate': list(np.linspace(0.0001,0.02, 10)),        \n",
    "    'layers1': list(range(1,3, 1)),\n",
    "    'layers2': list(range(1,3, 1)),\n",
    "    'layers3': list(range(1,3, 1)),\n",
    "    'layers4': list(range(1,3, 1)),\n",
    "    'dropout_rate': list(np.linspace(0.0,0.2, 10)),\n",
    "    'batch_size': list(range(16, 64, 16)),\n",
    "    'epochs': [200]\n",
    "}\n",
    "\n",
    "def create_model(x_train, y_train, x_val, y_val, params):\n",
    "            nn = Sequential()\n",
    "            nn.add(Flatten(input_shape=i_shape))\n",
    "\n",
    "            for i in range(params['layers1']):\n",
    "                nn.add(Dense(params['neurons_0'], input_shape=i_shape, activation=params['activation_0']))\n",
    "                \n",
    "            for i in range(params['layers2']):\n",
    "                nn.add(Dense(params['neurons_1'], activation=params['activation_1']))\n",
    "   \n",
    "            for i in range(params['layers3']):\n",
    "                nn.add(Dense(params['neurons_2'], activation=params['activation_2']))\n",
    "            \n",
    "            nn.add(Dropout(params['dropout_rate'], seed=123))\n",
    "            \n",
    "            for i in range(params['layers4']):\n",
    "                nn.add(Dense(params['neurons_3'], activation=params['activation_3']))\n",
    "                \n",
    "            nn.add(Dense(params['neurons_4'], activation=params['activation_4']))\n",
    "            nn.add(Dense(16))\n",
    "            opt = Adam(lr = params['learning_rate'])\n",
    "            nn.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mean_absolute_error'])\n",
    "            out=nn.fit(x_train, y_train,validation_data=[x_val, y_val], batch_size=params['batch_size'], epochs=params['epochs'], verbose=2, \n",
    "                          callbacks=[talos.utils.early_stopper(epochs=params['epochs'], mode='moderate', \n",
    "                                                               min_delta=0.001, monitor='mean_absolute_error')])\n",
    "            return out, nn\n",
    "        \n",
    "h = talos.Scan(x=X_train_Scaled,y=Y_train_Scaled,x_val=X_test_Scaled,  y_val=Y_test_scaled, params=pp, model=create_model,val_split=0.2,\n",
    "            experiment_name ='bm_1-3', random_method='quantum', round_limit=30, print_params=True)\n",
    "h.data.sort_values(by='val_mean_absolute_error', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb46279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single Headed DNN Models Search 90 days Training Data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import concat\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from math import floor\n",
    "from bayes_opt import BayesianOptimization\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "#tensorflow &keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "from keras.activations import *\n",
    "\n",
    "from keras.layers import LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "\n",
    "#sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import talos \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "date_format=\"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat=pd.DataFrame(dat)\n",
    "dat=dat.bfill(axis ='rows')\n",
    "dat=dat.ffill(axis ='rows')\n",
    "dat=dat._get_numeric_data()\n",
    "\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[4322:7731,:]\n",
    "Y_train=Y.iloc[4322:7731,:]\n",
    "X_test=X.iloc[7731:8739,:]\n",
    "Y_test=Y.iloc[7731:8739,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[8738:12146,:]\n",
    "# Y_train=Y.iloc[8738:12146,:]\n",
    "# X_test=X.iloc[12146:13155,:]\n",
    "# Y_test=Y.iloc[12146:13155,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[13154:16514,:]\n",
    "# Y_train=Y.iloc[13154:16514,:]\n",
    "# X_test=X.iloc[16514:17523,:]\n",
    "# Y_test=Y.iloc[16514:17523,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[17522:20834,:]\n",
    "# Y_train=Y.iloc[17522:20834,:]\n",
    "# X_test=X.iloc[20834:21843,:]\n",
    "# Y_test=Y.iloc[20834:21843,:]\n",
    "\n",
    "rnn_train1_a=X_train[[\"lag_-3x1\", \"lag_-4x1\",\"lag_-5x1\",\"lag_-6x1\",\"lag_-7x1\", \"lag_-8x1\",\"lag_-9x1\",\"lag_-10x1\",\"lag_-11x1\", \"lag_-12x1\",\"lag_-13x1\",\"lag_-14x1\",\"lag_-15x1\", \"lag_-16x1\",\"lag_-17x1\",\"lag_-18x1\"]]\n",
    "rnn_train1_b=X_train[[\"lag_-19x1\", \"lag_-20x1\",\"lag_-21x1\",\"lag_-22x1\",\"lag_-23x1\", \"lag_-24x1\",\"lag_-25x1\",\"lag_-26x1\",\"lag_-27x1\", \"lag_-28x1\",\"lag_-29x1\",\"lag_-30x1\",\"lag_-31x1\", \"lag_-32x1\",\"lag_-33x1\",\"lag_-34x1\"]]\n",
    "rnn_train1_c=X_train[[\"lag_-35x1\", \"lag_-36x1\",\"lag_-37x1\",\"lag_-38x1\",\"lag_-39x1\", \"lag_-40x1\",\"lag_-41x1\",\"lag_-42x1\",\"lag_-43x1\", \"lag_-44x1\",\"lag_-45x1\",\"lag_-46x1\",\"lag_-47x1\", \"lag_-48x1\",\"lag_-49x1\",\"lag_-50x1\"]]\n",
    "\n",
    "rnn_train2_a=X_train[[\"lag_-3x2\", \"lag_-4x2\",\"lag_-5x2\",\"lag_-6x2\",\"lag_-7x2\", \"lag_-8x2\",\"lag_-9x2\",\"lag_-10x2\",\"lag_-11x2\", \"lag_-12x2\",\"lag_-13x2\",\"lag_-14x2\",\"lag_-15x2\", \"lag_-16x2\",\"lag_-17x2\",\"lag_-18x2\"]]\n",
    "rnn_train2_b=X_train[[\"lag_-19x2\", \"lag_-20x2\",\"lag_-21x2\",\"lag_-22x2\",\"lag_-23x2\", \"lag_-24x2\",\"lag_-25x2\",\"lag_-26x2\",\"lag_-27x2\", \"lag_-28x2\",\"lag_-29x2\",\"lag_-30x2\",\"lag_-31x2\", \"lag_-32x2\",\"lag_-33x2\",\"lag_-34x2\"]]\n",
    "rnn_train2_c=X_train[[\"lag_-35x2\", \"lag_-36x2\",\"lag_-37x2\",\"lag_-38x2\",\"lag_-39x2\", \"lag_-40x2\",\"lag_-41x2\",\"lag_-42x2\",\"lag_-43x2\", \"lag_-44x2\",\"lag_-45x2\",\"lag_-46x2\",\"lag_-47x2\", \"lag_-48x2\",\"lag_-49x2\",\"lag_-50x2\"]]\n",
    "\n",
    "rnn_train3_a=X_train[[\"lag_-2x3\",\"lag_-3x3\", \"lag_-4x3\",\"lag_-5x3\",\"lag_-6x3\",\"lag_-7x3\", \"lag_-8x3\",\"lag_-9x3\",\"lag_-10x3\",\"lag_-11x3\", \"lag_-12x3\",\"lag_-13x3\",\"lag_-14x3\",\"lag_-15x3\", \"lag_-16x3\",\"lag_-17x3\"]]\n",
    "rnn_train3_b=X_train[[\"lag_-18x3\",\"lag_-19x3\", \"lag_-20x3\",\"lag_-21x3\",\"lag_-22x3\",\"lag_-23x3\", \"lag_-24x3\",\"lag_-25x3\",\"lag_-26x3\",\"lag_-27x3\", \"lag_-28x3\",\"lag_-29x3\",\"lag_-30x3\",\"lag_-31x3\", \"lag_-32x3\",\"lag_-33x3\"]]\n",
    "rnn_train3_c=X_train[[\"lag_-34x3\",\"lag_-35x3\", \"lag_-36x3\",\"lag_-37x3\",\"lag_-38x3\",\"lag_-39x3\", \"lag_-40x3\",\"lag_-41x3\",\"lag_-42x3\",\"lag_-43x3\", \"lag_-44x3\",\"lag_-45x3\",\"lag_-46x3\",\"lag_-47x3\", \"lag_-48x3\",\"lag_-49x3\"]]\n",
    "\n",
    "rnn_train4_a=X_train[[\"lag_0x6\",\"lag_-1x6\",\"lag_-2x6\",\"lag_-3x6\", \"lag_-4x6\",\"lag_-5x6\",\"lag_-6x6\",\"lag_-7x6\", \"lag_-8x6\",\"lag_-9x6\",\"lag_-10x6\",\"lag_-11x6\", \"lag_-12x6\",\"lag_-13x6\",\"lag_-14x6\",\"lag_-15x6\"]]\n",
    "rnn_train4_b=X_train[[\"lag_-16x6\",\"lag_-17x6\",\"lag_-18x6\",\"lag_-19x6\", \"lag_-20x6\",\"lag_-21x6\",\"lag_-22x6\",\"lag_-23x6\", \"lag_-24x6\",\"lag_-25x6\",\"lag_-26x6\",\"lag_-27x6\", \"lag_-28x6\",\"lag_-29x6\",\"lag_-30x6\",\"lag_-31x6\"]]\n",
    "rnn_train4_c=X_train[[\"lag_-32x6\",\"lag_-33x6\",\"lag_-34x6\",\"lag_-35x6\", \"lag_-36x6\",\"lag_-37x6\",\"lag_-38x6\",\"lag_-39x6\", \"lag_-40x6\",\"lag_-41x6\",\"lag_-42x6\",\"lag_-43x6\", \"lag_-44x6\",\"lag_-45x6\",\"lag_-46x6\",\"lag_-47x6\"]]\n",
    "\n",
    "rnn_train5_a=X_train[[\"lag_-2x12\",\"lag_-3x12\", \"lag_-4x12\",\"lag_-5x12\",\"lag_-6x12\",\"lag_-7x12\", \"lag_-8x12\",\"lag_-9x12\",\"lag_-10x12\",\"lag_-11x12\", \"lag_-12x12\",\"lag_-13x12\",\"lag_-14x12\",\"lag_-15x12\", \"lag_-16x12\",\"lag_-17x12\"]]\n",
    "rnn_train5_b=X_train[[\"lag_-18x12\",\"lag_-19x12\", \"lag_-20x12\",\"lag_-21x12\",\"lag_-22x12\",\"lag_-23x12\", \"lag_-24x12\",\"lag_-25x12\",\"lag_-26x12\",\"lag_-27x12\", \"lag_-28x12\",\"lag_-29x12\",\"lag_-30x12\",\"lag_-31x12\", \"lag_-32x12\",\"lag_-33x12\"]]\n",
    "rnn_train5_c=X_train[[\"lag_-34x12\",\"lag_-35x12\", \"lag_-36x12\",\"lag_-37x12\",\"lag_-38x12\",\"lag_-39x12\", \"lag_-40x12\",\"lag_-41x12\",\"lag_-42x12\",\"lag_-43x12\", \"lag_-44x12\",\"lag_-45x12\",\"lag_-46x12\",\"lag_-47x12\", \"lag_-48x12\",\"lag_-49x12\"]]\n",
    "\n",
    "rnn_train6=X_train[[\"lag_2x7\",\"lag_3x7\",\"lag_4x7\",\"lag_5x7\",\"lag_6x7\",\"lag_7x7\",\"lag_8x7\",\"lag_9x7\",\"lag_10x7\",\"lag_11x7\",\"lag_12x7\",\"lag_13x7\",\"lag_14x7\",\"lag_15x7\",\"lag_16x7\",\"lag_17x7\"]]\n",
    "rnn_train7=X_train[[\"lag_2x8\",\"lag_3x8\",\"lag_4x8\",\"lag_5x8\",\"lag_6x8\",\"lag_7x8\",\"lag_8x8\",\"lag_9x8\",\"lag_10x8\",\"lag_11x8\",\"lag_12x8\",\"lag_13x8\",\"lag_14x8\",\"lag_15x8\",\"lag_16x8\",\"lag_17x8\"]]\n",
    "rnn_train8=X_train[[\"lag_2x9\",\"lag_3x9\",\"lag_4x9\",\"lag_5x9\",\"lag_6x9\",\"lag_7x9\",\"lag_8x9\",\"lag_9x9\",\"lag_10x9\",\"lag_11x9\",\"lag_12x9\",\"lag_13x9\",\"lag_14x9\",\"lag_15x9\",\"lag_16x9\",\"lag_17x9\"]]\n",
    "rnn_train9=X_train[[\"lag_2x10\",\"lag_3x10\",\"lag_4x10\",\"lag_5x10\",\"lag_6x10\",\"lag_7x10\",\"lag_8x10\",\"lag_9x10\",\"lag_10x10\",\"lag_11x10\",\"lag_12x10\",\"lag_13x10\",\"lag_14x10\",\"lag_15x10\",\"lag_16x10\",\"lag_17x10\"]]\n",
    "rnn_train10=X_train[[\"lag_2x11\",\"lag_3x11\",\"lag_4x11\",\"lag_5x11\",\"lag_6x11\",\"lag_7x11\",\"lag_8x11\",\"lag_9x11\",\"lag_10x11\",\"lag_11x11\",\"lag_12x11\",\"lag_13x11\",\"lag_14x11\",\"lag_15x11\",\"lag_16x11\",\"lag_17x11\"]]\n",
    "\n",
    "\n",
    "\n",
    "rnn_test1_a=X_test[[\"lag_-3x1\", \"lag_-4x1\",\"lag_-5x1\",\"lag_-6x1\",\"lag_-7x1\", \"lag_-8x1\",\"lag_-9x1\",\"lag_-10x1\",\"lag_-11x1\", \"lag_-12x1\",\"lag_-13x1\",\"lag_-14x1\",\"lag_-15x1\", \"lag_-16x1\",\"lag_-17x1\",\"lag_-18x1\"]]\n",
    "rnn_test1_b=X_test[[\"lag_-19x1\", \"lag_-20x1\",\"lag_-21x1\",\"lag_-22x1\",\"lag_-23x1\", \"lag_-24x1\",\"lag_-25x1\",\"lag_-26x1\",\"lag_-27x1\", \"lag_-28x1\",\"lag_-29x1\",\"lag_-30x1\",\"lag_-31x1\", \"lag_-32x1\",\"lag_-33x1\",\"lag_-34x1\"]]\n",
    "rnn_test1_c=X_test[[\"lag_-35x1\", \"lag_-36x1\",\"lag_-37x1\",\"lag_-38x1\",\"lag_-39x1\", \"lag_-40x1\",\"lag_-41x1\",\"lag_-42x1\",\"lag_-43x1\", \"lag_-44x1\",\"lag_-45x1\",\"lag_-46x1\",\"lag_-47x1\", \"lag_-48x1\",\"lag_-49x1\",\"lag_-50x1\"]]\n",
    "\n",
    "rnn_test2_a=X_test[[\"lag_-3x2\", \"lag_-4x2\",\"lag_-5x2\",\"lag_-6x2\",\"lag_-7x2\", \"lag_-8x2\",\"lag_-9x2\",\"lag_-10x2\",\"lag_-11x2\", \"lag_-12x2\",\"lag_-13x2\",\"lag_-14x2\",\"lag_-15x2\", \"lag_-16x2\",\"lag_-17x2\",\"lag_-18x2\"]]\n",
    "rnn_test2_b=X_test[[\"lag_-19x2\", \"lag_-20x2\",\"lag_-21x2\",\"lag_-22x2\",\"lag_-23x2\", \"lag_-24x2\",\"lag_-25x2\",\"lag_-26x2\",\"lag_-27x2\", \"lag_-28x2\",\"lag_-29x2\",\"lag_-30x2\",\"lag_-31x2\", \"lag_-32x2\",\"lag_-33x2\",\"lag_-34x2\"]]\n",
    "rnn_test2_c=X_test[[\"lag_-35x2\", \"lag_-36x2\",\"lag_-37x2\",\"lag_-38x2\",\"lag_-39x2\", \"lag_-40x2\",\"lag_-41x2\",\"lag_-42x2\",\"lag_-43x2\", \"lag_-44x2\",\"lag_-45x2\",\"lag_-46x2\",\"lag_-47x2\", \"lag_-48x2\",\"lag_-49x2\",\"lag_-50x2\"]]\n",
    "\n",
    "rnn_test3_a=X_test[[\"lag_-2x3\",\"lag_-3x3\", \"lag_-4x3\",\"lag_-5x3\",\"lag_-6x3\",\"lag_-7x3\", \"lag_-8x3\",\"lag_-9x3\",\"lag_-10x3\",\"lag_-11x3\", \"lag_-12x3\",\"lag_-13x3\",\"lag_-14x3\",\"lag_-15x3\", \"lag_-16x3\",\"lag_-17x3\"]]\n",
    "rnn_test3_b=X_test[[\"lag_-18x3\",\"lag_-19x3\", \"lag_-20x3\",\"lag_-21x3\",\"lag_-22x3\",\"lag_-23x3\", \"lag_-24x3\",\"lag_-25x3\",\"lag_-26x3\",\"lag_-27x3\", \"lag_-28x3\",\"lag_-29x3\",\"lag_-30x3\",\"lag_-31x3\", \"lag_-32x3\",\"lag_-33x3\"]]\n",
    "rnn_test3_c=X_test[[\"lag_-34x3\",\"lag_-35x3\", \"lag_-36x3\",\"lag_-37x3\",\"lag_-38x3\",\"lag_-39x3\", \"lag_-40x3\",\"lag_-41x3\",\"lag_-42x3\",\"lag_-43x3\", \"lag_-44x3\",\"lag_-45x3\",\"lag_-46x3\",\"lag_-47x3\", \"lag_-48x3\",\"lag_-49x3\"]]\n",
    "\n",
    "rnn_test4_a=X_test[[\"lag_0x6\",\"lag_-1x6\",\"lag_-2x6\",\"lag_-3x6\", \"lag_-4x6\",\"lag_-5x6\",\"lag_-6x6\",\"lag_-7x6\", \"lag_-8x6\",\"lag_-9x6\",\"lag_-10x6\",\"lag_-11x6\", \"lag_-12x6\",\"lag_-13x6\",\"lag_-14x6\",\"lag_-15x6\"]]\n",
    "rnn_test4_b=X_test[[\"lag_-16x6\",\"lag_-17x6\",\"lag_-18x6\",\"lag_-19x6\", \"lag_-20x6\",\"lag_-21x6\",\"lag_-22x6\",\"lag_-23x6\", \"lag_-24x6\",\"lag_-25x6\",\"lag_-26x6\",\"lag_-27x6\", \"lag_-28x6\",\"lag_-29x6\",\"lag_-30x6\",\"lag_-31x6\"]]\n",
    "rnn_test4_c=X_test[[\"lag_-32x6\",\"lag_-33x6\",\"lag_-34x6\",\"lag_-35x6\", \"lag_-36x6\",\"lag_-37x6\",\"lag_-38x6\",\"lag_-39x6\", \"lag_-40x6\",\"lag_-41x6\",\"lag_-42x6\",\"lag_-43x6\", \"lag_-44x6\",\"lag_-45x6\",\"lag_-46x6\",\"lag_-47x6\"]]\n",
    "\n",
    "rnn_test5_a=X_test[[\"lag_-2x12\",\"lag_-3x12\", \"lag_-4x12\",\"lag_-5x12\",\"lag_-6x12\",\"lag_-7x12\", \"lag_-8x12\",\"lag_-9x12\",\"lag_-10x12\",\"lag_-11x12\", \"lag_-12x12\",\"lag_-13x12\",\"lag_-14x12\",\"lag_-15x12\", \"lag_-16x12\",\"lag_-17x12\"]]\n",
    "rnn_test5_b=X_test[[\"lag_-18x12\",\"lag_-19x12\", \"lag_-20x12\",\"lag_-21x12\",\"lag_-22x12\",\"lag_-23x12\", \"lag_-24x12\",\"lag_-25x12\",\"lag_-26x12\",\"lag_-27x12\", \"lag_-28x12\",\"lag_-29x12\",\"lag_-30x12\",\"lag_-31x12\", \"lag_-32x12\",\"lag_-33x12\"]]\n",
    "rnn_test5_c=X_test[[\"lag_-34x12\",\"lag_-35x12\", \"lag_-36x12\",\"lag_-37x12\",\"lag_-38x12\",\"lag_-39x12\", \"lag_-40x12\",\"lag_-41x12\",\"lag_-42x12\",\"lag_-43x12\", \"lag_-44x12\",\"lag_-45x12\",\"lag_-46x12\",\"lag_-47x12\", \"lag_-48x12\",\"lag_-49x12\"]]\n",
    "\n",
    "rnn_test6=X_test[[\"lag_2x7\",\"lag_3x7\",\"lag_4x7\",\"lag_5x7\",\"lag_6x7\",\"lag_7x7\",\"lag_8x7\",\"lag_9x7\",\"lag_10x7\",\"lag_11x7\",\"lag_12x7\",\"lag_13x7\",\"lag_14x7\",\"lag_15x7\",\"lag_16x7\",\"lag_17x7\"]]\n",
    "rnn_test7=X_test[[\"lag_2x8\",\"lag_3x8\",\"lag_4x8\",\"lag_5x8\",\"lag_6x8\",\"lag_7x8\",\"lag_8x8\",\"lag_9x8\",\"lag_10x8\",\"lag_11x8\",\"lag_12x8\",\"lag_13x8\",\"lag_14x8\",\"lag_15x8\",\"lag_16x8\",\"lag_17x8\"]]\n",
    "rnn_test8=X_test[[\"lag_2x9\",\"lag_3x9\",\"lag_4x9\",\"lag_5x9\",\"lag_6x9\",\"lag_7x9\",\"lag_8x9\",\"lag_9x9\",\"lag_10x9\",\"lag_11x9\",\"lag_12x9\",\"lag_13x9\",\"lag_14x9\",\"lag_15x9\",\"lag_16x9\",\"lag_17x9\"]]\n",
    "rnn_test9=X_test[[\"lag_2x10\",\"lag_3x10\",\"lag_4x10\",\"lag_5x10\",\"lag_6x10\",\"lag_7x10\",\"lag_8x10\",\"lag_9x10\",\"lag_10x10\",\"lag_11x10\",\"lag_12x10\",\"lag_13x10\",\"lag_14x10\",\"lag_15x10\",\"lag_16x10\",\"lag_17x10\"]]\n",
    "rnn_test10=X_test[[\"lag_2x11\",\"lag_3x11\",\"lag_4x11\",\"lag_5x11\",\"lag_6x11\",\"lag_7x11\",\"lag_8x11\",\"lag_9x11\",\"lag_10x11\",\"lag_11x11\",\"lag_12x11\",\"lag_13x11\",\"lag_14x11\",\"lag_15x11\",\"lag_16x11\",\"lag_17x11\"]]\n",
    "\n",
    "rnn_Y=Y_train[[\"lag_2y\", \"lag_3y\", \"lag_4y\", \"lag_5y\", \"lag_6y\", \"lag_7y\", \"lag_8y\", \"lag_9y\", \"lag_10y\", \"lag_11y\", \"lag_12y\", \"lag_13y\", \"lag_14y\", \"lag_15y\", \"lag_16y\", \"lag_17y\"]]\n",
    "\n",
    "X_scaler1_a = preprocessing.MinMaxScaler()\n",
    "X_scaler1_b = preprocessing.MinMaxScaler()\n",
    "X_scaler1_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler2_a = preprocessing.MinMaxScaler()\n",
    "X_scaler2_b = preprocessing.MinMaxScaler()\n",
    "X_scaler2_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler3_a = preprocessing.MinMaxScaler()\n",
    "X_scaler3_b = preprocessing.MinMaxScaler()\n",
    "X_scaler3_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler4_a = preprocessing.MinMaxScaler()\n",
    "X_scaler4_b = preprocessing.MinMaxScaler()\n",
    "X_scaler4_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler5_a = preprocessing.MinMaxScaler()\n",
    "X_scaler5_b = preprocessing.MinMaxScaler()\n",
    "X_scaler5_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "\n",
    "X_scaler6 = preprocessing.MinMaxScaler()\n",
    "X_scaler7 = preprocessing.MinMaxScaler()\n",
    "X_scaler8 = preprocessing.MinMaxScaler()\n",
    "X_scaler9 = preprocessing.MinMaxScaler()\n",
    "X_scaler10 = preprocessing.MinMaxScaler()\n",
    "\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train1_a = X_scaler1_a.fit_transform(rnn_train1_a)\n",
    "rnn_scaled_train1_b = X_scaler1_b.fit_transform(rnn_train1_b)\n",
    "rnn_scaled_train1_c = X_scaler1_c.fit_transform(rnn_train1_c)\n",
    "\n",
    "rnn_scaled_train2_a = X_scaler2_a.fit_transform(rnn_train2_a)\n",
    "rnn_scaled_train2_b = X_scaler2_b.fit_transform(rnn_train2_b)\n",
    "rnn_scaled_train2_c = X_scaler2_c.fit_transform(rnn_train2_c)\n",
    "\n",
    "rnn_scaled_train3_a = X_scaler3_a.fit_transform(rnn_train3_a)\n",
    "rnn_scaled_train3_b = X_scaler3_b.fit_transform(rnn_train3_b)\n",
    "rnn_scaled_train3_c = X_scaler3_c.fit_transform(rnn_train3_c)\n",
    "\n",
    "rnn_scaled_train4_a = X_scaler4_a.fit_transform(rnn_train4_a)\n",
    "rnn_scaled_train4_b = X_scaler4_b.fit_transform(rnn_train4_b)\n",
    "rnn_scaled_train4_c = X_scaler4_c.fit_transform(rnn_train4_c)\n",
    "\n",
    "rnn_scaled_train5_a = X_scaler5_a.fit_transform(rnn_train5_a)\n",
    "rnn_scaled_train5_b = X_scaler5_b.fit_transform(rnn_train5_b)\n",
    "rnn_scaled_train5_c = X_scaler5_c.fit_transform(rnn_train5_c)\n",
    "\n",
    "rnn_scaled_train6 = X_scaler6.fit_transform(rnn_train6)\n",
    "rnn_scaled_train7 = X_scaler7.fit_transform(rnn_train7)\n",
    "rnn_scaled_train8 = X_scaler8.fit_transform(rnn_train8)\n",
    "rnn_scaled_train9 = X_scaler9.fit_transform(rnn_train9)\n",
    "rnn_scaled_train10 = X_scaler10.fit_transform(rnn_train10)\n",
    "\n",
    "Y_train_Scaled   = Y_scaler.fit_transform(Y_train)\n",
    "Y_test_scaled = Y_scaler.transform(Y_test)\n",
    "\n",
    "X_train_Scaled = np.hstack(\n",
    "    (rnn_scaled_train1_a, rnn_scaled_train1_b, rnn_scaled_train1_c, rnn_scaled_train2_a, rnn_scaled_train2_b, rnn_scaled_train2_c,\n",
    "     rnn_scaled_train3_a, rnn_scaled_train3_b, rnn_scaled_train3_c, rnn_scaled_train4_a, rnn_scaled_train4_b, rnn_scaled_train4_c,\n",
    "     rnn_scaled_train5_a, rnn_scaled_train5_b, rnn_scaled_train5_c,rnn_scaled_train6,rnn_scaled_train7, rnn_scaled_train8,\n",
    "     rnn_scaled_train9, rnn_scaled_train10)\n",
    ").reshape(rnn_train6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled = np.hstack(\n",
    "    (X_scaler1_a.transform(rnn_test1_a),X_scaler1_b.transform(rnn_test1_b),X_scaler1_c.transform(rnn_test1_c),\n",
    "     X_scaler2_a.transform(rnn_test2_a),X_scaler2_b.transform(rnn_test2_b),X_scaler2_c.transform(rnn_test2_c),\n",
    "     X_scaler3_a.transform(rnn_test3_a),X_scaler3_b.transform(rnn_test3_b),X_scaler3_c.transform(rnn_test3_c),\n",
    "     X_scaler4_a.transform(rnn_test4_a),X_scaler4_b.transform(rnn_test4_b),X_scaler4_c.transform(rnn_test4_c),\n",
    "     X_scaler5_a.transform(rnn_test5_a),X_scaler5_b.transform(rnn_test5_b),X_scaler5_c.transform(rnn_test5_c),\n",
    "     X_scaler6.transform(rnn_test6),X_scaler7.transform(rnn_test7),X_scaler8.transform(rnn_test8),\n",
    "     X_scaler9.transform(rnn_test9), X_scaler10.transform(rnn_test10))\n",
    ").reshape(rnn_test6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "\n",
    "\n",
    "score_acc = make_scorer(mean_absolute_error)\n",
    "mse = make_scorer(MSE, greater_is_better=False)\n",
    "i_shape=(X_train_Scaled.shape[1], X_train_Scaled.shape[2])\n",
    "      \n",
    "pp ={\n",
    "    'neurons_0': list(range(16, 256, 16)),\n",
    "    'activation_0':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "    'neurons_1': list(range(16, 256, 16)),\n",
    "    'activation_1':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "    'neurons_2': list(range(16, 256, 16)),\n",
    "    'activation_2':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "    'neurons_3': list(range(16, 256, 16)),\n",
    "    'activation_3':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "    'neurons_4': list(range(16, 256, 16)),\n",
    "    'activation_4':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "\n",
    "    'learning_rate': list(np.linspace(0.0001,0.02, 10)),        \n",
    "    'layers1': list(range(1,3, 1)),\n",
    "    'layers2': list(range(1,3, 1)),\n",
    "    'layers3': list(range(1,3, 1)),\n",
    "    'layers4': list(range(1,3, 1)),\n",
    "    'dropout_rate': list(np.linspace(0.0,0.2, 10)),\n",
    "    'batch_size': list(range(16, 64, 16)),\n",
    "    'epochs': [200]\n",
    "}\n",
    "\n",
    "def create_model(x_train, y_train, x_val, y_val, params):\n",
    "            nn = Sequential()\n",
    "            nn.add(Flatten(input_shape=i_shape))\n",
    "\n",
    "            for i in range(params['layers1']):\n",
    "                nn.add(Dense(params['neurons_0'], input_shape=i_shape, activation=params['activation_0']))\n",
    "                \n",
    "            for i in range(params['layers2']):\n",
    "                nn.add(Dense(params['neurons_1'], activation=params['activation_1']))\n",
    "   \n",
    "            for i in range(params['layers3']):\n",
    "                nn.add(Dense(params['neurons_2'], activation=params['activation_2']))\n",
    "            \n",
    "            nn.add(Dropout(params['dropout_rate'], seed=123))\n",
    "            \n",
    "            for i in range(params['layers4']):\n",
    "                nn.add(Dense(params['neurons_3'], activation=params['activation_3']))\n",
    "                \n",
    "            nn.add(Dense(params['neurons_4'], activation=params['activation_4']))\n",
    "            nn.add(Dense(16))\n",
    "            opt = Adam(lr = params['learning_rate'])\n",
    "            nn.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mean_absolute_error'])\n",
    "            out=nn.fit(x_train, y_train,validation_data=[x_val, y_val], batch_size=params['batch_size'], epochs=params['epochs'], verbose=2, \n",
    "                          callbacks=[talos.utils.early_stopper(epochs=params['epochs'], mode='moderate', \n",
    "                                                               min_delta=0.001, monitor='mean_absolute_error')])\n",
    "            return out, nn\n",
    "        \n",
    "h = talos.Scan(x=X_train_Scaled,y=Y_train_Scaled,x_val=X_test_Scaled,  y_val=Y_test_scaled, params=pp, model=create_model,val_split=0.2,\n",
    "            experiment_name ='bm_1-3', random_method='quantum', round_limit=30, print_params=True)\n",
    "h.data.sort_values(by='val_mean_absolute_error', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b6b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single Headed DNN Models Search 60 days Training Data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import concat\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from math import floor\n",
    "from bayes_opt import BayesianOptimization\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "#tensorflow &keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "from keras.activations import *\n",
    "\n",
    "from keras.layers import LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "\n",
    "#sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import talos \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "date_format=\"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat=pd.DataFrame(dat)\n",
    "dat=dat.bfill(axis ='rows')\n",
    "dat=dat.ffill(axis ='rows')\n",
    "dat=dat._get_numeric_data()\n",
    "\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[5810:8066,:]\n",
    "Y_train=Y.iloc[5810:8066,:]\n",
    "X_test=X.iloc[8066:8739,:]\n",
    "Y_test=Y.iloc[8066:8739,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[10178:12482,:]\n",
    "# Y_train=Y.iloc[10178:12482,:]\n",
    "# X_test=X.iloc[12482:13155,:]\n",
    "# Y_test=Y.iloc[12482:13155,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[14594:16850,:]\n",
    "# Y_train=Y.iloc[14594:16850,:]\n",
    "# X_test=X.iloc[16850:17523,:]\n",
    "# Y_test=Y.iloc[16850:17523,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[19010:21170,:]\n",
    "# Y_train=Y.iloc[19010:21170,:]\n",
    "# X_test=X.iloc[21170:21843,:]\n",
    "# Y_test=Y.iloc[21170:21843,:]\n",
    "\n",
    "rnn_train1_a=X_train[[\"lag_-3x1\", \"lag_-4x1\",\"lag_-5x1\",\"lag_-6x1\",\"lag_-7x1\", \"lag_-8x1\",\"lag_-9x1\",\"lag_-10x1\",\"lag_-11x1\", \"lag_-12x1\",\"lag_-13x1\",\"lag_-14x1\",\"lag_-15x1\", \"lag_-16x1\",\"lag_-17x1\",\"lag_-18x1\"]]\n",
    "rnn_train1_b=X_train[[\"lag_-19x1\", \"lag_-20x1\",\"lag_-21x1\",\"lag_-22x1\",\"lag_-23x1\", \"lag_-24x1\",\"lag_-25x1\",\"lag_-26x1\",\"lag_-27x1\", \"lag_-28x1\",\"lag_-29x1\",\"lag_-30x1\",\"lag_-31x1\", \"lag_-32x1\",\"lag_-33x1\",\"lag_-34x1\"]]\n",
    "rnn_train1_c=X_train[[\"lag_-35x1\", \"lag_-36x1\",\"lag_-37x1\",\"lag_-38x1\",\"lag_-39x1\", \"lag_-40x1\",\"lag_-41x1\",\"lag_-42x1\",\"lag_-43x1\", \"lag_-44x1\",\"lag_-45x1\",\"lag_-46x1\",\"lag_-47x1\", \"lag_-48x1\",\"lag_-49x1\",\"lag_-50x1\"]]\n",
    "\n",
    "rnn_train2_a=X_train[[\"lag_-3x2\", \"lag_-4x2\",\"lag_-5x2\",\"lag_-6x2\",\"lag_-7x2\", \"lag_-8x2\",\"lag_-9x2\",\"lag_-10x2\",\"lag_-11x2\", \"lag_-12x2\",\"lag_-13x2\",\"lag_-14x2\",\"lag_-15x2\", \"lag_-16x2\",\"lag_-17x2\",\"lag_-18x2\"]]\n",
    "rnn_train2_b=X_train[[\"lag_-19x2\", \"lag_-20x2\",\"lag_-21x2\",\"lag_-22x2\",\"lag_-23x2\", \"lag_-24x2\",\"lag_-25x2\",\"lag_-26x2\",\"lag_-27x2\", \"lag_-28x2\",\"lag_-29x2\",\"lag_-30x2\",\"lag_-31x2\", \"lag_-32x2\",\"lag_-33x2\",\"lag_-34x2\"]]\n",
    "rnn_train2_c=X_train[[\"lag_-35x2\", \"lag_-36x2\",\"lag_-37x2\",\"lag_-38x2\",\"lag_-39x2\", \"lag_-40x2\",\"lag_-41x2\",\"lag_-42x2\",\"lag_-43x2\", \"lag_-44x2\",\"lag_-45x2\",\"lag_-46x2\",\"lag_-47x2\", \"lag_-48x2\",\"lag_-49x2\",\"lag_-50x2\"]]\n",
    "\n",
    "rnn_train3_a=X_train[[\"lag_-2x3\",\"lag_-3x3\", \"lag_-4x3\",\"lag_-5x3\",\"lag_-6x3\",\"lag_-7x3\", \"lag_-8x3\",\"lag_-9x3\",\"lag_-10x3\",\"lag_-11x3\", \"lag_-12x3\",\"lag_-13x3\",\"lag_-14x3\",\"lag_-15x3\", \"lag_-16x3\",\"lag_-17x3\"]]\n",
    "rnn_train3_b=X_train[[\"lag_-18x3\",\"lag_-19x3\", \"lag_-20x3\",\"lag_-21x3\",\"lag_-22x3\",\"lag_-23x3\", \"lag_-24x3\",\"lag_-25x3\",\"lag_-26x3\",\"lag_-27x3\", \"lag_-28x3\",\"lag_-29x3\",\"lag_-30x3\",\"lag_-31x3\", \"lag_-32x3\",\"lag_-33x3\"]]\n",
    "rnn_train3_c=X_train[[\"lag_-34x3\",\"lag_-35x3\", \"lag_-36x3\",\"lag_-37x3\",\"lag_-38x3\",\"lag_-39x3\", \"lag_-40x3\",\"lag_-41x3\",\"lag_-42x3\",\"lag_-43x3\", \"lag_-44x3\",\"lag_-45x3\",\"lag_-46x3\",\"lag_-47x3\", \"lag_-48x3\",\"lag_-49x3\"]]\n",
    "\n",
    "rnn_train4_a=X_train[[\"lag_0x6\",\"lag_-1x6\",\"lag_-2x6\",\"lag_-3x6\", \"lag_-4x6\",\"lag_-5x6\",\"lag_-6x6\",\"lag_-7x6\", \"lag_-8x6\",\"lag_-9x6\",\"lag_-10x6\",\"lag_-11x6\", \"lag_-12x6\",\"lag_-13x6\",\"lag_-14x6\",\"lag_-15x6\"]]\n",
    "rnn_train4_b=X_train[[\"lag_-16x6\",\"lag_-17x6\",\"lag_-18x6\",\"lag_-19x6\", \"lag_-20x6\",\"lag_-21x6\",\"lag_-22x6\",\"lag_-23x6\", \"lag_-24x6\",\"lag_-25x6\",\"lag_-26x6\",\"lag_-27x6\", \"lag_-28x6\",\"lag_-29x6\",\"lag_-30x6\",\"lag_-31x6\"]]\n",
    "rnn_train4_c=X_train[[\"lag_-32x6\",\"lag_-33x6\",\"lag_-34x6\",\"lag_-35x6\", \"lag_-36x6\",\"lag_-37x6\",\"lag_-38x6\",\"lag_-39x6\", \"lag_-40x6\",\"lag_-41x6\",\"lag_-42x6\",\"lag_-43x6\", \"lag_-44x6\",\"lag_-45x6\",\"lag_-46x6\",\"lag_-47x6\"]]\n",
    "\n",
    "rnn_train5_a=X_train[[\"lag_-2x12\",\"lag_-3x12\", \"lag_-4x12\",\"lag_-5x12\",\"lag_-6x12\",\"lag_-7x12\", \"lag_-8x12\",\"lag_-9x12\",\"lag_-10x12\",\"lag_-11x12\", \"lag_-12x12\",\"lag_-13x12\",\"lag_-14x12\",\"lag_-15x12\", \"lag_-16x12\",\"lag_-17x12\"]]\n",
    "rnn_train5_b=X_train[[\"lag_-18x12\",\"lag_-19x12\", \"lag_-20x12\",\"lag_-21x12\",\"lag_-22x12\",\"lag_-23x12\", \"lag_-24x12\",\"lag_-25x12\",\"lag_-26x12\",\"lag_-27x12\", \"lag_-28x12\",\"lag_-29x12\",\"lag_-30x12\",\"lag_-31x12\", \"lag_-32x12\",\"lag_-33x12\"]]\n",
    "rnn_train5_c=X_train[[\"lag_-34x12\",\"lag_-35x12\", \"lag_-36x12\",\"lag_-37x12\",\"lag_-38x12\",\"lag_-39x12\", \"lag_-40x12\",\"lag_-41x12\",\"lag_-42x12\",\"lag_-43x12\", \"lag_-44x12\",\"lag_-45x12\",\"lag_-46x12\",\"lag_-47x12\", \"lag_-48x12\",\"lag_-49x12\"]]\n",
    "\n",
    "rnn_train6=X_train[[\"lag_2x7\",\"lag_3x7\",\"lag_4x7\",\"lag_5x7\",\"lag_6x7\",\"lag_7x7\",\"lag_8x7\",\"lag_9x7\",\"lag_10x7\",\"lag_11x7\",\"lag_12x7\",\"lag_13x7\",\"lag_14x7\",\"lag_15x7\",\"lag_16x7\",\"lag_17x7\"]]\n",
    "rnn_train7=X_train[[\"lag_2x8\",\"lag_3x8\",\"lag_4x8\",\"lag_5x8\",\"lag_6x8\",\"lag_7x8\",\"lag_8x8\",\"lag_9x8\",\"lag_10x8\",\"lag_11x8\",\"lag_12x8\",\"lag_13x8\",\"lag_14x8\",\"lag_15x8\",\"lag_16x8\",\"lag_17x8\"]]\n",
    "rnn_train8=X_train[[\"lag_2x9\",\"lag_3x9\",\"lag_4x9\",\"lag_5x9\",\"lag_6x9\",\"lag_7x9\",\"lag_8x9\",\"lag_9x9\",\"lag_10x9\",\"lag_11x9\",\"lag_12x9\",\"lag_13x9\",\"lag_14x9\",\"lag_15x9\",\"lag_16x9\",\"lag_17x9\"]]\n",
    "rnn_train9=X_train[[\"lag_2x10\",\"lag_3x10\",\"lag_4x10\",\"lag_5x10\",\"lag_6x10\",\"lag_7x10\",\"lag_8x10\",\"lag_9x10\",\"lag_10x10\",\"lag_11x10\",\"lag_12x10\",\"lag_13x10\",\"lag_14x10\",\"lag_15x10\",\"lag_16x10\",\"lag_17x10\"]]\n",
    "rnn_train10=X_train[[\"lag_2x11\",\"lag_3x11\",\"lag_4x11\",\"lag_5x11\",\"lag_6x11\",\"lag_7x11\",\"lag_8x11\",\"lag_9x11\",\"lag_10x11\",\"lag_11x11\",\"lag_12x11\",\"lag_13x11\",\"lag_14x11\",\"lag_15x11\",\"lag_16x11\",\"lag_17x11\"]]\n",
    "\n",
    "\n",
    "\n",
    "rnn_test1_a=X_test[[\"lag_-3x1\", \"lag_-4x1\",\"lag_-5x1\",\"lag_-6x1\",\"lag_-7x1\", \"lag_-8x1\",\"lag_-9x1\",\"lag_-10x1\",\"lag_-11x1\", \"lag_-12x1\",\"lag_-13x1\",\"lag_-14x1\",\"lag_-15x1\", \"lag_-16x1\",\"lag_-17x1\",\"lag_-18x1\"]]\n",
    "rnn_test1_b=X_test[[\"lag_-19x1\", \"lag_-20x1\",\"lag_-21x1\",\"lag_-22x1\",\"lag_-23x1\", \"lag_-24x1\",\"lag_-25x1\",\"lag_-26x1\",\"lag_-27x1\", \"lag_-28x1\",\"lag_-29x1\",\"lag_-30x1\",\"lag_-31x1\", \"lag_-32x1\",\"lag_-33x1\",\"lag_-34x1\"]]\n",
    "rnn_test1_c=X_test[[\"lag_-35x1\", \"lag_-36x1\",\"lag_-37x1\",\"lag_-38x1\",\"lag_-39x1\", \"lag_-40x1\",\"lag_-41x1\",\"lag_-42x1\",\"lag_-43x1\", \"lag_-44x1\",\"lag_-45x1\",\"lag_-46x1\",\"lag_-47x1\", \"lag_-48x1\",\"lag_-49x1\",\"lag_-50x1\"]]\n",
    "\n",
    "rnn_test2_a=X_test[[\"lag_-3x2\", \"lag_-4x2\",\"lag_-5x2\",\"lag_-6x2\",\"lag_-7x2\", \"lag_-8x2\",\"lag_-9x2\",\"lag_-10x2\",\"lag_-11x2\", \"lag_-12x2\",\"lag_-13x2\",\"lag_-14x2\",\"lag_-15x2\", \"lag_-16x2\",\"lag_-17x2\",\"lag_-18x2\"]]\n",
    "rnn_test2_b=X_test[[\"lag_-19x2\", \"lag_-20x2\",\"lag_-21x2\",\"lag_-22x2\",\"lag_-23x2\", \"lag_-24x2\",\"lag_-25x2\",\"lag_-26x2\",\"lag_-27x2\", \"lag_-28x2\",\"lag_-29x2\",\"lag_-30x2\",\"lag_-31x2\", \"lag_-32x2\",\"lag_-33x2\",\"lag_-34x2\"]]\n",
    "rnn_test2_c=X_test[[\"lag_-35x2\", \"lag_-36x2\",\"lag_-37x2\",\"lag_-38x2\",\"lag_-39x2\", \"lag_-40x2\",\"lag_-41x2\",\"lag_-42x2\",\"lag_-43x2\", \"lag_-44x2\",\"lag_-45x2\",\"lag_-46x2\",\"lag_-47x2\", \"lag_-48x2\",\"lag_-49x2\",\"lag_-50x2\"]]\n",
    "\n",
    "rnn_test3_a=X_test[[\"lag_-2x3\",\"lag_-3x3\", \"lag_-4x3\",\"lag_-5x3\",\"lag_-6x3\",\"lag_-7x3\", \"lag_-8x3\",\"lag_-9x3\",\"lag_-10x3\",\"lag_-11x3\", \"lag_-12x3\",\"lag_-13x3\",\"lag_-14x3\",\"lag_-15x3\", \"lag_-16x3\",\"lag_-17x3\"]]\n",
    "rnn_test3_b=X_test[[\"lag_-18x3\",\"lag_-19x3\", \"lag_-20x3\",\"lag_-21x3\",\"lag_-22x3\",\"lag_-23x3\", \"lag_-24x3\",\"lag_-25x3\",\"lag_-26x3\",\"lag_-27x3\", \"lag_-28x3\",\"lag_-29x3\",\"lag_-30x3\",\"lag_-31x3\", \"lag_-32x3\",\"lag_-33x3\"]]\n",
    "rnn_test3_c=X_test[[\"lag_-34x3\",\"lag_-35x3\", \"lag_-36x3\",\"lag_-37x3\",\"lag_-38x3\",\"lag_-39x3\", \"lag_-40x3\",\"lag_-41x3\",\"lag_-42x3\",\"lag_-43x3\", \"lag_-44x3\",\"lag_-45x3\",\"lag_-46x3\",\"lag_-47x3\", \"lag_-48x3\",\"lag_-49x3\"]]\n",
    "\n",
    "rnn_test4_a=X_test[[\"lag_0x6\",\"lag_-1x6\",\"lag_-2x6\",\"lag_-3x6\", \"lag_-4x6\",\"lag_-5x6\",\"lag_-6x6\",\"lag_-7x6\", \"lag_-8x6\",\"lag_-9x6\",\"lag_-10x6\",\"lag_-11x6\", \"lag_-12x6\",\"lag_-13x6\",\"lag_-14x6\",\"lag_-15x6\"]]\n",
    "rnn_test4_b=X_test[[\"lag_-16x6\",\"lag_-17x6\",\"lag_-18x6\",\"lag_-19x6\", \"lag_-20x6\",\"lag_-21x6\",\"lag_-22x6\",\"lag_-23x6\", \"lag_-24x6\",\"lag_-25x6\",\"lag_-26x6\",\"lag_-27x6\", \"lag_-28x6\",\"lag_-29x6\",\"lag_-30x6\",\"lag_-31x6\"]]\n",
    "rnn_test4_c=X_test[[\"lag_-32x6\",\"lag_-33x6\",\"lag_-34x6\",\"lag_-35x6\", \"lag_-36x6\",\"lag_-37x6\",\"lag_-38x6\",\"lag_-39x6\", \"lag_-40x6\",\"lag_-41x6\",\"lag_-42x6\",\"lag_-43x6\", \"lag_-44x6\",\"lag_-45x6\",\"lag_-46x6\",\"lag_-47x6\"]]\n",
    "\n",
    "rnn_test5_a=X_test[[\"lag_-2x12\",\"lag_-3x12\", \"lag_-4x12\",\"lag_-5x12\",\"lag_-6x12\",\"lag_-7x12\", \"lag_-8x12\",\"lag_-9x12\",\"lag_-10x12\",\"lag_-11x12\", \"lag_-12x12\",\"lag_-13x12\",\"lag_-14x12\",\"lag_-15x12\", \"lag_-16x12\",\"lag_-17x12\"]]\n",
    "rnn_test5_b=X_test[[\"lag_-18x12\",\"lag_-19x12\", \"lag_-20x12\",\"lag_-21x12\",\"lag_-22x12\",\"lag_-23x12\", \"lag_-24x12\",\"lag_-25x12\",\"lag_-26x12\",\"lag_-27x12\", \"lag_-28x12\",\"lag_-29x12\",\"lag_-30x12\",\"lag_-31x12\", \"lag_-32x12\",\"lag_-33x12\"]]\n",
    "rnn_test5_c=X_test[[\"lag_-34x12\",\"lag_-35x12\", \"lag_-36x12\",\"lag_-37x12\",\"lag_-38x12\",\"lag_-39x12\", \"lag_-40x12\",\"lag_-41x12\",\"lag_-42x12\",\"lag_-43x12\", \"lag_-44x12\",\"lag_-45x12\",\"lag_-46x12\",\"lag_-47x12\", \"lag_-48x12\",\"lag_-49x12\"]]\n",
    "\n",
    "rnn_test6=X_test[[\"lag_2x7\",\"lag_3x7\",\"lag_4x7\",\"lag_5x7\",\"lag_6x7\",\"lag_7x7\",\"lag_8x7\",\"lag_9x7\",\"lag_10x7\",\"lag_11x7\",\"lag_12x7\",\"lag_13x7\",\"lag_14x7\",\"lag_15x7\",\"lag_16x7\",\"lag_17x7\"]]\n",
    "rnn_test7=X_test[[\"lag_2x8\",\"lag_3x8\",\"lag_4x8\",\"lag_5x8\",\"lag_6x8\",\"lag_7x8\",\"lag_8x8\",\"lag_9x8\",\"lag_10x8\",\"lag_11x8\",\"lag_12x8\",\"lag_13x8\",\"lag_14x8\",\"lag_15x8\",\"lag_16x8\",\"lag_17x8\"]]\n",
    "rnn_test8=X_test[[\"lag_2x9\",\"lag_3x9\",\"lag_4x9\",\"lag_5x9\",\"lag_6x9\",\"lag_7x9\",\"lag_8x9\",\"lag_9x9\",\"lag_10x9\",\"lag_11x9\",\"lag_12x9\",\"lag_13x9\",\"lag_14x9\",\"lag_15x9\",\"lag_16x9\",\"lag_17x9\"]]\n",
    "rnn_test9=X_test[[\"lag_2x10\",\"lag_3x10\",\"lag_4x10\",\"lag_5x10\",\"lag_6x10\",\"lag_7x10\",\"lag_8x10\",\"lag_9x10\",\"lag_10x10\",\"lag_11x10\",\"lag_12x10\",\"lag_13x10\",\"lag_14x10\",\"lag_15x10\",\"lag_16x10\",\"lag_17x10\"]]\n",
    "rnn_test10=X_test[[\"lag_2x11\",\"lag_3x11\",\"lag_4x11\",\"lag_5x11\",\"lag_6x11\",\"lag_7x11\",\"lag_8x11\",\"lag_9x11\",\"lag_10x11\",\"lag_11x11\",\"lag_12x11\",\"lag_13x11\",\"lag_14x11\",\"lag_15x11\",\"lag_16x11\",\"lag_17x11\"]]\n",
    "\n",
    "rnn_Y=Y_train[[\"lag_2y\", \"lag_3y\", \"lag_4y\", \"lag_5y\", \"lag_6y\", \"lag_7y\", \"lag_8y\", \"lag_9y\", \"lag_10y\", \"lag_11y\", \"lag_12y\", \"lag_13y\", \"lag_14y\", \"lag_15y\", \"lag_16y\", \"lag_17y\"]]\n",
    "\n",
    "X_scaler1_a = preprocessing.MinMaxScaler()\n",
    "X_scaler1_b = preprocessing.MinMaxScaler()\n",
    "X_scaler1_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler2_a = preprocessing.MinMaxScaler()\n",
    "X_scaler2_b = preprocessing.MinMaxScaler()\n",
    "X_scaler2_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler3_a = preprocessing.MinMaxScaler()\n",
    "X_scaler3_b = preprocessing.MinMaxScaler()\n",
    "X_scaler3_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler4_a = preprocessing.MinMaxScaler()\n",
    "X_scaler4_b = preprocessing.MinMaxScaler()\n",
    "X_scaler4_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler5_a = preprocessing.MinMaxScaler()\n",
    "X_scaler5_b = preprocessing.MinMaxScaler()\n",
    "X_scaler5_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "\n",
    "X_scaler6 = preprocessing.MinMaxScaler()\n",
    "X_scaler7 = preprocessing.MinMaxScaler()\n",
    "X_scaler8 = preprocessing.MinMaxScaler()\n",
    "X_scaler9 = preprocessing.MinMaxScaler()\n",
    "X_scaler10 = preprocessing.MinMaxScaler()\n",
    "\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train1_a = X_scaler1_a.fit_transform(rnn_train1_a)\n",
    "rnn_scaled_train1_b = X_scaler1_b.fit_transform(rnn_train1_b)\n",
    "rnn_scaled_train1_c = X_scaler1_c.fit_transform(rnn_train1_c)\n",
    "\n",
    "rnn_scaled_train2_a = X_scaler2_a.fit_transform(rnn_train2_a)\n",
    "rnn_scaled_train2_b = X_scaler2_b.fit_transform(rnn_train2_b)\n",
    "rnn_scaled_train2_c = X_scaler2_c.fit_transform(rnn_train2_c)\n",
    "\n",
    "rnn_scaled_train3_a = X_scaler3_a.fit_transform(rnn_train3_a)\n",
    "rnn_scaled_train3_b = X_scaler3_b.fit_transform(rnn_train3_b)\n",
    "rnn_scaled_train3_c = X_scaler3_c.fit_transform(rnn_train3_c)\n",
    "\n",
    "rnn_scaled_train4_a = X_scaler4_a.fit_transform(rnn_train4_a)\n",
    "rnn_scaled_train4_b = X_scaler4_b.fit_transform(rnn_train4_b)\n",
    "rnn_scaled_train4_c = X_scaler4_c.fit_transform(rnn_train4_c)\n",
    "\n",
    "rnn_scaled_train5_a = X_scaler5_a.fit_transform(rnn_train5_a)\n",
    "rnn_scaled_train5_b = X_scaler5_b.fit_transform(rnn_train5_b)\n",
    "rnn_scaled_train5_c = X_scaler5_c.fit_transform(rnn_train5_c)\n",
    "\n",
    "rnn_scaled_train6 = X_scaler6.fit_transform(rnn_train6)\n",
    "rnn_scaled_train7 = X_scaler7.fit_transform(rnn_train7)\n",
    "rnn_scaled_train8 = X_scaler8.fit_transform(rnn_train8)\n",
    "rnn_scaled_train9 = X_scaler9.fit_transform(rnn_train9)\n",
    "rnn_scaled_train10 = X_scaler10.fit_transform(rnn_train10)\n",
    "\n",
    "Y_train_Scaled   = Y_scaler.fit_transform(Y_train)\n",
    "Y_test_scaled = Y_scaler.transform(Y_test)\n",
    "\n",
    "X_train_Scaled = np.hstack(\n",
    "    (rnn_scaled_train1_a, rnn_scaled_train1_b, rnn_scaled_train1_c, rnn_scaled_train2_a, rnn_scaled_train2_b, rnn_scaled_train2_c,\n",
    "     rnn_scaled_train3_a, rnn_scaled_train3_b, rnn_scaled_train3_c, rnn_scaled_train4_a, rnn_scaled_train4_b, rnn_scaled_train4_c,\n",
    "     rnn_scaled_train5_a, rnn_scaled_train5_b, rnn_scaled_train5_c,rnn_scaled_train6,rnn_scaled_train7, rnn_scaled_train8,\n",
    "     rnn_scaled_train9, rnn_scaled_train10)\n",
    ").reshape(rnn_train6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled = np.hstack(\n",
    "    (X_scaler1_a.transform(rnn_test1_a),X_scaler1_b.transform(rnn_test1_b),X_scaler1_c.transform(rnn_test1_c),\n",
    "     X_scaler2_a.transform(rnn_test2_a),X_scaler2_b.transform(rnn_test2_b),X_scaler2_c.transform(rnn_test2_c),\n",
    "     X_scaler3_a.transform(rnn_test3_a),X_scaler3_b.transform(rnn_test3_b),X_scaler3_c.transform(rnn_test3_c),\n",
    "     X_scaler4_a.transform(rnn_test4_a),X_scaler4_b.transform(rnn_test4_b),X_scaler4_c.transform(rnn_test4_c),\n",
    "     X_scaler5_a.transform(rnn_test5_a),X_scaler5_b.transform(rnn_test5_b),X_scaler5_c.transform(rnn_test5_c),\n",
    "     X_scaler6.transform(rnn_test6),X_scaler7.transform(rnn_test7),X_scaler8.transform(rnn_test8),\n",
    "     X_scaler9.transform(rnn_test9), X_scaler10.transform(rnn_test10))\n",
    ").reshape(rnn_test6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "\n",
    "\n",
    "score_acc = make_scorer(mean_absolute_error)\n",
    "mse = make_scorer(MSE, greater_is_better=False)\n",
    "i_shape=(X_train_Scaled.shape[1], X_train_Scaled.shape[2])\n",
    "      \n",
    "pp ={\n",
    "    'neurons_0': list(range(16, 256, 16)),\n",
    "    'activation_0':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "    'neurons_1': list(range(16, 256, 16)),\n",
    "    'activation_1':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "    'neurons_2': list(range(16, 256, 16)),\n",
    "    'activation_2':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "    'neurons_3': list(range(16, 256, 16)),\n",
    "    'activation_3':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "    'neurons_4': list(range(16, 256, 16)),\n",
    "    'activation_4':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "\n",
    "    'learning_rate': list(np.linspace(0.0001,0.02, 10)),        \n",
    "    'layers1': list(range(1,3, 1)),\n",
    "    'layers2': list(range(1,3, 1)),\n",
    "    'layers3': list(range(1,3, 1)),\n",
    "    'layers4': list(range(1,3, 1)),\n",
    "    'dropout_rate': list(np.linspace(0.0,0.2, 10)),\n",
    "    'batch_size': list(range(16, 64, 16)),\n",
    "    'epochs': [200]\n",
    "}\n",
    "\n",
    "def create_model(x_train, y_train, x_val, y_val, params):\n",
    "            nn = Sequential()\n",
    "            nn.add(Flatten(input_shape=i_shape))\n",
    "\n",
    "            for i in range(params['layers1']):\n",
    "                nn.add(Dense(params['neurons_0'], input_shape=i_shape, activation=params['activation_0']))\n",
    "                \n",
    "            for i in range(params['layers2']):\n",
    "                nn.add(Dense(params['neurons_1'], activation=params['activation_1']))\n",
    "   \n",
    "            for i in range(params['layers3']):\n",
    "                nn.add(Dense(params['neurons_2'], activation=params['activation_2']))\n",
    "            \n",
    "            nn.add(Dropout(params['dropout_rate'], seed=123))\n",
    "            \n",
    "            for i in range(params['layers4']):\n",
    "                nn.add(Dense(params['neurons_3'], activation=params['activation_3']))\n",
    "                \n",
    "            nn.add(Dense(params['neurons_4'], activation=params['activation_4']))\n",
    "            nn.add(Dense(16))\n",
    "            opt = Adam(lr = params['learning_rate'])\n",
    "            nn.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mean_absolute_error'])\n",
    "            out=nn.fit(x_train, y_train,validation_data=[x_val, y_val], batch_size=params['batch_size'], epochs=params['epochs'], verbose=2, \n",
    "                          callbacks=[talos.utils.early_stopper(epochs=params['epochs'], mode='moderate', \n",
    "                                                               min_delta=0.001, monitor='mean_absolute_error')])\n",
    "            return out, nn\n",
    "        \n",
    "h = talos.Scan(x=X_train_Scaled,y=Y_train_Scaled,x_val=X_test_Scaled,  y_val=Y_test_scaled, params=pp, model=create_model,val_split=0.2,\n",
    "            experiment_name ='bm_1-3', random_method='quantum', round_limit=30, print_params=True)\n",
    "h.data.sort_values(by='val_mean_absolute_error', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1161ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single Headed DNN Models Search 30 days Training Data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import concat\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from math import floor\n",
    "from bayes_opt import BayesianOptimization\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "#tensorflow &keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "from keras.activations import *\n",
    "\n",
    "from keras.layers import LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "\n",
    "#sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import talos \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "date_format=\"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat=pd.DataFrame(dat)\n",
    "dat=dat.bfill(axis ='rows')\n",
    "dat=dat.ffill(axis ='rows')\n",
    "dat=dat._get_numeric_data()\n",
    "\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[7250:8402,:]\n",
    "Y_train=Y.iloc[7250:8402,:]\n",
    "X_test=X.iloc[8402:8739,:]\n",
    "Y_test=Y.iloc[8402:8739,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[11666:12818,:]\n",
    "# Y_train=Y.iloc[11666:12818,:]\n",
    "# X_test=X.iloc[12818:13155,:]\n",
    "# Y_test=Y.iloc[12818:13155,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[16082:17186,:]\n",
    "# Y_train=Y.iloc[16082:17186,:]\n",
    "# X_test=X.iloc[17186:17523,:]\n",
    "# Y_test=Y.iloc[17186:17523,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[20498:21506,:]\n",
    "# Y_train=Y.iloc[20498:21506,:]\n",
    "# X_test=X.iloc[21506:21843,:]\n",
    "# Y_test=Y.iloc[21506:21843,:]\n",
    "\n",
    "rnn_train1_a=X_train[[\"lag_-3x1\", \"lag_-4x1\",\"lag_-5x1\",\"lag_-6x1\",\"lag_-7x1\", \"lag_-8x1\",\"lag_-9x1\",\"lag_-10x1\",\"lag_-11x1\", \"lag_-12x1\",\"lag_-13x1\",\"lag_-14x1\",\"lag_-15x1\", \"lag_-16x1\",\"lag_-17x1\",\"lag_-18x1\"]]\n",
    "rnn_train1_b=X_train[[\"lag_-19x1\", \"lag_-20x1\",\"lag_-21x1\",\"lag_-22x1\",\"lag_-23x1\", \"lag_-24x1\",\"lag_-25x1\",\"lag_-26x1\",\"lag_-27x1\", \"lag_-28x1\",\"lag_-29x1\",\"lag_-30x1\",\"lag_-31x1\", \"lag_-32x1\",\"lag_-33x1\",\"lag_-34x1\"]]\n",
    "rnn_train1_c=X_train[[\"lag_-35x1\", \"lag_-36x1\",\"lag_-37x1\",\"lag_-38x1\",\"lag_-39x1\", \"lag_-40x1\",\"lag_-41x1\",\"lag_-42x1\",\"lag_-43x1\", \"lag_-44x1\",\"lag_-45x1\",\"lag_-46x1\",\"lag_-47x1\", \"lag_-48x1\",\"lag_-49x1\",\"lag_-50x1\"]]\n",
    "\n",
    "rnn_train2_a=X_train[[\"lag_-3x2\", \"lag_-4x2\",\"lag_-5x2\",\"lag_-6x2\",\"lag_-7x2\", \"lag_-8x2\",\"lag_-9x2\",\"lag_-10x2\",\"lag_-11x2\", \"lag_-12x2\",\"lag_-13x2\",\"lag_-14x2\",\"lag_-15x2\", \"lag_-16x2\",\"lag_-17x2\",\"lag_-18x2\"]]\n",
    "rnn_train2_b=X_train[[\"lag_-19x2\", \"lag_-20x2\",\"lag_-21x2\",\"lag_-22x2\",\"lag_-23x2\", \"lag_-24x2\",\"lag_-25x2\",\"lag_-26x2\",\"lag_-27x2\", \"lag_-28x2\",\"lag_-29x2\",\"lag_-30x2\",\"lag_-31x2\", \"lag_-32x2\",\"lag_-33x2\",\"lag_-34x2\"]]\n",
    "rnn_train2_c=X_train[[\"lag_-35x2\", \"lag_-36x2\",\"lag_-37x2\",\"lag_-38x2\",\"lag_-39x2\", \"lag_-40x2\",\"lag_-41x2\",\"lag_-42x2\",\"lag_-43x2\", \"lag_-44x2\",\"lag_-45x2\",\"lag_-46x2\",\"lag_-47x2\", \"lag_-48x2\",\"lag_-49x2\",\"lag_-50x2\"]]\n",
    "\n",
    "rnn_train3_a=X_train[[\"lag_-2x3\",\"lag_-3x3\", \"lag_-4x3\",\"lag_-5x3\",\"lag_-6x3\",\"lag_-7x3\", \"lag_-8x3\",\"lag_-9x3\",\"lag_-10x3\",\"lag_-11x3\", \"lag_-12x3\",\"lag_-13x3\",\"lag_-14x3\",\"lag_-15x3\", \"lag_-16x3\",\"lag_-17x3\"]]\n",
    "rnn_train3_b=X_train[[\"lag_-18x3\",\"lag_-19x3\", \"lag_-20x3\",\"lag_-21x3\",\"lag_-22x3\",\"lag_-23x3\", \"lag_-24x3\",\"lag_-25x3\",\"lag_-26x3\",\"lag_-27x3\", \"lag_-28x3\",\"lag_-29x3\",\"lag_-30x3\",\"lag_-31x3\", \"lag_-32x3\",\"lag_-33x3\"]]\n",
    "rnn_train3_c=X_train[[\"lag_-34x3\",\"lag_-35x3\", \"lag_-36x3\",\"lag_-37x3\",\"lag_-38x3\",\"lag_-39x3\", \"lag_-40x3\",\"lag_-41x3\",\"lag_-42x3\",\"lag_-43x3\", \"lag_-44x3\",\"lag_-45x3\",\"lag_-46x3\",\"lag_-47x3\", \"lag_-48x3\",\"lag_-49x3\"]]\n",
    "\n",
    "rnn_train4_a=X_train[[\"lag_0x6\",\"lag_-1x6\",\"lag_-2x6\",\"lag_-3x6\", \"lag_-4x6\",\"lag_-5x6\",\"lag_-6x6\",\"lag_-7x6\", \"lag_-8x6\",\"lag_-9x6\",\"lag_-10x6\",\"lag_-11x6\", \"lag_-12x6\",\"lag_-13x6\",\"lag_-14x6\",\"lag_-15x6\"]]\n",
    "rnn_train4_b=X_train[[\"lag_-16x6\",\"lag_-17x6\",\"lag_-18x6\",\"lag_-19x6\", \"lag_-20x6\",\"lag_-21x6\",\"lag_-22x6\",\"lag_-23x6\", \"lag_-24x6\",\"lag_-25x6\",\"lag_-26x6\",\"lag_-27x6\", \"lag_-28x6\",\"lag_-29x6\",\"lag_-30x6\",\"lag_-31x6\"]]\n",
    "rnn_train4_c=X_train[[\"lag_-32x6\",\"lag_-33x6\",\"lag_-34x6\",\"lag_-35x6\", \"lag_-36x6\",\"lag_-37x6\",\"lag_-38x6\",\"lag_-39x6\", \"lag_-40x6\",\"lag_-41x6\",\"lag_-42x6\",\"lag_-43x6\", \"lag_-44x6\",\"lag_-45x6\",\"lag_-46x6\",\"lag_-47x6\"]]\n",
    "\n",
    "rnn_train5_a=X_train[[\"lag_-2x12\",\"lag_-3x12\", \"lag_-4x12\",\"lag_-5x12\",\"lag_-6x12\",\"lag_-7x12\", \"lag_-8x12\",\"lag_-9x12\",\"lag_-10x12\",\"lag_-11x12\", \"lag_-12x12\",\"lag_-13x12\",\"lag_-14x12\",\"lag_-15x12\", \"lag_-16x12\",\"lag_-17x12\"]]\n",
    "rnn_train5_b=X_train[[\"lag_-18x12\",\"lag_-19x12\", \"lag_-20x12\",\"lag_-21x12\",\"lag_-22x12\",\"lag_-23x12\", \"lag_-24x12\",\"lag_-25x12\",\"lag_-26x12\",\"lag_-27x12\", \"lag_-28x12\",\"lag_-29x12\",\"lag_-30x12\",\"lag_-31x12\", \"lag_-32x12\",\"lag_-33x12\"]]\n",
    "rnn_train5_c=X_train[[\"lag_-34x12\",\"lag_-35x12\", \"lag_-36x12\",\"lag_-37x12\",\"lag_-38x12\",\"lag_-39x12\", \"lag_-40x12\",\"lag_-41x12\",\"lag_-42x12\",\"lag_-43x12\", \"lag_-44x12\",\"lag_-45x12\",\"lag_-46x12\",\"lag_-47x12\", \"lag_-48x12\",\"lag_-49x12\"]]\n",
    "\n",
    "rnn_train6=X_train[[\"lag_2x7\",\"lag_3x7\",\"lag_4x7\",\"lag_5x7\",\"lag_6x7\",\"lag_7x7\",\"lag_8x7\",\"lag_9x7\",\"lag_10x7\",\"lag_11x7\",\"lag_12x7\",\"lag_13x7\",\"lag_14x7\",\"lag_15x7\",\"lag_16x7\",\"lag_17x7\"]]\n",
    "rnn_train7=X_train[[\"lag_2x8\",\"lag_3x8\",\"lag_4x8\",\"lag_5x8\",\"lag_6x8\",\"lag_7x8\",\"lag_8x8\",\"lag_9x8\",\"lag_10x8\",\"lag_11x8\",\"lag_12x8\",\"lag_13x8\",\"lag_14x8\",\"lag_15x8\",\"lag_16x8\",\"lag_17x8\"]]\n",
    "rnn_train8=X_train[[\"lag_2x9\",\"lag_3x9\",\"lag_4x9\",\"lag_5x9\",\"lag_6x9\",\"lag_7x9\",\"lag_8x9\",\"lag_9x9\",\"lag_10x9\",\"lag_11x9\",\"lag_12x9\",\"lag_13x9\",\"lag_14x9\",\"lag_15x9\",\"lag_16x9\",\"lag_17x9\"]]\n",
    "rnn_train9=X_train[[\"lag_2x10\",\"lag_3x10\",\"lag_4x10\",\"lag_5x10\",\"lag_6x10\",\"lag_7x10\",\"lag_8x10\",\"lag_9x10\",\"lag_10x10\",\"lag_11x10\",\"lag_12x10\",\"lag_13x10\",\"lag_14x10\",\"lag_15x10\",\"lag_16x10\",\"lag_17x10\"]]\n",
    "rnn_train10=X_train[[\"lag_2x11\",\"lag_3x11\",\"lag_4x11\",\"lag_5x11\",\"lag_6x11\",\"lag_7x11\",\"lag_8x11\",\"lag_9x11\",\"lag_10x11\",\"lag_11x11\",\"lag_12x11\",\"lag_13x11\",\"lag_14x11\",\"lag_15x11\",\"lag_16x11\",\"lag_17x11\"]]\n",
    "\n",
    "\n",
    "\n",
    "rnn_test1_a=X_test[[\"lag_-3x1\", \"lag_-4x1\",\"lag_-5x1\",\"lag_-6x1\",\"lag_-7x1\", \"lag_-8x1\",\"lag_-9x1\",\"lag_-10x1\",\"lag_-11x1\", \"lag_-12x1\",\"lag_-13x1\",\"lag_-14x1\",\"lag_-15x1\", \"lag_-16x1\",\"lag_-17x1\",\"lag_-18x1\"]]\n",
    "rnn_test1_b=X_test[[\"lag_-19x1\", \"lag_-20x1\",\"lag_-21x1\",\"lag_-22x1\",\"lag_-23x1\", \"lag_-24x1\",\"lag_-25x1\",\"lag_-26x1\",\"lag_-27x1\", \"lag_-28x1\",\"lag_-29x1\",\"lag_-30x1\",\"lag_-31x1\", \"lag_-32x1\",\"lag_-33x1\",\"lag_-34x1\"]]\n",
    "rnn_test1_c=X_test[[\"lag_-35x1\", \"lag_-36x1\",\"lag_-37x1\",\"lag_-38x1\",\"lag_-39x1\", \"lag_-40x1\",\"lag_-41x1\",\"lag_-42x1\",\"lag_-43x1\", \"lag_-44x1\",\"lag_-45x1\",\"lag_-46x1\",\"lag_-47x1\", \"lag_-48x1\",\"lag_-49x1\",\"lag_-50x1\"]]\n",
    "\n",
    "rnn_test2_a=X_test[[\"lag_-3x2\", \"lag_-4x2\",\"lag_-5x2\",\"lag_-6x2\",\"lag_-7x2\", \"lag_-8x2\",\"lag_-9x2\",\"lag_-10x2\",\"lag_-11x2\", \"lag_-12x2\",\"lag_-13x2\",\"lag_-14x2\",\"lag_-15x2\", \"lag_-16x2\",\"lag_-17x2\",\"lag_-18x2\"]]\n",
    "rnn_test2_b=X_test[[\"lag_-19x2\", \"lag_-20x2\",\"lag_-21x2\",\"lag_-22x2\",\"lag_-23x2\", \"lag_-24x2\",\"lag_-25x2\",\"lag_-26x2\",\"lag_-27x2\", \"lag_-28x2\",\"lag_-29x2\",\"lag_-30x2\",\"lag_-31x2\", \"lag_-32x2\",\"lag_-33x2\",\"lag_-34x2\"]]\n",
    "rnn_test2_c=X_test[[\"lag_-35x2\", \"lag_-36x2\",\"lag_-37x2\",\"lag_-38x2\",\"lag_-39x2\", \"lag_-40x2\",\"lag_-41x2\",\"lag_-42x2\",\"lag_-43x2\", \"lag_-44x2\",\"lag_-45x2\",\"lag_-46x2\",\"lag_-47x2\", \"lag_-48x2\",\"lag_-49x2\",\"lag_-50x2\"]]\n",
    "\n",
    "rnn_test3_a=X_test[[\"lag_-2x3\",\"lag_-3x3\", \"lag_-4x3\",\"lag_-5x3\",\"lag_-6x3\",\"lag_-7x3\", \"lag_-8x3\",\"lag_-9x3\",\"lag_-10x3\",\"lag_-11x3\", \"lag_-12x3\",\"lag_-13x3\",\"lag_-14x3\",\"lag_-15x3\", \"lag_-16x3\",\"lag_-17x3\"]]\n",
    "rnn_test3_b=X_test[[\"lag_-18x3\",\"lag_-19x3\", \"lag_-20x3\",\"lag_-21x3\",\"lag_-22x3\",\"lag_-23x3\", \"lag_-24x3\",\"lag_-25x3\",\"lag_-26x3\",\"lag_-27x3\", \"lag_-28x3\",\"lag_-29x3\",\"lag_-30x3\",\"lag_-31x3\", \"lag_-32x3\",\"lag_-33x3\"]]\n",
    "rnn_test3_c=X_test[[\"lag_-34x3\",\"lag_-35x3\", \"lag_-36x3\",\"lag_-37x3\",\"lag_-38x3\",\"lag_-39x3\", \"lag_-40x3\",\"lag_-41x3\",\"lag_-42x3\",\"lag_-43x3\", \"lag_-44x3\",\"lag_-45x3\",\"lag_-46x3\",\"lag_-47x3\", \"lag_-48x3\",\"lag_-49x3\"]]\n",
    "\n",
    "rnn_test4_a=X_test[[\"lag_0x6\",\"lag_-1x6\",\"lag_-2x6\",\"lag_-3x6\", \"lag_-4x6\",\"lag_-5x6\",\"lag_-6x6\",\"lag_-7x6\", \"lag_-8x6\",\"lag_-9x6\",\"lag_-10x6\",\"lag_-11x6\", \"lag_-12x6\",\"lag_-13x6\",\"lag_-14x6\",\"lag_-15x6\"]]\n",
    "rnn_test4_b=X_test[[\"lag_-16x6\",\"lag_-17x6\",\"lag_-18x6\",\"lag_-19x6\", \"lag_-20x6\",\"lag_-21x6\",\"lag_-22x6\",\"lag_-23x6\", \"lag_-24x6\",\"lag_-25x6\",\"lag_-26x6\",\"lag_-27x6\", \"lag_-28x6\",\"lag_-29x6\",\"lag_-30x6\",\"lag_-31x6\"]]\n",
    "rnn_test4_c=X_test[[\"lag_-32x6\",\"lag_-33x6\",\"lag_-34x6\",\"lag_-35x6\", \"lag_-36x6\",\"lag_-37x6\",\"lag_-38x6\",\"lag_-39x6\", \"lag_-40x6\",\"lag_-41x6\",\"lag_-42x6\",\"lag_-43x6\", \"lag_-44x6\",\"lag_-45x6\",\"lag_-46x6\",\"lag_-47x6\"]]\n",
    "\n",
    "rnn_test5_a=X_test[[\"lag_-2x12\",\"lag_-3x12\", \"lag_-4x12\",\"lag_-5x12\",\"lag_-6x12\",\"lag_-7x12\", \"lag_-8x12\",\"lag_-9x12\",\"lag_-10x12\",\"lag_-11x12\", \"lag_-12x12\",\"lag_-13x12\",\"lag_-14x12\",\"lag_-15x12\", \"lag_-16x12\",\"lag_-17x12\"]]\n",
    "rnn_test5_b=X_test[[\"lag_-18x12\",\"lag_-19x12\", \"lag_-20x12\",\"lag_-21x12\",\"lag_-22x12\",\"lag_-23x12\", \"lag_-24x12\",\"lag_-25x12\",\"lag_-26x12\",\"lag_-27x12\", \"lag_-28x12\",\"lag_-29x12\",\"lag_-30x12\",\"lag_-31x12\", \"lag_-32x12\",\"lag_-33x12\"]]\n",
    "rnn_test5_c=X_test[[\"lag_-34x12\",\"lag_-35x12\", \"lag_-36x12\",\"lag_-37x12\",\"lag_-38x12\",\"lag_-39x12\", \"lag_-40x12\",\"lag_-41x12\",\"lag_-42x12\",\"lag_-43x12\", \"lag_-44x12\",\"lag_-45x12\",\"lag_-46x12\",\"lag_-47x12\", \"lag_-48x12\",\"lag_-49x12\"]]\n",
    "\n",
    "rnn_test6=X_test[[\"lag_2x7\",\"lag_3x7\",\"lag_4x7\",\"lag_5x7\",\"lag_6x7\",\"lag_7x7\",\"lag_8x7\",\"lag_9x7\",\"lag_10x7\",\"lag_11x7\",\"lag_12x7\",\"lag_13x7\",\"lag_14x7\",\"lag_15x7\",\"lag_16x7\",\"lag_17x7\"]]\n",
    "rnn_test7=X_test[[\"lag_2x8\",\"lag_3x8\",\"lag_4x8\",\"lag_5x8\",\"lag_6x8\",\"lag_7x8\",\"lag_8x8\",\"lag_9x8\",\"lag_10x8\",\"lag_11x8\",\"lag_12x8\",\"lag_13x8\",\"lag_14x8\",\"lag_15x8\",\"lag_16x8\",\"lag_17x8\"]]\n",
    "rnn_test8=X_test[[\"lag_2x9\",\"lag_3x9\",\"lag_4x9\",\"lag_5x9\",\"lag_6x9\",\"lag_7x9\",\"lag_8x9\",\"lag_9x9\",\"lag_10x9\",\"lag_11x9\",\"lag_12x9\",\"lag_13x9\",\"lag_14x9\",\"lag_15x9\",\"lag_16x9\",\"lag_17x9\"]]\n",
    "rnn_test9=X_test[[\"lag_2x10\",\"lag_3x10\",\"lag_4x10\",\"lag_5x10\",\"lag_6x10\",\"lag_7x10\",\"lag_8x10\",\"lag_9x10\",\"lag_10x10\",\"lag_11x10\",\"lag_12x10\",\"lag_13x10\",\"lag_14x10\",\"lag_15x10\",\"lag_16x10\",\"lag_17x10\"]]\n",
    "rnn_test10=X_test[[\"lag_2x11\",\"lag_3x11\",\"lag_4x11\",\"lag_5x11\",\"lag_6x11\",\"lag_7x11\",\"lag_8x11\",\"lag_9x11\",\"lag_10x11\",\"lag_11x11\",\"lag_12x11\",\"lag_13x11\",\"lag_14x11\",\"lag_15x11\",\"lag_16x11\",\"lag_17x11\"]]\n",
    "\n",
    "rnn_Y=Y_train[[\"lag_2y\", \"lag_3y\", \"lag_4y\", \"lag_5y\", \"lag_6y\", \"lag_7y\", \"lag_8y\", \"lag_9y\", \"lag_10y\", \"lag_11y\", \"lag_12y\", \"lag_13y\", \"lag_14y\", \"lag_15y\", \"lag_16y\", \"lag_17y\"]]\n",
    "\n",
    "X_scaler1_a = preprocessing.MinMaxScaler()\n",
    "X_scaler1_b = preprocessing.MinMaxScaler()\n",
    "X_scaler1_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler2_a = preprocessing.MinMaxScaler()\n",
    "X_scaler2_b = preprocessing.MinMaxScaler()\n",
    "X_scaler2_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler3_a = preprocessing.MinMaxScaler()\n",
    "X_scaler3_b = preprocessing.MinMaxScaler()\n",
    "X_scaler3_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler4_a = preprocessing.MinMaxScaler()\n",
    "X_scaler4_b = preprocessing.MinMaxScaler()\n",
    "X_scaler4_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler5_a = preprocessing.MinMaxScaler()\n",
    "X_scaler5_b = preprocessing.MinMaxScaler()\n",
    "X_scaler5_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "\n",
    "X_scaler6 = preprocessing.MinMaxScaler()\n",
    "X_scaler7 = preprocessing.MinMaxScaler()\n",
    "X_scaler8 = preprocessing.MinMaxScaler()\n",
    "X_scaler9 = preprocessing.MinMaxScaler()\n",
    "X_scaler10 = preprocessing.MinMaxScaler()\n",
    "\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train1_a = X_scaler1_a.fit_transform(rnn_train1_a)\n",
    "rnn_scaled_train1_b = X_scaler1_b.fit_transform(rnn_train1_b)\n",
    "rnn_scaled_train1_c = X_scaler1_c.fit_transform(rnn_train1_c)\n",
    "\n",
    "rnn_scaled_train2_a = X_scaler2_a.fit_transform(rnn_train2_a)\n",
    "rnn_scaled_train2_b = X_scaler2_b.fit_transform(rnn_train2_b)\n",
    "rnn_scaled_train2_c = X_scaler2_c.fit_transform(rnn_train2_c)\n",
    "\n",
    "rnn_scaled_train3_a = X_scaler3_a.fit_transform(rnn_train3_a)\n",
    "rnn_scaled_train3_b = X_scaler3_b.fit_transform(rnn_train3_b)\n",
    "rnn_scaled_train3_c = X_scaler3_c.fit_transform(rnn_train3_c)\n",
    "\n",
    "rnn_scaled_train4_a = X_scaler4_a.fit_transform(rnn_train4_a)\n",
    "rnn_scaled_train4_b = X_scaler4_b.fit_transform(rnn_train4_b)\n",
    "rnn_scaled_train4_c = X_scaler4_c.fit_transform(rnn_train4_c)\n",
    "\n",
    "rnn_scaled_train5_a = X_scaler5_a.fit_transform(rnn_train5_a)\n",
    "rnn_scaled_train5_b = X_scaler5_b.fit_transform(rnn_train5_b)\n",
    "rnn_scaled_train5_c = X_scaler5_c.fit_transform(rnn_train5_c)\n",
    "\n",
    "rnn_scaled_train6 = X_scaler6.fit_transform(rnn_train6)\n",
    "rnn_scaled_train7 = X_scaler7.fit_transform(rnn_train7)\n",
    "rnn_scaled_train8 = X_scaler8.fit_transform(rnn_train8)\n",
    "rnn_scaled_train9 = X_scaler9.fit_transform(rnn_train9)\n",
    "rnn_scaled_train10 = X_scaler10.fit_transform(rnn_train10)\n",
    "\n",
    "Y_train_Scaled   = Y_scaler.fit_transform(Y_train)\n",
    "Y_test_scaled = Y_scaler.transform(Y_test)\n",
    "\n",
    "X_train_Scaled = np.hstack(\n",
    "    (rnn_scaled_train1_a, rnn_scaled_train1_b, rnn_scaled_train1_c, rnn_scaled_train2_a, rnn_scaled_train2_b, rnn_scaled_train2_c,\n",
    "     rnn_scaled_train3_a, rnn_scaled_train3_b, rnn_scaled_train3_c, rnn_scaled_train4_a, rnn_scaled_train4_b, rnn_scaled_train4_c,\n",
    "     rnn_scaled_train5_a, rnn_scaled_train5_b, rnn_scaled_train5_c,rnn_scaled_train6,rnn_scaled_train7, rnn_scaled_train8,\n",
    "     rnn_scaled_train9, rnn_scaled_train10)\n",
    ").reshape(rnn_train6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled = np.hstack(\n",
    "    (X_scaler1_a.transform(rnn_test1_a),X_scaler1_b.transform(rnn_test1_b),X_scaler1_c.transform(rnn_test1_c),\n",
    "     X_scaler2_a.transform(rnn_test2_a),X_scaler2_b.transform(rnn_test2_b),X_scaler2_c.transform(rnn_test2_c),\n",
    "     X_scaler3_a.transform(rnn_test3_a),X_scaler3_b.transform(rnn_test3_b),X_scaler3_c.transform(rnn_test3_c),\n",
    "     X_scaler4_a.transform(rnn_test4_a),X_scaler4_b.transform(rnn_test4_b),X_scaler4_c.transform(rnn_test4_c),\n",
    "     X_scaler5_a.transform(rnn_test5_a),X_scaler5_b.transform(rnn_test5_b),X_scaler5_c.transform(rnn_test5_c),\n",
    "     X_scaler6.transform(rnn_test6),X_scaler7.transform(rnn_test7),X_scaler8.transform(rnn_test8),\n",
    "     X_scaler9.transform(rnn_test9), X_scaler10.transform(rnn_test10))\n",
    ").reshape(rnn_test6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "\n",
    "\n",
    "score_acc = make_scorer(mean_absolute_error)\n",
    "mse = make_scorer(MSE, greater_is_better=False)\n",
    "i_shape=(X_train_Scaled.shape[1], X_train_Scaled.shape[2])\n",
    "      \n",
    "pp ={\n",
    "    'neurons_0': list(range(16, 256, 16)),\n",
    "    'activation_0':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "    'neurons_1': list(range(16, 256, 16)),\n",
    "    'activation_1':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "    'neurons_2': list(range(16, 256, 16)),\n",
    "    'activation_2':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "    'neurons_3': list(range(16, 256, 16)),\n",
    "    'activation_3':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "    'neurons_4': list(range(16, 256, 16)),\n",
    "    'activation_4':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "\n",
    "    'learning_rate': list(np.linspace(0.0001,0.02, 10)),        \n",
    "    'layers1': list(range(1,3, 1)),\n",
    "    'layers2': list(range(1,3, 1)),\n",
    "    'layers3': list(range(1,3, 1)),\n",
    "    'layers4': list(range(1,3, 1)),\n",
    "    'dropout_rate': list(np.linspace(0.0,0.2, 10)),\n",
    "    'batch_size': list(range(16, 64, 16)),\n",
    "    'epochs': [200]\n",
    "}\n",
    "\n",
    "def create_model(x_train, y_train, x_val, y_val, params):\n",
    "            nn = Sequential()\n",
    "            nn.add(Flatten(input_shape=i_shape))\n",
    "\n",
    "            for i in range(params['layers1']):\n",
    "                nn.add(Dense(params['neurons_0'], input_shape=i_shape, activation=params['activation_0']))\n",
    "                \n",
    "            for i in range(params['layers2']):\n",
    "                nn.add(Dense(params['neurons_1'], activation=params['activation_1']))\n",
    "   \n",
    "            for i in range(params['layers3']):\n",
    "                nn.add(Dense(params['neurons_2'], activation=params['activation_2']))\n",
    "            \n",
    "            nn.add(Dropout(params['dropout_rate'], seed=123))\n",
    "            \n",
    "            for i in range(params['layers4']):\n",
    "                nn.add(Dense(params['neurons_3'], activation=params['activation_3']))\n",
    "                \n",
    "            nn.add(Dense(params['neurons_4'], activation=params['activation_4']))\n",
    "            nn.add(Dense(16))\n",
    "            opt = Adam(lr = params['learning_rate'])\n",
    "            nn.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mean_absolute_error'])\n",
    "            out=nn.fit(x_train, y_train,validation_data=[x_val, y_val], batch_size=params['batch_size'], epochs=params['epochs'], verbose=2, \n",
    "                          callbacks=[talos.utils.early_stopper(epochs=params['epochs'], mode='moderate', \n",
    "                                                               min_delta=0.001, monitor='mean_absolute_error')])\n",
    "            return out, nn\n",
    "        \n",
    "h = talos.Scan(x=X_train_Scaled,y=Y_train_Scaled,x_val=X_test_Scaled,  y_val=Y_test_scaled, params=pp, model=create_model,val_split=0.2,\n",
    "            experiment_name ='bm_1-3', random_method='quantum', round_limit=30, print_params=True)\n",
    "h.data.sort_values(by='val_mean_absolute_error', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed1363f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b71fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi Headed RNN DNN Models Search Maximum training Data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import concat\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from math import floor\n",
    "from math import sqrt\n",
    "from datetime import timedelta as td\n",
    "\n",
    "# tensorflow &keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "from tensorflow.keras.models import load_model, save_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "# from keras.layers import concatenate\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "from keras.activations import *\n",
    "\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import talos\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "# dat = pd.read_csv(\"C:/Users/coconnor/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "# dat = pd.read_csv(\"./data/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True,  date_parser=date_parse)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True,  date_parser=date_parse)\n",
    "# dat = pd.read_csv(\"/home/coconnor/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[:7250,:]\n",
    "Y_train=Y.iloc[:7250,:]\n",
    "X_test=X.iloc[7250:8739,:]\n",
    "Y_test=Y.iloc[7250:8739,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:11666,:]\n",
    "# Y_train=Y.iloc[:11666,:]\n",
    "# X_test=X.iloc[11666:13155,:]\n",
    "# Y_test=Y.iloc[11666:13155,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:16082,:]\n",
    "# Y_train=Y.iloc[:16082,:]\n",
    "# X_test=X.iloc[16082:17523,:]\n",
    "# Y_test=Y.iloc[16082:17523,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:20498,:]\n",
    "# Y_train=Y.iloc[:20498,:]\n",
    "# X_test=X.iloc[20498:21843,:]\n",
    "# Y_test=Y.iloc[20498:21843,:]\n",
    "\n",
    "rnn_train_LSTM_1 = X_train[\n",
    "    [\"lag_-3x1\", \"lag_-4x1\", \"lag_-5x1\", \"lag_-6x1\", \"lag_-7x1\", \"lag_-8x1\", \"lag_-9x1\", \"lag_-10x1\", \"lag_-11x1\",\n",
    "     \"lag_-12x1\", \"lag_-13x1\", \"lag_-14x1\", \"lag_-15x1\", \"lag_-16x1\", \"lag_-17x1\", \"lag_-18x1\", \"lag_-19x1\",\n",
    "     \"lag_-20x1\", \"lag_-21x1\", \"lag_-22x1\", \"lag_-23x1\", \"lag_-24x1\", \"lag_-25x1\", \"lag_-26x1\", \"lag_-27x1\",\n",
    "     \"lag_-28x1\", \"lag_-29x1\", \"lag_-30x1\", \"lag_-31x1\", \"lag_-32x1\", \"lag_-33x1\", \"lag_-34x1\", \"lag_-35x1\",\n",
    "     \"lag_-36x1\", \"lag_-37x1\", \"lag_-38x1\", \"lag_-39x1\", \"lag_-40x1\", \"lag_-41x1\", \"lag_-42x1\", \"lag_-43x1\",\n",
    "     \"lag_-44x1\", \"lag_-45x1\", \"lag_-46x1\", \"lag_-47x1\", \"lag_-48x1\", \"lag_-49x1\", \"lag_-50x1\"]]\n",
    "rnn_train_LSTM_2 = X_train[\n",
    "    [\"lag_-3x2\", \"lag_-4x2\", \"lag_-5x2\", \"lag_-6x2\", \"lag_-7x2\", \"lag_-8x2\", \"lag_-9x2\", \"lag_-10x2\", \"lag_-11x2\",\n",
    "     \"lag_-12x2\", \"lag_-13x2\", \"lag_-14x2\", \"lag_-15x2\", \"lag_-16x2\", \"lag_-17x2\", \"lag_-18x2\", \"lag_-19x2\",\n",
    "     \"lag_-20x2\", \"lag_-21x2\", \"lag_-22x2\", \"lag_-23x2\", \"lag_-24x2\", \"lag_-25x2\", \"lag_-26x2\", \"lag_-27x2\",\n",
    "     \"lag_-28x2\", \"lag_-29x2\", \"lag_-30x2\", \"lag_-31x2\", \"lag_-32x2\", \"lag_-33x2\", \"lag_-34x2\", \"lag_-35x2\",\n",
    "     \"lag_-36x2\", \"lag_-37x2\", \"lag_-38x2\", \"lag_-39x2\", \"lag_-40x2\", \"lag_-41x2\", \"lag_-42x2\", \"lag_-43x2\",\n",
    "     \"lag_-44x2\", \"lag_-45x2\", \"lag_-46x2\", \"lag_-47x2\", \"lag_-48x2\", \"lag_-49x2\", \"lag_-50x2\"]]\n",
    "rnn_train_LSTM_3 = X_train[\n",
    "    [\"lag_-2x3\", \"lag_-3x3\", \"lag_-4x3\", \"lag_-5x3\", \"lag_-6x3\", \"lag_-7x3\", \"lag_-8x3\", \"lag_-9x3\", \"lag_-10x3\",\n",
    "     \"lag_-11x3\", \"lag_-12x3\", \"lag_-13x3\", \"lag_-14x3\", \"lag_-15x3\", \"lag_-16x3\", \"lag_-17x3\", \"lag_-18x3\",\n",
    "     \"lag_-19x3\", \"lag_-20x3\", \"lag_-21x3\", \"lag_-22x3\", \"lag_-23x3\", \"lag_-24x3\", \"lag_-25x3\", \"lag_-26x3\",\n",
    "     \"lag_-27x3\", \"lag_-28x3\", \"lag_-29x3\", \"lag_-30x3\", \"lag_-31x3\", \"lag_-32x3\", \"lag_-33x3\", \"lag_-34x3\",\n",
    "     \"lag_-35x3\", \"lag_-36x3\", \"lag_-37x3\", \"lag_-38x3\", \"lag_-39x3\", \"lag_-40x3\", \"lag_-41x3\", \"lag_-42x3\",\n",
    "     \"lag_-43x3\", \"lag_-44x3\", \"lag_-45x3\", \"lag_-46x3\", \"lag_-47x3\", \"lag_-48x3\", \"lag_-49x3\"]]\n",
    "rnn_train_LSTM_4 = X_train[\n",
    "    [\"lag_0x6\", \"lag_-1x6\", \"lag_-2x6\", \"lag_-3x6\", \"lag_-4x6\", \"lag_-5x6\", \"lag_-6x6\", \"lag_-7x6\", \"lag_-8x6\",\n",
    "     \"lag_-9x6\", \"lag_-10x6\", \"lag_-11x6\", \"lag_-12x6\", \"lag_-13x6\", \"lag_-14x6\", \"lag_-15x6\", \"lag_-16x6\", \"lag_-17x6\",\n",
    "     \"lag_-18x6\", \"lag_-19x6\", \"lag_-20x6\", \"lag_-21x6\", \"lag_-22x6\", \"lag_-23x6\", \"lag_-24x6\", \"lag_-25x6\",\n",
    "     \"lag_-26x6\", \"lag_-27x6\", \"lag_-28x6\", \"lag_-29x6\", \"lag_-30x6\", \"lag_-31x6\", \"lag_-32x6\", \"lag_-33x6\",\n",
    "     \"lag_-34x6\", \"lag_-35x6\", \"lag_-36x6\", \"lag_-37x6\", \"lag_-38x6\", \"lag_-39x6\", \"lag_-40x6\", \"lag_-41x6\",\n",
    "     \"lag_-42x6\", \"lag_-43x6\", \"lag_-44x6\", \"lag_-45x6\", \"lag_-46x6\", \"lag_-47x6\"]]\n",
    "rnn_train_LSTM_5 = X_train[\n",
    "    [\"lag_-2x12\", \"lag_-3x12\", \"lag_-4x12\", \"lag_-5x12\", \"lag_-6x12\", \"lag_-7x12\", \"lag_-8x12\", \"lag_-9x12\",\n",
    "     \"lag_-10x12\", \"lag_-11x12\", \"lag_-12x12\", \"lag_-13x12\", \"lag_-14x12\", \"lag_-15x12\", \"lag_-16x12\", \"lag_-17x12\",\n",
    "     \"lag_-18x12\", \"lag_-19x12\", \"lag_-20x12\", \"lag_-21x12\", \"lag_-22x12\", \"lag_-23x12\", \"lag_-24x12\", \"lag_-25x12\",\n",
    "     \"lag_-26x12\", \"lag_-27x12\", \"lag_-28x12\", \"lag_-29x12\", \"lag_-30x12\", \"lag_-31x12\", \"lag_-32x12\", \"lag_-33x12\",\n",
    "     \"lag_-34x12\", \"lag_-35x12\", \"lag_-36x12\", \"lag_-37x12\", \"lag_-38x12\", \"lag_-39x12\", \"lag_-40x12\", \"lag_-41x12\",\n",
    "     \"lag_-42x12\", \"lag_-43x12\", \"lag_-44x12\", \"lag_-45x12\", \"lag_-46x12\", \"lag_-47x12\", \"lag_-48x12\", \"lag_-49x12\"]]\n",
    "\n",
    "rnn_test_LSTM_1 = X_test[\n",
    "    [\"lag_-3x1\", \"lag_-4x1\", \"lag_-5x1\", \"lag_-6x1\", \"lag_-7x1\", \"lag_-8x1\", \"lag_-9x1\", \"lag_-10x1\", \"lag_-11x1\",\n",
    "     \"lag_-12x1\", \"lag_-13x1\", \"lag_-14x1\", \"lag_-15x1\", \"lag_-16x1\", \"lag_-17x1\", \"lag_-18x1\", \"lag_-19x1\",\n",
    "     \"lag_-20x1\", \"lag_-21x1\", \"lag_-22x1\", \"lag_-23x1\", \"lag_-24x1\", \"lag_-25x1\", \"lag_-26x1\", \"lag_-27x1\",\n",
    "     \"lag_-28x1\", \"lag_-29x1\", \"lag_-30x1\", \"lag_-31x1\", \"lag_-32x1\", \"lag_-33x1\", \"lag_-34x1\", \"lag_-35x1\",\n",
    "     \"lag_-36x1\", \"lag_-37x1\", \"lag_-38x1\", \"lag_-39x1\", \"lag_-40x1\", \"lag_-41x1\", \"lag_-42x1\", \"lag_-43x1\",\n",
    "     \"lag_-44x1\", \"lag_-45x1\", \"lag_-46x1\", \"lag_-47x1\", \"lag_-48x1\", \"lag_-49x1\", \"lag_-50x1\"]]\n",
    "rnn_test_LSTM_2 = X_test[\n",
    "    [\"lag_-3x2\", \"lag_-4x2\", \"lag_-5x2\", \"lag_-6x2\", \"lag_-7x2\", \"lag_-8x2\", \"lag_-9x2\", \"lag_-10x2\", \"lag_-11x2\",\n",
    "     \"lag_-12x2\", \"lag_-13x2\", \"lag_-14x2\", \"lag_-15x2\", \"lag_-16x2\", \"lag_-17x2\", \"lag_-18x2\", \"lag_-19x2\",\n",
    "     \"lag_-20x2\", \"lag_-21x2\", \"lag_-22x2\", \"lag_-23x2\", \"lag_-24x2\", \"lag_-25x2\", \"lag_-26x2\", \"lag_-27x2\",\n",
    "     \"lag_-28x2\", \"lag_-29x2\", \"lag_-30x2\", \"lag_-31x2\", \"lag_-32x2\", \"lag_-33x2\", \"lag_-34x2\", \"lag_-35x2\",\n",
    "     \"lag_-36x2\", \"lag_-37x2\", \"lag_-38x2\", \"lag_-39x2\", \"lag_-40x2\", \"lag_-41x2\", \"lag_-42x2\", \"lag_-43x2\",\n",
    "     \"lag_-44x2\", \"lag_-45x2\", \"lag_-46x2\", \"lag_-47x2\", \"lag_-48x2\", \"lag_-49x2\", \"lag_-50x2\"]]\n",
    "rnn_test_LSTM_3 = X_test[\n",
    "    [\"lag_-2x3\", \"lag_-3x3\", \"lag_-4x3\", \"lag_-5x3\", \"lag_-6x3\", \"lag_-7x3\", \"lag_-8x3\", \"lag_-9x3\", \"lag_-10x3\",\n",
    "     \"lag_-11x3\", \"lag_-12x3\", \"lag_-13x3\", \"lag_-14x3\", \"lag_-15x3\", \"lag_-16x3\", \"lag_-17x3\", \"lag_-18x3\",\n",
    "     \"lag_-19x3\", \"lag_-20x3\", \"lag_-21x3\", \"lag_-22x3\", \"lag_-23x3\", \"lag_-24x3\", \"lag_-25x3\", \"lag_-26x3\",\n",
    "     \"lag_-27x3\", \"lag_-28x3\", \"lag_-29x3\", \"lag_-30x3\", \"lag_-31x3\", \"lag_-32x3\", \"lag_-33x3\", \"lag_-34x3\",\n",
    "     \"lag_-35x3\", \"lag_-36x3\", \"lag_-37x3\", \"lag_-38x3\", \"lag_-39x3\", \"lag_-40x3\", \"lag_-41x3\", \"lag_-42x3\",\n",
    "     \"lag_-43x3\", \"lag_-44x3\", \"lag_-45x3\", \"lag_-46x3\", \"lag_-47x3\", \"lag_-48x3\", \"lag_-49x3\"]]\n",
    "rnn_test_LSTM_4 = X_test[\n",
    "    [\"lag_0x6\", \"lag_-1x6\", \"lag_-2x6\", \"lag_-3x6\", \"lag_-4x6\", \"lag_-5x6\", \"lag_-6x6\", \"lag_-7x6\", \"lag_-8x6\",\n",
    "     \"lag_-9x6\", \"lag_-10x6\", \"lag_-11x6\", \"lag_-12x6\", \"lag_-13x6\", \"lag_-14x6\", \"lag_-15x6\", \"lag_-16x6\", \"lag_-17x6\",\n",
    "     \"lag_-18x6\", \"lag_-19x6\", \"lag_-20x6\", \"lag_-21x6\", \"lag_-22x6\", \"lag_-23x6\", \"lag_-24x6\", \"lag_-25x6\",\n",
    "     \"lag_-26x6\", \"lag_-27x6\", \"lag_-28x6\", \"lag_-29x6\", \"lag_-30x6\", \"lag_-31x6\", \"lag_-32x6\", \"lag_-33x6\",\n",
    "     \"lag_-34x6\", \"lag_-35x6\", \"lag_-36x6\", \"lag_-37x6\", \"lag_-38x6\", \"lag_-39x6\", \"lag_-40x6\", \"lag_-41x6\",\n",
    "     \"lag_-42x6\", \"lag_-43x6\", \"lag_-44x6\", \"lag_-45x6\", \"lag_-46x6\", \"lag_-47x6\"]]\n",
    "rnn_test_LSTM_5 = X_test[\n",
    "    [\"lag_-2x12\", \"lag_-3x12\", \"lag_-4x12\", \"lag_-5x12\", \"lag_-6x12\", \"lag_-7x12\", \"lag_-8x12\", \"lag_-9x12\",\n",
    "     \"lag_-10x12\", \"lag_-11x12\", \"lag_-12x12\", \"lag_-13x12\", \"lag_-14x12\", \"lag_-15x12\", \"lag_-16x12\", \"lag_-17x12\",\n",
    "     \"lag_-18x12\", \"lag_-19x12\", \"lag_-20x12\", \"lag_-21x12\", \"lag_-22x12\", \"lag_-23x12\", \"lag_-24x12\", \"lag_-25x12\",\n",
    "     \"lag_-26x12\", \"lag_-27x12\", \"lag_-28x12\", \"lag_-29x12\", \"lag_-30x12\", \"lag_-31x12\", \"lag_-32x12\", \"lag_-33x12\",\n",
    "     \"lag_-34x12\", \"lag_-35x12\", \"lag_-36x12\", \"lag_-37x12\", \"lag_-38x12\", \"lag_-39x12\", \"lag_-40x12\", \"lag_-41x12\",\n",
    "     \"lag_-42x12\", \"lag_-43x12\", \"lag_-44x12\", \"lag_-45x12\", \"lag_-46x12\", \"lag_-47x12\", \"lag_-48x12\", \"lag_-49x12\"]]\n",
    "\n",
    "rnn_train_ffnn_1 = X_train[\n",
    "    [\"lag_2x7\", \"lag_3x7\", \"lag_4x7\", \"lag_5x7\", \"lag_6x7\", \"lag_7x7\", \"lag_8x7\", \"lag_9x7\", \"lag_10x7\", \"lag_11x7\",\n",
    "     \"lag_12x7\", \"lag_13x7\", \"lag_14x7\", \"lag_15x7\", \"lag_16x7\", \"lag_17x7\"]]\n",
    "rnn_train_ffnn_2 = X_train[\n",
    "    [\"lag_2x8\", \"lag_3x8\", \"lag_4x8\", \"lag_5x8\", \"lag_6x8\", \"lag_7x8\", \"lag_8x8\", \"lag_9x8\", \"lag_10x8\", \"lag_11x8\",\n",
    "     \"lag_12x8\", \"lag_13x8\", \"lag_14x8\", \"lag_15x8\", \"lag_16x8\", \"lag_17x8\"]]\n",
    "rnn_train_ffnn_3 = X_train[\n",
    "    [\"lag_2x9\", \"lag_3x9\", \"lag_4x9\", \"lag_5x9\", \"lag_6x9\", \"lag_7x9\", \"lag_8x9\", \"lag_9x9\", \"lag_10x9\", \"lag_11x9\",\n",
    "     \"lag_12x9\", \"lag_13x9\", \"lag_14x9\", \"lag_15x9\", \"lag_16x9\", \"lag_17x9\"]]\n",
    "rnn_train_ffnn_4 = X_train[\n",
    "    [\"lag_2x10\", \"lag_3x10\", \"lag_4x10\", \"lag_5x10\", \"lag_6x10\", \"lag_7x10\", \"lag_8x10\", \"lag_9x10\", \"lag_10x10\",\n",
    "     \"lag_11x10\", \"lag_12x10\", \"lag_13x10\", \"lag_14x10\", \"lag_15x10\", \"lag_16x10\", \"lag_17x10\"]]\n",
    "rnn_train_ffnn_5 = X_train[\n",
    "    [\"lag_2x11\", \"lag_3x11\", \"lag_4x11\", \"lag_5x11\", \"lag_6x11\", \"lag_7x11\", \"lag_8x11\", \"lag_9x11\", \"lag_10x11\",\n",
    "     \"lag_11x11\", \"lag_12x11\", \"lag_13x11\", \"lag_14x11\", \"lag_15x11\", \"lag_16x11\", \"lag_17x11\"]]\n",
    "\n",
    "rnn_test_ffnn_1 = X_test[\n",
    "    [\"lag_2x7\", \"lag_3x7\", \"lag_4x7\", \"lag_5x7\", \"lag_6x7\", \"lag_7x7\", \"lag_8x7\", \"lag_9x7\", \"lag_10x7\", \"lag_11x7\",\n",
    "     \"lag_12x7\", \"lag_13x7\", \"lag_14x7\", \"lag_15x7\", \"lag_16x7\", \"lag_17x7\"]]\n",
    "rnn_test_ffnn_2 = X_test[\n",
    "    [\"lag_2x8\", \"lag_3x8\", \"lag_4x8\", \"lag_5x8\", \"lag_6x8\", \"lag_7x8\", \"lag_8x8\", \"lag_9x8\", \"lag_10x8\", \"lag_11x8\",\n",
    "     \"lag_12x8\", \"lag_13x8\", \"lag_14x8\", \"lag_15x8\", \"lag_16x8\", \"lag_17x8\"]]\n",
    "rnn_test_ffnn_3 = X_test[\n",
    "    [\"lag_2x9\", \"lag_3x9\", \"lag_4x9\", \"lag_5x9\", \"lag_6x9\", \"lag_7x9\", \"lag_8x9\", \"lag_9x9\", \"lag_10x9\", \"lag_11x9\",\n",
    "     \"lag_12x9\", \"lag_13x9\", \"lag_14x9\", \"lag_15x9\", \"lag_16x9\", \"lag_17x9\"]]\n",
    "rnn_test_ffnn_4 = X_test[\n",
    "    [\"lag_2x10\", \"lag_3x10\", \"lag_4x10\", \"lag_5x10\", \"lag_6x10\", \"lag_7x10\", \"lag_8x10\", \"lag_9x10\", \"lag_10x10\",\n",
    "     \"lag_11x10\", \"lag_12x10\", \"lag_13x10\", \"lag_14x10\", \"lag_15x10\", \"lag_16x10\", \"lag_17x10\"]]\n",
    "rnn_test_ffnn_5 = X_test[\n",
    "    [\"lag_2x11\", \"lag_3x11\", \"lag_4x11\", \"lag_5x11\", \"lag_6x11\", \"lag_7x11\", \"lag_8x11\", \"lag_9x11\", \"lag_10x11\",\n",
    "     \"lag_11x11\", \"lag_12x11\", \"lag_13x11\", \"lag_14x11\", \"lag_15x11\", \"lag_16x11\", \"lag_17x11\"]]\n",
    "\n",
    "rnn_Y = Y_train[\n",
    "    [\"lag_2y\", \"lag_3y\", \"lag_4y\", \"lag_5y\", \"lag_6y\", \"lag_7y\", \"lag_8y\", \"lag_9y\", \"lag_10y\", \"lag_11y\", \"lag_12y\",\n",
    "     \"lag_13y\", \"lag_14y\", \"lag_15y\", \"lag_16y\", \"lag_17y\"]]\n",
    "\n",
    "X_scaler_LSTM_1 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_2 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_3 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_4 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler_ffnn_1 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_2 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_3 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_4 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train_LSTM_1 = X_scaler_LSTM_1.fit_transform(rnn_train_LSTM_1)\n",
    "rnn_scaled_train_LSTM_2 = X_scaler_LSTM_2.fit_transform(rnn_train_LSTM_2)\n",
    "rnn_scaled_train_LSTM_3 = X_scaler_LSTM_3.fit_transform(rnn_train_LSTM_3)\n",
    "rnn_scaled_train_LSTM_4 = X_scaler_LSTM_4.fit_transform(rnn_train_LSTM_4)\n",
    "rnn_scaled_train_LSTM_5 = X_scaler_LSTM_5.fit_transform(rnn_train_LSTM_5)\n",
    "\n",
    "rnn_scaled_train_ffnn_1 = X_scaler_ffnn_1.fit_transform(rnn_train_ffnn_1)\n",
    "rnn_scaled_train_ffnn_2 = X_scaler_ffnn_2.fit_transform(rnn_train_ffnn_2)\n",
    "rnn_scaled_train_ffnn_3 = X_scaler_ffnn_3.fit_transform(rnn_train_ffnn_3)\n",
    "rnn_scaled_train_ffnn_4 = X_scaler_ffnn_4.fit_transform(rnn_train_ffnn_4)\n",
    "rnn_scaled_train_ffnn_5 = X_scaler_ffnn_5.fit_transform(rnn_train_ffnn_5)\n",
    "\n",
    "Y_train_Scaled = Y_scaler.fit_transform(Y_train)\n",
    "Y_test_scaled = Y_scaler.transform(Y_test)\n",
    "\n",
    "X_train_Scaled_LSTM = np.hstack(\n",
    "    (rnn_scaled_train_LSTM_1, rnn_scaled_train_LSTM_2, rnn_scaled_train_LSTM_3,\n",
    "     rnn_scaled_train_LSTM_4, rnn_scaled_train_LSTM_5)\n",
    ").reshape(rnn_train_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "X_train_Scaled_ffnn = np.hstack(\n",
    "    (rnn_scaled_train_ffnn_1, rnn_scaled_train_ffnn_2, rnn_scaled_train_ffnn_3,\n",
    "     rnn_scaled_train_ffnn_4, rnn_scaled_train_ffnn_5)\n",
    ").reshape(rnn_train_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled_LSTM = np.hstack(\n",
    "    (X_scaler_LSTM_1.transform(rnn_test_LSTM_1), X_scaler_LSTM_2.transform(rnn_test_LSTM_2),\n",
    "     X_scaler_LSTM_3.transform(rnn_test_LSTM_3), X_scaler_LSTM_4.transform(rnn_test_LSTM_4),\n",
    "     X_scaler_LSTM_5.transform(rnn_test_LSTM_5))\n",
    ").reshape(rnn_test_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled_ffnn = np.hstack(\n",
    "    (X_scaler_ffnn_1.transform(rnn_test_ffnn_1), X_scaler_ffnn_2.transform(rnn_test_ffnn_2),\n",
    "     X_scaler_ffnn_3.transform(rnn_test_ffnn_3), X_scaler_ffnn_4.transform(rnn_test_ffnn_4),\n",
    "     X_scaler_ffnn_5.transform(rnn_test_ffnn_5))\n",
    ").reshape(rnn_test_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "pp = {'lstm_neurons_0': [16, 32, 64, 128, 192, 256],\n",
    "\n",
    "      'ffnn_neurons_0': [16, 32, 64, 128, 192, 256],\n",
    "\n",
    "      'dense_f_neurons': [16, 32, 64, 128, 192, 256],\n",
    "      'lstm_activation_0': ['relu', 'sigmoid', 'tanh'],\n",
    "\n",
    "      'activation_0': ['relu', 'sigmoid', 'tanh', LeakyReLU],\n",
    "      'activation_1': ['relu', 'sigmoid', 'tanh', LeakyReLU],\n",
    "\n",
    "      'learning_rate': list(np.linspace(0.0001, 0.02, 10)),\n",
    "      'dropout_rate_lstm': list(np.linspace(0.0,0.2, 10)),\n",
    "      'dropout_rate_ffnn': list(np.linspace(0.0,0.2, 10)),\n",
    "      \n",
    "      'batch_size': [4, 8, 16, 32, 48],\n",
    "      'epochs': [300]\n",
    "      }\n",
    "i_shape_lstm = (X_train_Scaled_LSTM.shape[1], X_train_Scaled_LSTM.shape[2])\n",
    "i_shape_ffnn = (X_train_Scaled_ffnn.shape[1], X_train_Scaled_ffnn.shape[2])\n",
    "\n",
    "\n",
    "def create_model(x_train, y_train, x_val, y_val, params):\n",
    "    visible1 = Input(shape=(i_shape_lstm))\n",
    "    dense1 = LSTM(params['lstm_neurons_0'], return_sequences=True, activation=params['lstm_activation_0'], input_shape=i_shape_lstm)(visible1)\n",
    "    dense2 = LSTM(params['lstm_neurons_0'], return_sequences=True, activation=params['lstm_activation_0'], input_shape=i_shape_lstm)(dense1)\n",
    "    do_lstm = Dropout(params['dropout_rate_lstm'])(dense2)\n",
    "    dense3 = LSTM(params['lstm_neurons_0'], return_sequences=True, activation=params['lstm_activation_0'], input_shape=i_shape_lstm)(do_lstm)\n",
    "    flat1 = Flatten()(dense3)\n",
    "\n",
    "    visible2 = Input(shape=(i_shape_ffnn))\n",
    "    dense5 = Dense(params['ffnn_neurons_0'], activation=params['activation_0'])(visible2)\n",
    "    dense6 = Dense(params['ffnn_neurons_0'], activation=params['activation_0'])(dense5)\n",
    "    do_ffnn = Dropout(params['dropout_rate_ffnn'])(dense6)\n",
    "    dense7 = Dense(params['ffnn_neurons_0'], activation=params['activation_0'])(do_ffnn)\n",
    "    flat2 = Flatten()(dense7)\n",
    "\n",
    "    merged = concatenate([flat1, flat2])\n",
    "    dense_f = Dense(params['dense_f_neurons'], activation=params['activation_1'])(merged)\n",
    "    output = Dense(16)(dense_f)\n",
    "    model = Model(inputs=[visible1, visible2], outputs=output)\n",
    "\n",
    "    opt = Adam(lr=params['learning_rate'])\n",
    "    model.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mean_absolute_error'])\n",
    "\n",
    "\n",
    "    out = model.fit(x=x_train, y=y_train, validation_data=[x_val, y_val],\n",
    "                    batch_size=params['batch_size'], epochs=params['epochs'], verbose=2,\n",
    "                    callbacks=[talos.utils.early_stopper(epochs=params['epochs'], mode='moderate',\n",
    "                                                         min_delta=0.001, monitor='mean_absolute_error')])\n",
    "    return out, model\n",
    "\n",
    "\n",
    "h = talos.Scan(x=[X_train_Scaled_LSTM, X_train_Scaled_ffnn], y=Y_train_Scaled, x_val=[X_train_Scaled_LSTM, X_train_Scaled_ffnn], y_val=Y_train_Scaled, params=pp, model=create_model,\n",
    "               experiment_name='MH_DNN_1-3', round_limit=30, print_params=True)\n",
    "h.data.sort_values(by='val_mean_absolute_error', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f090d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi Headed RNN DNN Models Search 90 days Training Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import concat\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from math import floor\n",
    "from math import sqrt\n",
    "from datetime import timedelta as td\n",
    "\n",
    "# tensorflow &keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "from tensorflow.keras.models import load_model, save_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "# from keras.layers import concatenate\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "from keras.activations import *\n",
    "\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import talos\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "# dat = pd.read_csv(\"C:/Users/coconnor/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "# dat = pd.read_csv(\"./data/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True,  date_parser=date_parse)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True,  date_parser=date_parse)\n",
    "# dat = pd.read_csv(\"/home/coconnor/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[4322:7731,:]\n",
    "Y_train=Y.iloc[4322:7731,:]\n",
    "X_test=X.iloc[7731:8739,:]\n",
    "Y_test=Y.iloc[7731:8739,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[8738:12146,:]\n",
    "# Y_train=Y.iloc[8738:12146,:]\n",
    "# X_test=X.iloc[12146:13155,:]\n",
    "# Y_test=Y.iloc[12146:13155,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[13154:16514,:]\n",
    "# Y_train=Y.iloc[13154:16514,:]\n",
    "# X_test=X.iloc[16514:17523,:]\n",
    "# Y_test=Y.iloc[16514:17523,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[17522:20834,:]\n",
    "# Y_train=Y.iloc[17522:20834,:]\n",
    "# X_test=X.iloc[20834:21843,:]\n",
    "# Y_test=Y.iloc[20834:21843,:]\n",
    "\n",
    "rnn_train_LSTM_1 = X_train[\n",
    "    [\"lag_-3x1\", \"lag_-4x1\", \"lag_-5x1\", \"lag_-6x1\", \"lag_-7x1\", \"lag_-8x1\", \"lag_-9x1\", \"lag_-10x1\", \"lag_-11x1\",\n",
    "     \"lag_-12x1\", \"lag_-13x1\", \"lag_-14x1\", \"lag_-15x1\", \"lag_-16x1\", \"lag_-17x1\", \"lag_-18x1\", \"lag_-19x1\",\n",
    "     \"lag_-20x1\", \"lag_-21x1\", \"lag_-22x1\", \"lag_-23x1\", \"lag_-24x1\", \"lag_-25x1\", \"lag_-26x1\", \"lag_-27x1\",\n",
    "     \"lag_-28x1\", \"lag_-29x1\", \"lag_-30x1\", \"lag_-31x1\", \"lag_-32x1\", \"lag_-33x1\", \"lag_-34x1\", \"lag_-35x1\",\n",
    "     \"lag_-36x1\", \"lag_-37x1\", \"lag_-38x1\", \"lag_-39x1\", \"lag_-40x1\", \"lag_-41x1\", \"lag_-42x1\", \"lag_-43x1\",\n",
    "     \"lag_-44x1\", \"lag_-45x1\", \"lag_-46x1\", \"lag_-47x1\", \"lag_-48x1\", \"lag_-49x1\", \"lag_-50x1\"]]\n",
    "rnn_train_LSTM_2 = X_train[\n",
    "    [\"lag_-3x2\", \"lag_-4x2\", \"lag_-5x2\", \"lag_-6x2\", \"lag_-7x2\", \"lag_-8x2\", \"lag_-9x2\", \"lag_-10x2\", \"lag_-11x2\",\n",
    "     \"lag_-12x2\", \"lag_-13x2\", \"lag_-14x2\", \"lag_-15x2\", \"lag_-16x2\", \"lag_-17x2\", \"lag_-18x2\", \"lag_-19x2\",\n",
    "     \"lag_-20x2\", \"lag_-21x2\", \"lag_-22x2\", \"lag_-23x2\", \"lag_-24x2\", \"lag_-25x2\", \"lag_-26x2\", \"lag_-27x2\",\n",
    "     \"lag_-28x2\", \"lag_-29x2\", \"lag_-30x2\", \"lag_-31x2\", \"lag_-32x2\", \"lag_-33x2\", \"lag_-34x2\", \"lag_-35x2\",\n",
    "     \"lag_-36x2\", \"lag_-37x2\", \"lag_-38x2\", \"lag_-39x2\", \"lag_-40x2\", \"lag_-41x2\", \"lag_-42x2\", \"lag_-43x2\",\n",
    "     \"lag_-44x2\", \"lag_-45x2\", \"lag_-46x2\", \"lag_-47x2\", \"lag_-48x2\", \"lag_-49x2\", \"lag_-50x2\"]]\n",
    "rnn_train_LSTM_3 = X_train[\n",
    "    [\"lag_-2x3\", \"lag_-3x3\", \"lag_-4x3\", \"lag_-5x3\", \"lag_-6x3\", \"lag_-7x3\", \"lag_-8x3\", \"lag_-9x3\", \"lag_-10x3\",\n",
    "     \"lag_-11x3\", \"lag_-12x3\", \"lag_-13x3\", \"lag_-14x3\", \"lag_-15x3\", \"lag_-16x3\", \"lag_-17x3\", \"lag_-18x3\",\n",
    "     \"lag_-19x3\", \"lag_-20x3\", \"lag_-21x3\", \"lag_-22x3\", \"lag_-23x3\", \"lag_-24x3\", \"lag_-25x3\", \"lag_-26x3\",\n",
    "     \"lag_-27x3\", \"lag_-28x3\", \"lag_-29x3\", \"lag_-30x3\", \"lag_-31x3\", \"lag_-32x3\", \"lag_-33x3\", \"lag_-34x3\",\n",
    "     \"lag_-35x3\", \"lag_-36x3\", \"lag_-37x3\", \"lag_-38x3\", \"lag_-39x3\", \"lag_-40x3\", \"lag_-41x3\", \"lag_-42x3\",\n",
    "     \"lag_-43x3\", \"lag_-44x3\", \"lag_-45x3\", \"lag_-46x3\", \"lag_-47x3\", \"lag_-48x3\", \"lag_-49x3\"]]\n",
    "rnn_train_LSTM_4 = X_train[\n",
    "    [\"lag_0x6\", \"lag_-1x6\", \"lag_-2x6\", \"lag_-3x6\", \"lag_-4x6\", \"lag_-5x6\", \"lag_-6x6\", \"lag_-7x6\", \"lag_-8x6\",\n",
    "     \"lag_-9x6\", \"lag_-10x6\", \"lag_-11x6\", \"lag_-12x6\", \"lag_-13x6\", \"lag_-14x6\", \"lag_-15x6\", \"lag_-16x6\", \"lag_-17x6\",\n",
    "     \"lag_-18x6\", \"lag_-19x6\", \"lag_-20x6\", \"lag_-21x6\", \"lag_-22x6\", \"lag_-23x6\", \"lag_-24x6\", \"lag_-25x6\",\n",
    "     \"lag_-26x6\", \"lag_-27x6\", \"lag_-28x6\", \"lag_-29x6\", \"lag_-30x6\", \"lag_-31x6\", \"lag_-32x6\", \"lag_-33x6\",\n",
    "     \"lag_-34x6\", \"lag_-35x6\", \"lag_-36x6\", \"lag_-37x6\", \"lag_-38x6\", \"lag_-39x6\", \"lag_-40x6\", \"lag_-41x6\",\n",
    "     \"lag_-42x6\", \"lag_-43x6\", \"lag_-44x6\", \"lag_-45x6\", \"lag_-46x6\", \"lag_-47x6\"]]\n",
    "rnn_train_LSTM_5 = X_train[\n",
    "    [\"lag_-2x12\", \"lag_-3x12\", \"lag_-4x12\", \"lag_-5x12\", \"lag_-6x12\", \"lag_-7x12\", \"lag_-8x12\", \"lag_-9x12\",\n",
    "     \"lag_-10x12\", \"lag_-11x12\", \"lag_-12x12\", \"lag_-13x12\", \"lag_-14x12\", \"lag_-15x12\", \"lag_-16x12\", \"lag_-17x12\",\n",
    "     \"lag_-18x12\", \"lag_-19x12\", \"lag_-20x12\", \"lag_-21x12\", \"lag_-22x12\", \"lag_-23x12\", \"lag_-24x12\", \"lag_-25x12\",\n",
    "     \"lag_-26x12\", \"lag_-27x12\", \"lag_-28x12\", \"lag_-29x12\", \"lag_-30x12\", \"lag_-31x12\", \"lag_-32x12\", \"lag_-33x12\",\n",
    "     \"lag_-34x12\", \"lag_-35x12\", \"lag_-36x12\", \"lag_-37x12\", \"lag_-38x12\", \"lag_-39x12\", \"lag_-40x12\", \"lag_-41x12\",\n",
    "     \"lag_-42x12\", \"lag_-43x12\", \"lag_-44x12\", \"lag_-45x12\", \"lag_-46x12\", \"lag_-47x12\", \"lag_-48x12\", \"lag_-49x12\"]]\n",
    "\n",
    "rnn_test_LSTM_1 = X_test[\n",
    "    [\"lag_-3x1\", \"lag_-4x1\", \"lag_-5x1\", \"lag_-6x1\", \"lag_-7x1\", \"lag_-8x1\", \"lag_-9x1\", \"lag_-10x1\", \"lag_-11x1\",\n",
    "     \"lag_-12x1\", \"lag_-13x1\", \"lag_-14x1\", \"lag_-15x1\", \"lag_-16x1\", \"lag_-17x1\", \"lag_-18x1\", \"lag_-19x1\",\n",
    "     \"lag_-20x1\", \"lag_-21x1\", \"lag_-22x1\", \"lag_-23x1\", \"lag_-24x1\", \"lag_-25x1\", \"lag_-26x1\", \"lag_-27x1\",\n",
    "     \"lag_-28x1\", \"lag_-29x1\", \"lag_-30x1\", \"lag_-31x1\", \"lag_-32x1\", \"lag_-33x1\", \"lag_-34x1\", \"lag_-35x1\",\n",
    "     \"lag_-36x1\", \"lag_-37x1\", \"lag_-38x1\", \"lag_-39x1\", \"lag_-40x1\", \"lag_-41x1\", \"lag_-42x1\", \"lag_-43x1\",\n",
    "     \"lag_-44x1\", \"lag_-45x1\", \"lag_-46x1\", \"lag_-47x1\", \"lag_-48x1\", \"lag_-49x1\", \"lag_-50x1\"]]\n",
    "rnn_test_LSTM_2 = X_test[\n",
    "    [\"lag_-3x2\", \"lag_-4x2\", \"lag_-5x2\", \"lag_-6x2\", \"lag_-7x2\", \"lag_-8x2\", \"lag_-9x2\", \"lag_-10x2\", \"lag_-11x2\",\n",
    "     \"lag_-12x2\", \"lag_-13x2\", \"lag_-14x2\", \"lag_-15x2\", \"lag_-16x2\", \"lag_-17x2\", \"lag_-18x2\", \"lag_-19x2\",\n",
    "     \"lag_-20x2\", \"lag_-21x2\", \"lag_-22x2\", \"lag_-23x2\", \"lag_-24x2\", \"lag_-25x2\", \"lag_-26x2\", \"lag_-27x2\",\n",
    "     \"lag_-28x2\", \"lag_-29x2\", \"lag_-30x2\", \"lag_-31x2\", \"lag_-32x2\", \"lag_-33x2\", \"lag_-34x2\", \"lag_-35x2\",\n",
    "     \"lag_-36x2\", \"lag_-37x2\", \"lag_-38x2\", \"lag_-39x2\", \"lag_-40x2\", \"lag_-41x2\", \"lag_-42x2\", \"lag_-43x2\",\n",
    "     \"lag_-44x2\", \"lag_-45x2\", \"lag_-46x2\", \"lag_-47x2\", \"lag_-48x2\", \"lag_-49x2\", \"lag_-50x2\"]]\n",
    "rnn_test_LSTM_3 = X_test[\n",
    "    [\"lag_-2x3\", \"lag_-3x3\", \"lag_-4x3\", \"lag_-5x3\", \"lag_-6x3\", \"lag_-7x3\", \"lag_-8x3\", \"lag_-9x3\", \"lag_-10x3\",\n",
    "     \"lag_-11x3\", \"lag_-12x3\", \"lag_-13x3\", \"lag_-14x3\", \"lag_-15x3\", \"lag_-16x3\", \"lag_-17x3\", \"lag_-18x3\",\n",
    "     \"lag_-19x3\", \"lag_-20x3\", \"lag_-21x3\", \"lag_-22x3\", \"lag_-23x3\", \"lag_-24x3\", \"lag_-25x3\", \"lag_-26x3\",\n",
    "     \"lag_-27x3\", \"lag_-28x3\", \"lag_-29x3\", \"lag_-30x3\", \"lag_-31x3\", \"lag_-32x3\", \"lag_-33x3\", \"lag_-34x3\",\n",
    "     \"lag_-35x3\", \"lag_-36x3\", \"lag_-37x3\", \"lag_-38x3\", \"lag_-39x3\", \"lag_-40x3\", \"lag_-41x3\", \"lag_-42x3\",\n",
    "     \"lag_-43x3\", \"lag_-44x3\", \"lag_-45x3\", \"lag_-46x3\", \"lag_-47x3\", \"lag_-48x3\", \"lag_-49x3\"]]\n",
    "rnn_test_LSTM_4 = X_test[\n",
    "    [\"lag_0x6\", \"lag_-1x6\", \"lag_-2x6\", \"lag_-3x6\", \"lag_-4x6\", \"lag_-5x6\", \"lag_-6x6\", \"lag_-7x6\", \"lag_-8x6\",\n",
    "     \"lag_-9x6\", \"lag_-10x6\", \"lag_-11x6\", \"lag_-12x6\", \"lag_-13x6\", \"lag_-14x6\", \"lag_-15x6\", \"lag_-16x6\", \"lag_-17x6\",\n",
    "     \"lag_-18x6\", \"lag_-19x6\", \"lag_-20x6\", \"lag_-21x6\", \"lag_-22x6\", \"lag_-23x6\", \"lag_-24x6\", \"lag_-25x6\",\n",
    "     \"lag_-26x6\", \"lag_-27x6\", \"lag_-28x6\", \"lag_-29x6\", \"lag_-30x6\", \"lag_-31x6\", \"lag_-32x6\", \"lag_-33x6\",\n",
    "     \"lag_-34x6\", \"lag_-35x6\", \"lag_-36x6\", \"lag_-37x6\", \"lag_-38x6\", \"lag_-39x6\", \"lag_-40x6\", \"lag_-41x6\",\n",
    "     \"lag_-42x6\", \"lag_-43x6\", \"lag_-44x6\", \"lag_-45x6\", \"lag_-46x6\", \"lag_-47x6\"]]\n",
    "rnn_test_LSTM_5 = X_test[\n",
    "    [\"lag_-2x12\", \"lag_-3x12\", \"lag_-4x12\", \"lag_-5x12\", \"lag_-6x12\", \"lag_-7x12\", \"lag_-8x12\", \"lag_-9x12\",\n",
    "     \"lag_-10x12\", \"lag_-11x12\", \"lag_-12x12\", \"lag_-13x12\", \"lag_-14x12\", \"lag_-15x12\", \"lag_-16x12\", \"lag_-17x12\",\n",
    "     \"lag_-18x12\", \"lag_-19x12\", \"lag_-20x12\", \"lag_-21x12\", \"lag_-22x12\", \"lag_-23x12\", \"lag_-24x12\", \"lag_-25x12\",\n",
    "     \"lag_-26x12\", \"lag_-27x12\", \"lag_-28x12\", \"lag_-29x12\", \"lag_-30x12\", \"lag_-31x12\", \"lag_-32x12\", \"lag_-33x12\",\n",
    "     \"lag_-34x12\", \"lag_-35x12\", \"lag_-36x12\", \"lag_-37x12\", \"lag_-38x12\", \"lag_-39x12\", \"lag_-40x12\", \"lag_-41x12\",\n",
    "     \"lag_-42x12\", \"lag_-43x12\", \"lag_-44x12\", \"lag_-45x12\", \"lag_-46x12\", \"lag_-47x12\", \"lag_-48x12\", \"lag_-49x12\"]]\n",
    "\n",
    "rnn_train_ffnn_1 = X_train[\n",
    "    [\"lag_2x7\", \"lag_3x7\", \"lag_4x7\", \"lag_5x7\", \"lag_6x7\", \"lag_7x7\", \"lag_8x7\", \"lag_9x7\", \"lag_10x7\", \"lag_11x7\",\n",
    "     \"lag_12x7\", \"lag_13x7\", \"lag_14x7\", \"lag_15x7\", \"lag_16x7\", \"lag_17x7\"]]\n",
    "rnn_train_ffnn_2 = X_train[\n",
    "    [\"lag_2x8\", \"lag_3x8\", \"lag_4x8\", \"lag_5x8\", \"lag_6x8\", \"lag_7x8\", \"lag_8x8\", \"lag_9x8\", \"lag_10x8\", \"lag_11x8\",\n",
    "     \"lag_12x8\", \"lag_13x8\", \"lag_14x8\", \"lag_15x8\", \"lag_16x8\", \"lag_17x8\"]]\n",
    "rnn_train_ffnn_3 = X_train[\n",
    "    [\"lag_2x9\", \"lag_3x9\", \"lag_4x9\", \"lag_5x9\", \"lag_6x9\", \"lag_7x9\", \"lag_8x9\", \"lag_9x9\", \"lag_10x9\", \"lag_11x9\",\n",
    "     \"lag_12x9\", \"lag_13x9\", \"lag_14x9\", \"lag_15x9\", \"lag_16x9\", \"lag_17x9\"]]\n",
    "rnn_train_ffnn_4 = X_train[\n",
    "    [\"lag_2x10\", \"lag_3x10\", \"lag_4x10\", \"lag_5x10\", \"lag_6x10\", \"lag_7x10\", \"lag_8x10\", \"lag_9x10\", \"lag_10x10\",\n",
    "     \"lag_11x10\", \"lag_12x10\", \"lag_13x10\", \"lag_14x10\", \"lag_15x10\", \"lag_16x10\", \"lag_17x10\"]]\n",
    "rnn_train_ffnn_5 = X_train[\n",
    "    [\"lag_2x11\", \"lag_3x11\", \"lag_4x11\", \"lag_5x11\", \"lag_6x11\", \"lag_7x11\", \"lag_8x11\", \"lag_9x11\", \"lag_10x11\",\n",
    "     \"lag_11x11\", \"lag_12x11\", \"lag_13x11\", \"lag_14x11\", \"lag_15x11\", \"lag_16x11\", \"lag_17x11\"]]\n",
    "\n",
    "rnn_test_ffnn_1 = X_test[\n",
    "    [\"lag_2x7\", \"lag_3x7\", \"lag_4x7\", \"lag_5x7\", \"lag_6x7\", \"lag_7x7\", \"lag_8x7\", \"lag_9x7\", \"lag_10x7\", \"lag_11x7\",\n",
    "     \"lag_12x7\", \"lag_13x7\", \"lag_14x7\", \"lag_15x7\", \"lag_16x7\", \"lag_17x7\"]]\n",
    "rnn_test_ffnn_2 = X_test[\n",
    "    [\"lag_2x8\", \"lag_3x8\", \"lag_4x8\", \"lag_5x8\", \"lag_6x8\", \"lag_7x8\", \"lag_8x8\", \"lag_9x8\", \"lag_10x8\", \"lag_11x8\",\n",
    "     \"lag_12x8\", \"lag_13x8\", \"lag_14x8\", \"lag_15x8\", \"lag_16x8\", \"lag_17x8\"]]\n",
    "rnn_test_ffnn_3 = X_test[\n",
    "    [\"lag_2x9\", \"lag_3x9\", \"lag_4x9\", \"lag_5x9\", \"lag_6x9\", \"lag_7x9\", \"lag_8x9\", \"lag_9x9\", \"lag_10x9\", \"lag_11x9\",\n",
    "     \"lag_12x9\", \"lag_13x9\", \"lag_14x9\", \"lag_15x9\", \"lag_16x9\", \"lag_17x9\"]]\n",
    "rnn_test_ffnn_4 = X_test[\n",
    "    [\"lag_2x10\", \"lag_3x10\", \"lag_4x10\", \"lag_5x10\", \"lag_6x10\", \"lag_7x10\", \"lag_8x10\", \"lag_9x10\", \"lag_10x10\",\n",
    "     \"lag_11x10\", \"lag_12x10\", \"lag_13x10\", \"lag_14x10\", \"lag_15x10\", \"lag_16x10\", \"lag_17x10\"]]\n",
    "rnn_test_ffnn_5 = X_test[\n",
    "    [\"lag_2x11\", \"lag_3x11\", \"lag_4x11\", \"lag_5x11\", \"lag_6x11\", \"lag_7x11\", \"lag_8x11\", \"lag_9x11\", \"lag_10x11\",\n",
    "     \"lag_11x11\", \"lag_12x11\", \"lag_13x11\", \"lag_14x11\", \"lag_15x11\", \"lag_16x11\", \"lag_17x11\"]]\n",
    "\n",
    "rnn_Y = Y_train[\n",
    "    [\"lag_2y\", \"lag_3y\", \"lag_4y\", \"lag_5y\", \"lag_6y\", \"lag_7y\", \"lag_8y\", \"lag_9y\", \"lag_10y\", \"lag_11y\", \"lag_12y\",\n",
    "     \"lag_13y\", \"lag_14y\", \"lag_15y\", \"lag_16y\", \"lag_17y\"]]\n",
    "\n",
    "X_scaler_LSTM_1 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_2 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_3 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_4 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler_ffnn_1 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_2 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_3 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_4 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train_LSTM_1 = X_scaler_LSTM_1.fit_transform(rnn_train_LSTM_1)\n",
    "rnn_scaled_train_LSTM_2 = X_scaler_LSTM_2.fit_transform(rnn_train_LSTM_2)\n",
    "rnn_scaled_train_LSTM_3 = X_scaler_LSTM_3.fit_transform(rnn_train_LSTM_3)\n",
    "rnn_scaled_train_LSTM_4 = X_scaler_LSTM_4.fit_transform(rnn_train_LSTM_4)\n",
    "rnn_scaled_train_LSTM_5 = X_scaler_LSTM_5.fit_transform(rnn_train_LSTM_5)\n",
    "\n",
    "rnn_scaled_train_ffnn_1 = X_scaler_ffnn_1.fit_transform(rnn_train_ffnn_1)\n",
    "rnn_scaled_train_ffnn_2 = X_scaler_ffnn_2.fit_transform(rnn_train_ffnn_2)\n",
    "rnn_scaled_train_ffnn_3 = X_scaler_ffnn_3.fit_transform(rnn_train_ffnn_3)\n",
    "rnn_scaled_train_ffnn_4 = X_scaler_ffnn_4.fit_transform(rnn_train_ffnn_4)\n",
    "rnn_scaled_train_ffnn_5 = X_scaler_ffnn_5.fit_transform(rnn_train_ffnn_5)\n",
    "\n",
    "Y_train_Scaled = Y_scaler.fit_transform(Y_train)\n",
    "Y_test_scaled = Y_scaler.transform(Y_test)\n",
    "\n",
    "X_train_Scaled_LSTM = np.hstack(\n",
    "    (rnn_scaled_train_LSTM_1, rnn_scaled_train_LSTM_2, rnn_scaled_train_LSTM_3,\n",
    "     rnn_scaled_train_LSTM_4, rnn_scaled_train_LSTM_5)\n",
    ").reshape(rnn_train_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "X_train_Scaled_ffnn = np.hstack(\n",
    "    (rnn_scaled_train_ffnn_1, rnn_scaled_train_ffnn_2, rnn_scaled_train_ffnn_3,\n",
    "     rnn_scaled_train_ffnn_4, rnn_scaled_train_ffnn_5)\n",
    ").reshape(rnn_train_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled_LSTM = np.hstack(\n",
    "    (X_scaler_LSTM_1.transform(rnn_test_LSTM_1), X_scaler_LSTM_2.transform(rnn_test_LSTM_2),\n",
    "     X_scaler_LSTM_3.transform(rnn_test_LSTM_3), X_scaler_LSTM_4.transform(rnn_test_LSTM_4),\n",
    "     X_scaler_LSTM_5.transform(rnn_test_LSTM_5))\n",
    ").reshape(rnn_test_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled_ffnn = np.hstack(\n",
    "    (X_scaler_ffnn_1.transform(rnn_test_ffnn_1), X_scaler_ffnn_2.transform(rnn_test_ffnn_2),\n",
    "     X_scaler_ffnn_3.transform(rnn_test_ffnn_3), X_scaler_ffnn_4.transform(rnn_test_ffnn_4),\n",
    "     X_scaler_ffnn_5.transform(rnn_test_ffnn_5))\n",
    ").reshape(rnn_test_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "pp = {'lstm_neurons_0': [16, 32, 64, 128, 192, 256],\n",
    "\n",
    "      'ffnn_neurons_0': [16, 32, 64, 128, 192, 256],\n",
    "\n",
    "      'dense_f_neurons': [16, 32, 64, 128, 192, 256],\n",
    "      'lstm_activation_0': ['relu', 'sigmoid', 'tanh'],\n",
    "\n",
    "      'activation_0': ['relu', 'sigmoid', 'tanh', LeakyReLU],\n",
    "      'activation_1': ['relu', 'sigmoid', 'tanh', LeakyReLU],\n",
    "\n",
    "      'learning_rate': list(np.linspace(0.0001, 0.02, 10)),\n",
    "      'dropout_rate_lstm': list(np.linspace(0.0,0.2, 10)),\n",
    "      'dropout_rate_ffnn': list(np.linspace(0.0,0.2, 10)),\n",
    "      \n",
    "      'batch_size': [4, 8, 16, 32, 48],\n",
    "      'epochs': [300]\n",
    "      }\n",
    "i_shape_lstm = (X_train_Scaled_LSTM.shape[1], X_train_Scaled_LSTM.shape[2])\n",
    "i_shape_ffnn = (X_train_Scaled_ffnn.shape[1], X_train_Scaled_ffnn.shape[2])\n",
    "\n",
    "\n",
    "def create_model(x_train, y_train, x_val, y_val, params):\n",
    "    visible1 = Input(shape=(i_shape_lstm))\n",
    "    dense1 = LSTM(params['lstm_neurons_0'], return_sequences=True, activation=params['lstm_activation_0'], input_shape=i_shape_lstm)(visible1)\n",
    "    dense2 = LSTM(params['lstm_neurons_0'], return_sequences=True, activation=params['lstm_activation_0'], input_shape=i_shape_lstm)(dense1)\n",
    "    do_lstm = Dropout(params['dropout_rate_lstm'])(dense2)\n",
    "    dense3 = LSTM(params['lstm_neurons_0'], return_sequences=True, activation=params['lstm_activation_0'], input_shape=i_shape_lstm)(do_lstm)\n",
    "    flat1 = Flatten()(dense3)\n",
    "\n",
    "    visible2 = Input(shape=(i_shape_ffnn))\n",
    "    dense5 = Dense(params['ffnn_neurons_0'], activation=params['activation_0'])(visible2)\n",
    "    dense6 = Dense(params['ffnn_neurons_0'], activation=params['activation_0'])(dense5)\n",
    "    do_ffnn = Dropout(params['dropout_rate_ffnn'])(dense6)\n",
    "    dense7 = Dense(params['ffnn_neurons_0'], activation=params['activation_0'])(do_ffnn)\n",
    "    flat2 = Flatten()(dense7)\n",
    "\n",
    "    merged = concatenate([flat1, flat2])\n",
    "    dense_f = Dense(params['dense_f_neurons'], activation=params['activation_1'])(merged)\n",
    "    output = Dense(16)(dense_f)\n",
    "    model = Model(inputs=[visible1, visible2], outputs=output)\n",
    "\n",
    "    opt = Adam(lr=params['learning_rate'])\n",
    "    model.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mean_absolute_error'])\n",
    "\n",
    "\n",
    "    out = model.fit(x=x_train, y=y_train, validation_data=[x_val, y_val],\n",
    "                    batch_size=params['batch_size'], epochs=params['epochs'], verbose=2,\n",
    "                    callbacks=[talos.utils.early_stopper(epochs=params['epochs'], mode='moderate',\n",
    "                                                         min_delta=0.001, monitor='mean_absolute_error')])\n",
    "    return out, model\n",
    "\n",
    "\n",
    "h = talos.Scan(x=[X_train_Scaled_LSTM, X_train_Scaled_ffnn], y=Y_train_Scaled, x_val=[X_train_Scaled_LSTM, X_train_Scaled_ffnn], y_val=Y_train_Scaled, params=pp, model=create_model,\n",
    "               experiment_name='MH_DNN_1-3', round_limit=30, print_params=True)\n",
    "h.data.sort_values(by='val_mean_absolute_error', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa02ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi Headed RNN DNN Models Search 60 days Training Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import concat\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from math import floor\n",
    "from math import sqrt\n",
    "from datetime import timedelta as td\n",
    "\n",
    "# tensorflow &keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "from tensorflow.keras.models import load_model, save_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "# from keras.layers import concatenate\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "from keras.activations import *\n",
    "\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import talos\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "# dat = pd.read_csv(\"C:/Users/coconnor/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "# dat = pd.read_csv(\"./data/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True,  date_parser=date_parse)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True,  date_parser=date_parse)\n",
    "# dat = pd.read_csv(\"/home/coconnor/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[5810:8066,:]\n",
    "Y_train=Y.iloc[5810:8066,:]\n",
    "X_test=X.iloc[8066:8739,:]\n",
    "Y_test=Y.iloc[8066:8739,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[10178:12482,:]\n",
    "# Y_train=Y.iloc[10178:12482,:]\n",
    "# X_test=X.iloc[12482:13155,:]\n",
    "# Y_test=Y.iloc[12482:13155,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[14594:16850,:]\n",
    "# Y_train=Y.iloc[14594:16850,:]\n",
    "# X_test=X.iloc[16850:17523,:]\n",
    "# Y_test=Y.iloc[16850:17523,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[19010:21170,:]\n",
    "# Y_train=Y.iloc[19010:21170,:]\n",
    "# X_test=X.iloc[21170:21843,:]\n",
    "# Y_test=Y.iloc[21170:21843,:]\n",
    "\n",
    "rnn_train_LSTM_1 = X_train[\n",
    "    [\"lag_-3x1\", \"lag_-4x1\", \"lag_-5x1\", \"lag_-6x1\", \"lag_-7x1\", \"lag_-8x1\", \"lag_-9x1\", \"lag_-10x1\", \"lag_-11x1\",\n",
    "     \"lag_-12x1\", \"lag_-13x1\", \"lag_-14x1\", \"lag_-15x1\", \"lag_-16x1\", \"lag_-17x1\", \"lag_-18x1\", \"lag_-19x1\",\n",
    "     \"lag_-20x1\", \"lag_-21x1\", \"lag_-22x1\", \"lag_-23x1\", \"lag_-24x1\", \"lag_-25x1\", \"lag_-26x1\", \"lag_-27x1\",\n",
    "     \"lag_-28x1\", \"lag_-29x1\", \"lag_-30x1\", \"lag_-31x1\", \"lag_-32x1\", \"lag_-33x1\", \"lag_-34x1\", \"lag_-35x1\",\n",
    "     \"lag_-36x1\", \"lag_-37x1\", \"lag_-38x1\", \"lag_-39x1\", \"lag_-40x1\", \"lag_-41x1\", \"lag_-42x1\", \"lag_-43x1\",\n",
    "     \"lag_-44x1\", \"lag_-45x1\", \"lag_-46x1\", \"lag_-47x1\", \"lag_-48x1\", \"lag_-49x1\", \"lag_-50x1\"]]\n",
    "rnn_train_LSTM_2 = X_train[\n",
    "    [\"lag_-3x2\", \"lag_-4x2\", \"lag_-5x2\", \"lag_-6x2\", \"lag_-7x2\", \"lag_-8x2\", \"lag_-9x2\", \"lag_-10x2\", \"lag_-11x2\",\n",
    "     \"lag_-12x2\", \"lag_-13x2\", \"lag_-14x2\", \"lag_-15x2\", \"lag_-16x2\", \"lag_-17x2\", \"lag_-18x2\", \"lag_-19x2\",\n",
    "     \"lag_-20x2\", \"lag_-21x2\", \"lag_-22x2\", \"lag_-23x2\", \"lag_-24x2\", \"lag_-25x2\", \"lag_-26x2\", \"lag_-27x2\",\n",
    "     \"lag_-28x2\", \"lag_-29x2\", \"lag_-30x2\", \"lag_-31x2\", \"lag_-32x2\", \"lag_-33x2\", \"lag_-34x2\", \"lag_-35x2\",\n",
    "     \"lag_-36x2\", \"lag_-37x2\", \"lag_-38x2\", \"lag_-39x2\", \"lag_-40x2\", \"lag_-41x2\", \"lag_-42x2\", \"lag_-43x2\",\n",
    "     \"lag_-44x2\", \"lag_-45x2\", \"lag_-46x2\", \"lag_-47x2\", \"lag_-48x2\", \"lag_-49x2\", \"lag_-50x2\"]]\n",
    "rnn_train_LSTM_3 = X_train[\n",
    "    [\"lag_-2x3\", \"lag_-3x3\", \"lag_-4x3\", \"lag_-5x3\", \"lag_-6x3\", \"lag_-7x3\", \"lag_-8x3\", \"lag_-9x3\", \"lag_-10x3\",\n",
    "     \"lag_-11x3\", \"lag_-12x3\", \"lag_-13x3\", \"lag_-14x3\", \"lag_-15x3\", \"lag_-16x3\", \"lag_-17x3\", \"lag_-18x3\",\n",
    "     \"lag_-19x3\", \"lag_-20x3\", \"lag_-21x3\", \"lag_-22x3\", \"lag_-23x3\", \"lag_-24x3\", \"lag_-25x3\", \"lag_-26x3\",\n",
    "     \"lag_-27x3\", \"lag_-28x3\", \"lag_-29x3\", \"lag_-30x3\", \"lag_-31x3\", \"lag_-32x3\", \"lag_-33x3\", \"lag_-34x3\",\n",
    "     \"lag_-35x3\", \"lag_-36x3\", \"lag_-37x3\", \"lag_-38x3\", \"lag_-39x3\", \"lag_-40x3\", \"lag_-41x3\", \"lag_-42x3\",\n",
    "     \"lag_-43x3\", \"lag_-44x3\", \"lag_-45x3\", \"lag_-46x3\", \"lag_-47x3\", \"lag_-48x3\", \"lag_-49x3\"]]\n",
    "rnn_train_LSTM_4 = X_train[\n",
    "    [\"lag_0x6\", \"lag_-1x6\", \"lag_-2x6\", \"lag_-3x6\", \"lag_-4x6\", \"lag_-5x6\", \"lag_-6x6\", \"lag_-7x6\", \"lag_-8x6\",\n",
    "     \"lag_-9x6\", \"lag_-10x6\", \"lag_-11x6\", \"lag_-12x6\", \"lag_-13x6\", \"lag_-14x6\", \"lag_-15x6\", \"lag_-16x6\", \"lag_-17x6\",\n",
    "     \"lag_-18x6\", \"lag_-19x6\", \"lag_-20x6\", \"lag_-21x6\", \"lag_-22x6\", \"lag_-23x6\", \"lag_-24x6\", \"lag_-25x6\",\n",
    "     \"lag_-26x6\", \"lag_-27x6\", \"lag_-28x6\", \"lag_-29x6\", \"lag_-30x6\", \"lag_-31x6\", \"lag_-32x6\", \"lag_-33x6\",\n",
    "     \"lag_-34x6\", \"lag_-35x6\", \"lag_-36x6\", \"lag_-37x6\", \"lag_-38x6\", \"lag_-39x6\", \"lag_-40x6\", \"lag_-41x6\",\n",
    "     \"lag_-42x6\", \"lag_-43x6\", \"lag_-44x6\", \"lag_-45x6\", \"lag_-46x6\", \"lag_-47x6\"]]\n",
    "rnn_train_LSTM_5 = X_train[\n",
    "    [\"lag_-2x12\", \"lag_-3x12\", \"lag_-4x12\", \"lag_-5x12\", \"lag_-6x12\", \"lag_-7x12\", \"lag_-8x12\", \"lag_-9x12\",\n",
    "     \"lag_-10x12\", \"lag_-11x12\", \"lag_-12x12\", \"lag_-13x12\", \"lag_-14x12\", \"lag_-15x12\", \"lag_-16x12\", \"lag_-17x12\",\n",
    "     \"lag_-18x12\", \"lag_-19x12\", \"lag_-20x12\", \"lag_-21x12\", \"lag_-22x12\", \"lag_-23x12\", \"lag_-24x12\", \"lag_-25x12\",\n",
    "     \"lag_-26x12\", \"lag_-27x12\", \"lag_-28x12\", \"lag_-29x12\", \"lag_-30x12\", \"lag_-31x12\", \"lag_-32x12\", \"lag_-33x12\",\n",
    "     \"lag_-34x12\", \"lag_-35x12\", \"lag_-36x12\", \"lag_-37x12\", \"lag_-38x12\", \"lag_-39x12\", \"lag_-40x12\", \"lag_-41x12\",\n",
    "     \"lag_-42x12\", \"lag_-43x12\", \"lag_-44x12\", \"lag_-45x12\", \"lag_-46x12\", \"lag_-47x12\", \"lag_-48x12\", \"lag_-49x12\"]]\n",
    "\n",
    "rnn_test_LSTM_1 = X_test[\n",
    "    [\"lag_-3x1\", \"lag_-4x1\", \"lag_-5x1\", \"lag_-6x1\", \"lag_-7x1\", \"lag_-8x1\", \"lag_-9x1\", \"lag_-10x1\", \"lag_-11x1\",\n",
    "     \"lag_-12x1\", \"lag_-13x1\", \"lag_-14x1\", \"lag_-15x1\", \"lag_-16x1\", \"lag_-17x1\", \"lag_-18x1\", \"lag_-19x1\",\n",
    "     \"lag_-20x1\", \"lag_-21x1\", \"lag_-22x1\", \"lag_-23x1\", \"lag_-24x1\", \"lag_-25x1\", \"lag_-26x1\", \"lag_-27x1\",\n",
    "     \"lag_-28x1\", \"lag_-29x1\", \"lag_-30x1\", \"lag_-31x1\", \"lag_-32x1\", \"lag_-33x1\", \"lag_-34x1\", \"lag_-35x1\",\n",
    "     \"lag_-36x1\", \"lag_-37x1\", \"lag_-38x1\", \"lag_-39x1\", \"lag_-40x1\", \"lag_-41x1\", \"lag_-42x1\", \"lag_-43x1\",\n",
    "     \"lag_-44x1\", \"lag_-45x1\", \"lag_-46x1\", \"lag_-47x1\", \"lag_-48x1\", \"lag_-49x1\", \"lag_-50x1\"]]\n",
    "rnn_test_LSTM_2 = X_test[\n",
    "    [\"lag_-3x2\", \"lag_-4x2\", \"lag_-5x2\", \"lag_-6x2\", \"lag_-7x2\", \"lag_-8x2\", \"lag_-9x2\", \"lag_-10x2\", \"lag_-11x2\",\n",
    "     \"lag_-12x2\", \"lag_-13x2\", \"lag_-14x2\", \"lag_-15x2\", \"lag_-16x2\", \"lag_-17x2\", \"lag_-18x2\", \"lag_-19x2\",\n",
    "     \"lag_-20x2\", \"lag_-21x2\", \"lag_-22x2\", \"lag_-23x2\", \"lag_-24x2\", \"lag_-25x2\", \"lag_-26x2\", \"lag_-27x2\",\n",
    "     \"lag_-28x2\", \"lag_-29x2\", \"lag_-30x2\", \"lag_-31x2\", \"lag_-32x2\", \"lag_-33x2\", \"lag_-34x2\", \"lag_-35x2\",\n",
    "     \"lag_-36x2\", \"lag_-37x2\", \"lag_-38x2\", \"lag_-39x2\", \"lag_-40x2\", \"lag_-41x2\", \"lag_-42x2\", \"lag_-43x2\",\n",
    "     \"lag_-44x2\", \"lag_-45x2\", \"lag_-46x2\", \"lag_-47x2\", \"lag_-48x2\", \"lag_-49x2\", \"lag_-50x2\"]]\n",
    "rnn_test_LSTM_3 = X_test[\n",
    "    [\"lag_-2x3\", \"lag_-3x3\", \"lag_-4x3\", \"lag_-5x3\", \"lag_-6x3\", \"lag_-7x3\", \"lag_-8x3\", \"lag_-9x3\", \"lag_-10x3\",\n",
    "     \"lag_-11x3\", \"lag_-12x3\", \"lag_-13x3\", \"lag_-14x3\", \"lag_-15x3\", \"lag_-16x3\", \"lag_-17x3\", \"lag_-18x3\",\n",
    "     \"lag_-19x3\", \"lag_-20x3\", \"lag_-21x3\", \"lag_-22x3\", \"lag_-23x3\", \"lag_-24x3\", \"lag_-25x3\", \"lag_-26x3\",\n",
    "     \"lag_-27x3\", \"lag_-28x3\", \"lag_-29x3\", \"lag_-30x3\", \"lag_-31x3\", \"lag_-32x3\", \"lag_-33x3\", \"lag_-34x3\",\n",
    "     \"lag_-35x3\", \"lag_-36x3\", \"lag_-37x3\", \"lag_-38x3\", \"lag_-39x3\", \"lag_-40x3\", \"lag_-41x3\", \"lag_-42x3\",\n",
    "     \"lag_-43x3\", \"lag_-44x3\", \"lag_-45x3\", \"lag_-46x3\", \"lag_-47x3\", \"lag_-48x3\", \"lag_-49x3\"]]\n",
    "rnn_test_LSTM_4 = X_test[\n",
    "    [\"lag_0x6\", \"lag_-1x6\", \"lag_-2x6\", \"lag_-3x6\", \"lag_-4x6\", \"lag_-5x6\", \"lag_-6x6\", \"lag_-7x6\", \"lag_-8x6\",\n",
    "     \"lag_-9x6\", \"lag_-10x6\", \"lag_-11x6\", \"lag_-12x6\", \"lag_-13x6\", \"lag_-14x6\", \"lag_-15x6\", \"lag_-16x6\", \"lag_-17x6\",\n",
    "     \"lag_-18x6\", \"lag_-19x6\", \"lag_-20x6\", \"lag_-21x6\", \"lag_-22x6\", \"lag_-23x6\", \"lag_-24x6\", \"lag_-25x6\",\n",
    "     \"lag_-26x6\", \"lag_-27x6\", \"lag_-28x6\", \"lag_-29x6\", \"lag_-30x6\", \"lag_-31x6\", \"lag_-32x6\", \"lag_-33x6\",\n",
    "     \"lag_-34x6\", \"lag_-35x6\", \"lag_-36x6\", \"lag_-37x6\", \"lag_-38x6\", \"lag_-39x6\", \"lag_-40x6\", \"lag_-41x6\",\n",
    "     \"lag_-42x6\", \"lag_-43x6\", \"lag_-44x6\", \"lag_-45x6\", \"lag_-46x6\", \"lag_-47x6\"]]\n",
    "rnn_test_LSTM_5 = X_test[\n",
    "    [\"lag_-2x12\", \"lag_-3x12\", \"lag_-4x12\", \"lag_-5x12\", \"lag_-6x12\", \"lag_-7x12\", \"lag_-8x12\", \"lag_-9x12\",\n",
    "     \"lag_-10x12\", \"lag_-11x12\", \"lag_-12x12\", \"lag_-13x12\", \"lag_-14x12\", \"lag_-15x12\", \"lag_-16x12\", \"lag_-17x12\",\n",
    "     \"lag_-18x12\", \"lag_-19x12\", \"lag_-20x12\", \"lag_-21x12\", \"lag_-22x12\", \"lag_-23x12\", \"lag_-24x12\", \"lag_-25x12\",\n",
    "     \"lag_-26x12\", \"lag_-27x12\", \"lag_-28x12\", \"lag_-29x12\", \"lag_-30x12\", \"lag_-31x12\", \"lag_-32x12\", \"lag_-33x12\",\n",
    "     \"lag_-34x12\", \"lag_-35x12\", \"lag_-36x12\", \"lag_-37x12\", \"lag_-38x12\", \"lag_-39x12\", \"lag_-40x12\", \"lag_-41x12\",\n",
    "     \"lag_-42x12\", \"lag_-43x12\", \"lag_-44x12\", \"lag_-45x12\", \"lag_-46x12\", \"lag_-47x12\", \"lag_-48x12\", \"lag_-49x12\"]]\n",
    "\n",
    "rnn_train_ffnn_1 = X_train[\n",
    "    [\"lag_2x7\", \"lag_3x7\", \"lag_4x7\", \"lag_5x7\", \"lag_6x7\", \"lag_7x7\", \"lag_8x7\", \"lag_9x7\", \"lag_10x7\", \"lag_11x7\",\n",
    "     \"lag_12x7\", \"lag_13x7\", \"lag_14x7\", \"lag_15x7\", \"lag_16x7\", \"lag_17x7\"]]\n",
    "rnn_train_ffnn_2 = X_train[\n",
    "    [\"lag_2x8\", \"lag_3x8\", \"lag_4x8\", \"lag_5x8\", \"lag_6x8\", \"lag_7x8\", \"lag_8x8\", \"lag_9x8\", \"lag_10x8\", \"lag_11x8\",\n",
    "     \"lag_12x8\", \"lag_13x8\", \"lag_14x8\", \"lag_15x8\", \"lag_16x8\", \"lag_17x8\"]]\n",
    "rnn_train_ffnn_3 = X_train[\n",
    "    [\"lag_2x9\", \"lag_3x9\", \"lag_4x9\", \"lag_5x9\", \"lag_6x9\", \"lag_7x9\", \"lag_8x9\", \"lag_9x9\", \"lag_10x9\", \"lag_11x9\",\n",
    "     \"lag_12x9\", \"lag_13x9\", \"lag_14x9\", \"lag_15x9\", \"lag_16x9\", \"lag_17x9\"]]\n",
    "rnn_train_ffnn_4 = X_train[\n",
    "    [\"lag_2x10\", \"lag_3x10\", \"lag_4x10\", \"lag_5x10\", \"lag_6x10\", \"lag_7x10\", \"lag_8x10\", \"lag_9x10\", \"lag_10x10\",\n",
    "     \"lag_11x10\", \"lag_12x10\", \"lag_13x10\", \"lag_14x10\", \"lag_15x10\", \"lag_16x10\", \"lag_17x10\"]]\n",
    "rnn_train_ffnn_5 = X_train[\n",
    "    [\"lag_2x11\", \"lag_3x11\", \"lag_4x11\", \"lag_5x11\", \"lag_6x11\", \"lag_7x11\", \"lag_8x11\", \"lag_9x11\", \"lag_10x11\",\n",
    "     \"lag_11x11\", \"lag_12x11\", \"lag_13x11\", \"lag_14x11\", \"lag_15x11\", \"lag_16x11\", \"lag_17x11\"]]\n",
    "\n",
    "rnn_test_ffnn_1 = X_test[\n",
    "    [\"lag_2x7\", \"lag_3x7\", \"lag_4x7\", \"lag_5x7\", \"lag_6x7\", \"lag_7x7\", \"lag_8x7\", \"lag_9x7\", \"lag_10x7\", \"lag_11x7\",\n",
    "     \"lag_12x7\", \"lag_13x7\", \"lag_14x7\", \"lag_15x7\", \"lag_16x7\", \"lag_17x7\"]]\n",
    "rnn_test_ffnn_2 = X_test[\n",
    "    [\"lag_2x8\", \"lag_3x8\", \"lag_4x8\", \"lag_5x8\", \"lag_6x8\", \"lag_7x8\", \"lag_8x8\", \"lag_9x8\", \"lag_10x8\", \"lag_11x8\",\n",
    "     \"lag_12x8\", \"lag_13x8\", \"lag_14x8\", \"lag_15x8\", \"lag_16x8\", \"lag_17x8\"]]\n",
    "rnn_test_ffnn_3 = X_test[\n",
    "    [\"lag_2x9\", \"lag_3x9\", \"lag_4x9\", \"lag_5x9\", \"lag_6x9\", \"lag_7x9\", \"lag_8x9\", \"lag_9x9\", \"lag_10x9\", \"lag_11x9\",\n",
    "     \"lag_12x9\", \"lag_13x9\", \"lag_14x9\", \"lag_15x9\", \"lag_16x9\", \"lag_17x9\"]]\n",
    "rnn_test_ffnn_4 = X_test[\n",
    "    [\"lag_2x10\", \"lag_3x10\", \"lag_4x10\", \"lag_5x10\", \"lag_6x10\", \"lag_7x10\", \"lag_8x10\", \"lag_9x10\", \"lag_10x10\",\n",
    "     \"lag_11x10\", \"lag_12x10\", \"lag_13x10\", \"lag_14x10\", \"lag_15x10\", \"lag_16x10\", \"lag_17x10\"]]\n",
    "rnn_test_ffnn_5 = X_test[\n",
    "    [\"lag_2x11\", \"lag_3x11\", \"lag_4x11\", \"lag_5x11\", \"lag_6x11\", \"lag_7x11\", \"lag_8x11\", \"lag_9x11\", \"lag_10x11\",\n",
    "     \"lag_11x11\", \"lag_12x11\", \"lag_13x11\", \"lag_14x11\", \"lag_15x11\", \"lag_16x11\", \"lag_17x11\"]]\n",
    "\n",
    "rnn_Y = Y_train[\n",
    "    [\"lag_2y\", \"lag_3y\", \"lag_4y\", \"lag_5y\", \"lag_6y\", \"lag_7y\", \"lag_8y\", \"lag_9y\", \"lag_10y\", \"lag_11y\", \"lag_12y\",\n",
    "     \"lag_13y\", \"lag_14y\", \"lag_15y\", \"lag_16y\", \"lag_17y\"]]\n",
    "\n",
    "X_scaler_LSTM_1 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_2 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_3 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_4 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler_ffnn_1 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_2 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_3 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_4 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train_LSTM_1 = X_scaler_LSTM_1.fit_transform(rnn_train_LSTM_1)\n",
    "rnn_scaled_train_LSTM_2 = X_scaler_LSTM_2.fit_transform(rnn_train_LSTM_2)\n",
    "rnn_scaled_train_LSTM_3 = X_scaler_LSTM_3.fit_transform(rnn_train_LSTM_3)\n",
    "rnn_scaled_train_LSTM_4 = X_scaler_LSTM_4.fit_transform(rnn_train_LSTM_4)\n",
    "rnn_scaled_train_LSTM_5 = X_scaler_LSTM_5.fit_transform(rnn_train_LSTM_5)\n",
    "\n",
    "rnn_scaled_train_ffnn_1 = X_scaler_ffnn_1.fit_transform(rnn_train_ffnn_1)\n",
    "rnn_scaled_train_ffnn_2 = X_scaler_ffnn_2.fit_transform(rnn_train_ffnn_2)\n",
    "rnn_scaled_train_ffnn_3 = X_scaler_ffnn_3.fit_transform(rnn_train_ffnn_3)\n",
    "rnn_scaled_train_ffnn_4 = X_scaler_ffnn_4.fit_transform(rnn_train_ffnn_4)\n",
    "rnn_scaled_train_ffnn_5 = X_scaler_ffnn_5.fit_transform(rnn_train_ffnn_5)\n",
    "\n",
    "Y_train_Scaled = Y_scaler.fit_transform(Y_train)\n",
    "Y_test_scaled = Y_scaler.transform(Y_test)\n",
    "\n",
    "X_train_Scaled_LSTM = np.hstack(\n",
    "    (rnn_scaled_train_LSTM_1, rnn_scaled_train_LSTM_2, rnn_scaled_train_LSTM_3,\n",
    "     rnn_scaled_train_LSTM_4, rnn_scaled_train_LSTM_5)\n",
    ").reshape(rnn_train_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "X_train_Scaled_ffnn = np.hstack(\n",
    "    (rnn_scaled_train_ffnn_1, rnn_scaled_train_ffnn_2, rnn_scaled_train_ffnn_3,\n",
    "     rnn_scaled_train_ffnn_4, rnn_scaled_train_ffnn_5)\n",
    ").reshape(rnn_train_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled_LSTM = np.hstack(\n",
    "    (X_scaler_LSTM_1.transform(rnn_test_LSTM_1), X_scaler_LSTM_2.transform(rnn_test_LSTM_2),\n",
    "     X_scaler_LSTM_3.transform(rnn_test_LSTM_3), X_scaler_LSTM_4.transform(rnn_test_LSTM_4),\n",
    "     X_scaler_LSTM_5.transform(rnn_test_LSTM_5))\n",
    ").reshape(rnn_test_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled_ffnn = np.hstack(\n",
    "    (X_scaler_ffnn_1.transform(rnn_test_ffnn_1), X_scaler_ffnn_2.transform(rnn_test_ffnn_2),\n",
    "     X_scaler_ffnn_3.transform(rnn_test_ffnn_3), X_scaler_ffnn_4.transform(rnn_test_ffnn_4),\n",
    "     X_scaler_ffnn_5.transform(rnn_test_ffnn_5))\n",
    ").reshape(rnn_test_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "pp = {'lstm_neurons_0': [16, 32, 64, 128, 192, 256],\n",
    "\n",
    "      'ffnn_neurons_0': [16, 32, 64, 128, 192, 256],\n",
    "\n",
    "      'dense_f_neurons': [16, 32, 64, 128, 192, 256],\n",
    "      'lstm_activation_0': ['relu', 'sigmoid', 'tanh'],\n",
    "\n",
    "      'activation_0': ['relu', 'sigmoid', 'tanh', LeakyReLU],\n",
    "      'activation_1': ['relu', 'sigmoid', 'tanh', LeakyReLU],\n",
    "\n",
    "      'learning_rate': list(np.linspace(0.0001, 0.02, 10)),\n",
    "      'dropout_rate_lstm': list(np.linspace(0.0,0.2, 10)),\n",
    "      'dropout_rate_ffnn': list(np.linspace(0.0,0.2, 10)),\n",
    "      \n",
    "      'batch_size': [4, 8, 16, 32, 48],\n",
    "      'epochs': [300]\n",
    "      }\n",
    "i_shape_lstm = (X_train_Scaled_LSTM.shape[1], X_train_Scaled_LSTM.shape[2])\n",
    "i_shape_ffnn = (X_train_Scaled_ffnn.shape[1], X_train_Scaled_ffnn.shape[2])\n",
    "\n",
    "\n",
    "def create_model(x_train, y_train, x_val, y_val, params):\n",
    "    visible1 = Input(shape=(i_shape_lstm))\n",
    "    dense1 = LSTM(params['lstm_neurons_0'], return_sequences=True, activation=params['lstm_activation_0'], input_shape=i_shape_lstm)(visible1)\n",
    "    dense2 = LSTM(params['lstm_neurons_0'], return_sequences=True, activation=params['lstm_activation_0'], input_shape=i_shape_lstm)(dense1)\n",
    "    do_lstm = Dropout(params['dropout_rate_lstm'])(dense2)\n",
    "    dense3 = LSTM(params['lstm_neurons_0'], return_sequences=True, activation=params['lstm_activation_0'], input_shape=i_shape_lstm)(do_lstm)\n",
    "    flat1 = Flatten()(dense3)\n",
    "\n",
    "    visible2 = Input(shape=(i_shape_ffnn))\n",
    "    dense5 = Dense(params['ffnn_neurons_0'], activation=params['activation_0'])(visible2)\n",
    "    dense6 = Dense(params['ffnn_neurons_0'], activation=params['activation_0'])(dense5)\n",
    "    do_ffnn = Dropout(params['dropout_rate_ffnn'])(dense6)\n",
    "    dense7 = Dense(params['ffnn_neurons_0'], activation=params['activation_0'])(do_ffnn)\n",
    "    flat2 = Flatten()(dense7)\n",
    "\n",
    "    merged = concatenate([flat1, flat2])\n",
    "    dense_f = Dense(params['dense_f_neurons'], activation=params['activation_1'])(merged)\n",
    "    output = Dense(16)(dense_f)\n",
    "    model = Model(inputs=[visible1, visible2], outputs=output)\n",
    "\n",
    "    opt = Adam(lr=params['learning_rate'])\n",
    "    model.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mean_absolute_error'])\n",
    "\n",
    "\n",
    "    out = model.fit(x=x_train, y=y_train, validation_data=[x_val, y_val],\n",
    "                    batch_size=params['batch_size'], epochs=params['epochs'], verbose=2,\n",
    "                    callbacks=[talos.utils.early_stopper(epochs=params['epochs'], mode='moderate',\n",
    "                                                         min_delta=0.001, monitor='mean_absolute_error')])\n",
    "    return out, model\n",
    "\n",
    "\n",
    "h = talos.Scan(x=[X_train_Scaled_LSTM, X_train_Scaled_ffnn], y=Y_train_Scaled, x_val=[X_train_Scaled_LSTM, X_train_Scaled_ffnn], y_val=Y_train_Scaled, params=pp, model=create_model,\n",
    "               experiment_name='MH_DNN_1-3', round_limit=30, print_params=True)\n",
    "h.data.sort_values(by='val_mean_absolute_error', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc4f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi Headed RNN DNN Models Search 30 days Training Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import concat\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from math import floor\n",
    "from math import sqrt\n",
    "from datetime import timedelta as td\n",
    "\n",
    "# tensorflow &keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "from tensorflow.keras.models import load_model, save_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "# from keras.layers import concatenate\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "from keras.activations import *\n",
    "\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import talos\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True,  date_parser=date_parse)\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[7250:8402,:]\n",
    "Y_train=Y.iloc[7250:8402,:]\n",
    "X_test=X.iloc[8402:8739,:]\n",
    "Y_test=Y.iloc[8402:8739,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[11666:12818,:]\n",
    "# Y_train=Y.iloc[11666:12818,:]\n",
    "# X_test=X.iloc[12818:13155,:]\n",
    "# Y_test=Y.iloc[12818:13155,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[16082:17186,:]\n",
    "# Y_train=Y.iloc[16082:17186,:]\n",
    "# X_test=X.iloc[17186:17523,:]\n",
    "# Y_test=Y.iloc[17186:17523,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[20498:21506,:]\n",
    "# Y_train=Y.iloc[20498:21506,:]\n",
    "# X_test=X.iloc[21506:21843,:]\n",
    "# Y_test=Y.iloc[21506:21843,:]\n",
    "\n",
    "rnn_train_LSTM_1 = X_train[\n",
    "    [\"lag_-3x1\", \"lag_-4x1\", \"lag_-5x1\", \"lag_-6x1\", \"lag_-7x1\", \"lag_-8x1\", \"lag_-9x1\", \"lag_-10x1\", \"lag_-11x1\",\n",
    "     \"lag_-12x1\", \"lag_-13x1\", \"lag_-14x1\", \"lag_-15x1\", \"lag_-16x1\", \"lag_-17x1\", \"lag_-18x1\", \"lag_-19x1\",\n",
    "     \"lag_-20x1\", \"lag_-21x1\", \"lag_-22x1\", \"lag_-23x1\", \"lag_-24x1\", \"lag_-25x1\", \"lag_-26x1\", \"lag_-27x1\",\n",
    "     \"lag_-28x1\", \"lag_-29x1\", \"lag_-30x1\", \"lag_-31x1\", \"lag_-32x1\", \"lag_-33x1\", \"lag_-34x1\", \"lag_-35x1\",\n",
    "     \"lag_-36x1\", \"lag_-37x1\", \"lag_-38x1\", \"lag_-39x1\", \"lag_-40x1\", \"lag_-41x1\", \"lag_-42x1\", \"lag_-43x1\",\n",
    "     \"lag_-44x1\", \"lag_-45x1\", \"lag_-46x1\", \"lag_-47x1\", \"lag_-48x1\", \"lag_-49x1\", \"lag_-50x1\"]]\n",
    "rnn_train_LSTM_2 = X_train[\n",
    "    [\"lag_-3x2\", \"lag_-4x2\", \"lag_-5x2\", \"lag_-6x2\", \"lag_-7x2\", \"lag_-8x2\", \"lag_-9x2\", \"lag_-10x2\", \"lag_-11x2\",\n",
    "     \"lag_-12x2\", \"lag_-13x2\", \"lag_-14x2\", \"lag_-15x2\", \"lag_-16x2\", \"lag_-17x2\", \"lag_-18x2\", \"lag_-19x2\",\n",
    "     \"lag_-20x2\", \"lag_-21x2\", \"lag_-22x2\", \"lag_-23x2\", \"lag_-24x2\", \"lag_-25x2\", \"lag_-26x2\", \"lag_-27x2\",\n",
    "     \"lag_-28x2\", \"lag_-29x2\", \"lag_-30x2\", \"lag_-31x2\", \"lag_-32x2\", \"lag_-33x2\", \"lag_-34x2\", \"lag_-35x2\",\n",
    "     \"lag_-36x2\", \"lag_-37x2\", \"lag_-38x2\", \"lag_-39x2\", \"lag_-40x2\", \"lag_-41x2\", \"lag_-42x2\", \"lag_-43x2\",\n",
    "     \"lag_-44x2\", \"lag_-45x2\", \"lag_-46x2\", \"lag_-47x2\", \"lag_-48x2\", \"lag_-49x2\", \"lag_-50x2\"]]\n",
    "rnn_train_LSTM_3 = X_train[\n",
    "    [\"lag_-2x3\", \"lag_-3x3\", \"lag_-4x3\", \"lag_-5x3\", \"lag_-6x3\", \"lag_-7x3\", \"lag_-8x3\", \"lag_-9x3\", \"lag_-10x3\",\n",
    "     \"lag_-11x3\", \"lag_-12x3\", \"lag_-13x3\", \"lag_-14x3\", \"lag_-15x3\", \"lag_-16x3\", \"lag_-17x3\", \"lag_-18x3\",\n",
    "     \"lag_-19x3\", \"lag_-20x3\", \"lag_-21x3\", \"lag_-22x3\", \"lag_-23x3\", \"lag_-24x3\", \"lag_-25x3\", \"lag_-26x3\",\n",
    "     \"lag_-27x3\", \"lag_-28x3\", \"lag_-29x3\", \"lag_-30x3\", \"lag_-31x3\", \"lag_-32x3\", \"lag_-33x3\", \"lag_-34x3\",\n",
    "     \"lag_-35x3\", \"lag_-36x3\", \"lag_-37x3\", \"lag_-38x3\", \"lag_-39x3\", \"lag_-40x3\", \"lag_-41x3\", \"lag_-42x3\",\n",
    "     \"lag_-43x3\", \"lag_-44x3\", \"lag_-45x3\", \"lag_-46x3\", \"lag_-47x3\", \"lag_-48x3\", \"lag_-49x3\"]]\n",
    "rnn_train_LSTM_4 = X_train[\n",
    "    [\"lag_0x6\", \"lag_-1x6\", \"lag_-2x6\", \"lag_-3x6\", \"lag_-4x6\", \"lag_-5x6\", \"lag_-6x6\", \"lag_-7x6\", \"lag_-8x6\",\n",
    "     \"lag_-9x6\", \"lag_-10x6\", \"lag_-11x6\", \"lag_-12x6\", \"lag_-13x6\", \"lag_-14x6\", \"lag_-15x6\", \"lag_-16x6\", \"lag_-17x6\",\n",
    "     \"lag_-18x6\", \"lag_-19x6\", \"lag_-20x6\", \"lag_-21x6\", \"lag_-22x6\", \"lag_-23x6\", \"lag_-24x6\", \"lag_-25x6\",\n",
    "     \"lag_-26x6\", \"lag_-27x6\", \"lag_-28x6\", \"lag_-29x6\", \"lag_-30x6\", \"lag_-31x6\", \"lag_-32x6\", \"lag_-33x6\",\n",
    "     \"lag_-34x6\", \"lag_-35x6\", \"lag_-36x6\", \"lag_-37x6\", \"lag_-38x6\", \"lag_-39x6\", \"lag_-40x6\", \"lag_-41x6\",\n",
    "     \"lag_-42x6\", \"lag_-43x6\", \"lag_-44x6\", \"lag_-45x6\", \"lag_-46x6\", \"lag_-47x6\"]]\n",
    "rnn_train_LSTM_5 = X_train[\n",
    "    [\"lag_-2x12\", \"lag_-3x12\", \"lag_-4x12\", \"lag_-5x12\", \"lag_-6x12\", \"lag_-7x12\", \"lag_-8x12\", \"lag_-9x12\",\n",
    "     \"lag_-10x12\", \"lag_-11x12\", \"lag_-12x12\", \"lag_-13x12\", \"lag_-14x12\", \"lag_-15x12\", \"lag_-16x12\", \"lag_-17x12\",\n",
    "     \"lag_-18x12\", \"lag_-19x12\", \"lag_-20x12\", \"lag_-21x12\", \"lag_-22x12\", \"lag_-23x12\", \"lag_-24x12\", \"lag_-25x12\",\n",
    "     \"lag_-26x12\", \"lag_-27x12\", \"lag_-28x12\", \"lag_-29x12\", \"lag_-30x12\", \"lag_-31x12\", \"lag_-32x12\", \"lag_-33x12\",\n",
    "     \"lag_-34x12\", \"lag_-35x12\", \"lag_-36x12\", \"lag_-37x12\", \"lag_-38x12\", \"lag_-39x12\", \"lag_-40x12\", \"lag_-41x12\",\n",
    "     \"lag_-42x12\", \"lag_-43x12\", \"lag_-44x12\", \"lag_-45x12\", \"lag_-46x12\", \"lag_-47x12\", \"lag_-48x12\", \"lag_-49x12\"]]\n",
    "\n",
    "rnn_test_LSTM_1 = X_test[\n",
    "    [\"lag_-3x1\", \"lag_-4x1\", \"lag_-5x1\", \"lag_-6x1\", \"lag_-7x1\", \"lag_-8x1\", \"lag_-9x1\", \"lag_-10x1\", \"lag_-11x1\",\n",
    "     \"lag_-12x1\", \"lag_-13x1\", \"lag_-14x1\", \"lag_-15x1\", \"lag_-16x1\", \"lag_-17x1\", \"lag_-18x1\", \"lag_-19x1\",\n",
    "     \"lag_-20x1\", \"lag_-21x1\", \"lag_-22x1\", \"lag_-23x1\", \"lag_-24x1\", \"lag_-25x1\", \"lag_-26x1\", \"lag_-27x1\",\n",
    "     \"lag_-28x1\", \"lag_-29x1\", \"lag_-30x1\", \"lag_-31x1\", \"lag_-32x1\", \"lag_-33x1\", \"lag_-34x1\", \"lag_-35x1\",\n",
    "     \"lag_-36x1\", \"lag_-37x1\", \"lag_-38x1\", \"lag_-39x1\", \"lag_-40x1\", \"lag_-41x1\", \"lag_-42x1\", \"lag_-43x1\",\n",
    "     \"lag_-44x1\", \"lag_-45x1\", \"lag_-46x1\", \"lag_-47x1\", \"lag_-48x1\", \"lag_-49x1\", \"lag_-50x1\"]]\n",
    "rnn_test_LSTM_2 = X_test[\n",
    "    [\"lag_-3x2\", \"lag_-4x2\", \"lag_-5x2\", \"lag_-6x2\", \"lag_-7x2\", \"lag_-8x2\", \"lag_-9x2\", \"lag_-10x2\", \"lag_-11x2\",\n",
    "     \"lag_-12x2\", \"lag_-13x2\", \"lag_-14x2\", \"lag_-15x2\", \"lag_-16x2\", \"lag_-17x2\", \"lag_-18x2\", \"lag_-19x2\",\n",
    "     \"lag_-20x2\", \"lag_-21x2\", \"lag_-22x2\", \"lag_-23x2\", \"lag_-24x2\", \"lag_-25x2\", \"lag_-26x2\", \"lag_-27x2\",\n",
    "     \"lag_-28x2\", \"lag_-29x2\", \"lag_-30x2\", \"lag_-31x2\", \"lag_-32x2\", \"lag_-33x2\", \"lag_-34x2\", \"lag_-35x2\",\n",
    "     \"lag_-36x2\", \"lag_-37x2\", \"lag_-38x2\", \"lag_-39x2\", \"lag_-40x2\", \"lag_-41x2\", \"lag_-42x2\", \"lag_-43x2\",\n",
    "     \"lag_-44x2\", \"lag_-45x2\", \"lag_-46x2\", \"lag_-47x2\", \"lag_-48x2\", \"lag_-49x2\", \"lag_-50x2\"]]\n",
    "rnn_test_LSTM_3 = X_test[\n",
    "    [\"lag_-2x3\", \"lag_-3x3\", \"lag_-4x3\", \"lag_-5x3\", \"lag_-6x3\", \"lag_-7x3\", \"lag_-8x3\", \"lag_-9x3\", \"lag_-10x3\",\n",
    "     \"lag_-11x3\", \"lag_-12x3\", \"lag_-13x3\", \"lag_-14x3\", \"lag_-15x3\", \"lag_-16x3\", \"lag_-17x3\", \"lag_-18x3\",\n",
    "     \"lag_-19x3\", \"lag_-20x3\", \"lag_-21x3\", \"lag_-22x3\", \"lag_-23x3\", \"lag_-24x3\", \"lag_-25x3\", \"lag_-26x3\",\n",
    "     \"lag_-27x3\", \"lag_-28x3\", \"lag_-29x3\", \"lag_-30x3\", \"lag_-31x3\", \"lag_-32x3\", \"lag_-33x3\", \"lag_-34x3\",\n",
    "     \"lag_-35x3\", \"lag_-36x3\", \"lag_-37x3\", \"lag_-38x3\", \"lag_-39x3\", \"lag_-40x3\", \"lag_-41x3\", \"lag_-42x3\",\n",
    "     \"lag_-43x3\", \"lag_-44x3\", \"lag_-45x3\", \"lag_-46x3\", \"lag_-47x3\", \"lag_-48x3\", \"lag_-49x3\"]]\n",
    "rnn_test_LSTM_4 = X_test[\n",
    "    [\"lag_0x6\", \"lag_-1x6\", \"lag_-2x6\", \"lag_-3x6\", \"lag_-4x6\", \"lag_-5x6\", \"lag_-6x6\", \"lag_-7x6\", \"lag_-8x6\",\n",
    "     \"lag_-9x6\", \"lag_-10x6\", \"lag_-11x6\", \"lag_-12x6\", \"lag_-13x6\", \"lag_-14x6\", \"lag_-15x6\", \"lag_-16x6\", \"lag_-17x6\",\n",
    "     \"lag_-18x6\", \"lag_-19x6\", \"lag_-20x6\", \"lag_-21x6\", \"lag_-22x6\", \"lag_-23x6\", \"lag_-24x6\", \"lag_-25x6\",\n",
    "     \"lag_-26x6\", \"lag_-27x6\", \"lag_-28x6\", \"lag_-29x6\", \"lag_-30x6\", \"lag_-31x6\", \"lag_-32x6\", \"lag_-33x6\",\n",
    "     \"lag_-34x6\", \"lag_-35x6\", \"lag_-36x6\", \"lag_-37x6\", \"lag_-38x6\", \"lag_-39x6\", \"lag_-40x6\", \"lag_-41x6\",\n",
    "     \"lag_-42x6\", \"lag_-43x6\", \"lag_-44x6\", \"lag_-45x6\", \"lag_-46x6\", \"lag_-47x6\"]]\n",
    "rnn_test_LSTM_5 = X_test[\n",
    "    [\"lag_-2x12\", \"lag_-3x12\", \"lag_-4x12\", \"lag_-5x12\", \"lag_-6x12\", \"lag_-7x12\", \"lag_-8x12\", \"lag_-9x12\",\n",
    "     \"lag_-10x12\", \"lag_-11x12\", \"lag_-12x12\", \"lag_-13x12\", \"lag_-14x12\", \"lag_-15x12\", \"lag_-16x12\", \"lag_-17x12\",\n",
    "     \"lag_-18x12\", \"lag_-19x12\", \"lag_-20x12\", \"lag_-21x12\", \"lag_-22x12\", \"lag_-23x12\", \"lag_-24x12\", \"lag_-25x12\",\n",
    "     \"lag_-26x12\", \"lag_-27x12\", \"lag_-28x12\", \"lag_-29x12\", \"lag_-30x12\", \"lag_-31x12\", \"lag_-32x12\", \"lag_-33x12\",\n",
    "     \"lag_-34x12\", \"lag_-35x12\", \"lag_-36x12\", \"lag_-37x12\", \"lag_-38x12\", \"lag_-39x12\", \"lag_-40x12\", \"lag_-41x12\",\n",
    "     \"lag_-42x12\", \"lag_-43x12\", \"lag_-44x12\", \"lag_-45x12\", \"lag_-46x12\", \"lag_-47x12\", \"lag_-48x12\", \"lag_-49x12\"]]\n",
    "\n",
    "rnn_train_ffnn_1 = X_train[\n",
    "    [\"lag_2x7\", \"lag_3x7\", \"lag_4x7\", \"lag_5x7\", \"lag_6x7\", \"lag_7x7\", \"lag_8x7\", \"lag_9x7\", \"lag_10x7\", \"lag_11x7\",\n",
    "     \"lag_12x7\", \"lag_13x7\", \"lag_14x7\", \"lag_15x7\", \"lag_16x7\", \"lag_17x7\"]]\n",
    "rnn_train_ffnn_2 = X_train[\n",
    "    [\"lag_2x8\", \"lag_3x8\", \"lag_4x8\", \"lag_5x8\", \"lag_6x8\", \"lag_7x8\", \"lag_8x8\", \"lag_9x8\", \"lag_10x8\", \"lag_11x8\",\n",
    "     \"lag_12x8\", \"lag_13x8\", \"lag_14x8\", \"lag_15x8\", \"lag_16x8\", \"lag_17x8\"]]\n",
    "rnn_train_ffnn_3 = X_train[\n",
    "    [\"lag_2x9\", \"lag_3x9\", \"lag_4x9\", \"lag_5x9\", \"lag_6x9\", \"lag_7x9\", \"lag_8x9\", \"lag_9x9\", \"lag_10x9\", \"lag_11x9\",\n",
    "     \"lag_12x9\", \"lag_13x9\", \"lag_14x9\", \"lag_15x9\", \"lag_16x9\", \"lag_17x9\"]]\n",
    "rnn_train_ffnn_4 = X_train[\n",
    "    [\"lag_2x10\", \"lag_3x10\", \"lag_4x10\", \"lag_5x10\", \"lag_6x10\", \"lag_7x10\", \"lag_8x10\", \"lag_9x10\", \"lag_10x10\",\n",
    "     \"lag_11x10\", \"lag_12x10\", \"lag_13x10\", \"lag_14x10\", \"lag_15x10\", \"lag_16x10\", \"lag_17x10\"]]\n",
    "rnn_train_ffnn_5 = X_train[\n",
    "    [\"lag_2x11\", \"lag_3x11\", \"lag_4x11\", \"lag_5x11\", \"lag_6x11\", \"lag_7x11\", \"lag_8x11\", \"lag_9x11\", \"lag_10x11\",\n",
    "     \"lag_11x11\", \"lag_12x11\", \"lag_13x11\", \"lag_14x11\", \"lag_15x11\", \"lag_16x11\", \"lag_17x11\"]]\n",
    "\n",
    "rnn_test_ffnn_1 = X_test[\n",
    "    [\"lag_2x7\", \"lag_3x7\", \"lag_4x7\", \"lag_5x7\", \"lag_6x7\", \"lag_7x7\", \"lag_8x7\", \"lag_9x7\", \"lag_10x7\", \"lag_11x7\",\n",
    "     \"lag_12x7\", \"lag_13x7\", \"lag_14x7\", \"lag_15x7\", \"lag_16x7\", \"lag_17x7\"]]\n",
    "rnn_test_ffnn_2 = X_test[\n",
    "    [\"lag_2x8\", \"lag_3x8\", \"lag_4x8\", \"lag_5x8\", \"lag_6x8\", \"lag_7x8\", \"lag_8x8\", \"lag_9x8\", \"lag_10x8\", \"lag_11x8\",\n",
    "     \"lag_12x8\", \"lag_13x8\", \"lag_14x8\", \"lag_15x8\", \"lag_16x8\", \"lag_17x8\"]]\n",
    "rnn_test_ffnn_3 = X_test[\n",
    "    [\"lag_2x9\", \"lag_3x9\", \"lag_4x9\", \"lag_5x9\", \"lag_6x9\", \"lag_7x9\", \"lag_8x9\", \"lag_9x9\", \"lag_10x9\", \"lag_11x9\",\n",
    "     \"lag_12x9\", \"lag_13x9\", \"lag_14x9\", \"lag_15x9\", \"lag_16x9\", \"lag_17x9\"]]\n",
    "rnn_test_ffnn_4 = X_test[\n",
    "    [\"lag_2x10\", \"lag_3x10\", \"lag_4x10\", \"lag_5x10\", \"lag_6x10\", \"lag_7x10\", \"lag_8x10\", \"lag_9x10\", \"lag_10x10\",\n",
    "     \"lag_11x10\", \"lag_12x10\", \"lag_13x10\", \"lag_14x10\", \"lag_15x10\", \"lag_16x10\", \"lag_17x10\"]]\n",
    "rnn_test_ffnn_5 = X_test[\n",
    "    [\"lag_2x11\", \"lag_3x11\", \"lag_4x11\", \"lag_5x11\", \"lag_6x11\", \"lag_7x11\", \"lag_8x11\", \"lag_9x11\", \"lag_10x11\",\n",
    "     \"lag_11x11\", \"lag_12x11\", \"lag_13x11\", \"lag_14x11\", \"lag_15x11\", \"lag_16x11\", \"lag_17x11\"]]\n",
    "\n",
    "rnn_Y = Y_train[\n",
    "    [\"lag_2y\", \"lag_3y\", \"lag_4y\", \"lag_5y\", \"lag_6y\", \"lag_7y\", \"lag_8y\", \"lag_9y\", \"lag_10y\", \"lag_11y\", \"lag_12y\",\n",
    "     \"lag_13y\", \"lag_14y\", \"lag_15y\", \"lag_16y\", \"lag_17y\"]]\n",
    "\n",
    "X_scaler_LSTM_1 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_2 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_3 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_4 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler_ffnn_1 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_2 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_3 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_4 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train_LSTM_1 = X_scaler_LSTM_1.fit_transform(rnn_train_LSTM_1)\n",
    "rnn_scaled_train_LSTM_2 = X_scaler_LSTM_2.fit_transform(rnn_train_LSTM_2)\n",
    "rnn_scaled_train_LSTM_3 = X_scaler_LSTM_3.fit_transform(rnn_train_LSTM_3)\n",
    "rnn_scaled_train_LSTM_4 = X_scaler_LSTM_4.fit_transform(rnn_train_LSTM_4)\n",
    "rnn_scaled_train_LSTM_5 = X_scaler_LSTM_5.fit_transform(rnn_train_LSTM_5)\n",
    "\n",
    "rnn_scaled_train_ffnn_1 = X_scaler_ffnn_1.fit_transform(rnn_train_ffnn_1)\n",
    "rnn_scaled_train_ffnn_2 = X_scaler_ffnn_2.fit_transform(rnn_train_ffnn_2)\n",
    "rnn_scaled_train_ffnn_3 = X_scaler_ffnn_3.fit_transform(rnn_train_ffnn_3)\n",
    "rnn_scaled_train_ffnn_4 = X_scaler_ffnn_4.fit_transform(rnn_train_ffnn_4)\n",
    "rnn_scaled_train_ffnn_5 = X_scaler_ffnn_5.fit_transform(rnn_train_ffnn_5)\n",
    "\n",
    "Y_train_Scaled = Y_scaler.fit_transform(Y_train)\n",
    "Y_test_scaled = Y_scaler.transform(Y_test)\n",
    "\n",
    "X_train_Scaled_LSTM = np.hstack(\n",
    "    (rnn_scaled_train_LSTM_1, rnn_scaled_train_LSTM_2, rnn_scaled_train_LSTM_3,\n",
    "     rnn_scaled_train_LSTM_4, rnn_scaled_train_LSTM_5)\n",
    ").reshape(rnn_train_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "X_train_Scaled_ffnn = np.hstack(\n",
    "    (rnn_scaled_train_ffnn_1, rnn_scaled_train_ffnn_2, rnn_scaled_train_ffnn_3,\n",
    "     rnn_scaled_train_ffnn_4, rnn_scaled_train_ffnn_5)\n",
    ").reshape(rnn_train_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled_LSTM = np.hstack(\n",
    "    (X_scaler_LSTM_1.transform(rnn_test_LSTM_1), X_scaler_LSTM_2.transform(rnn_test_LSTM_2),\n",
    "     X_scaler_LSTM_3.transform(rnn_test_LSTM_3), X_scaler_LSTM_4.transform(rnn_test_LSTM_4),\n",
    "     X_scaler_LSTM_5.transform(rnn_test_LSTM_5))\n",
    ").reshape(rnn_test_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled_ffnn = np.hstack(\n",
    "    (X_scaler_ffnn_1.transform(rnn_test_ffnn_1), X_scaler_ffnn_2.transform(rnn_test_ffnn_2),\n",
    "     X_scaler_ffnn_3.transform(rnn_test_ffnn_3), X_scaler_ffnn_4.transform(rnn_test_ffnn_4),\n",
    "     X_scaler_ffnn_5.transform(rnn_test_ffnn_5))\n",
    ").reshape(rnn_test_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "pp = {'lstm_neurons_0': [16, 32, 64, 128, 192, 256],\n",
    "\n",
    "      'ffnn_neurons_0': [16, 32, 64, 128, 192, 256],\n",
    "\n",
    "      'dense_f_neurons': [16, 32, 64, 128, 192, 256],\n",
    "      'lstm_activation_0': ['relu', 'sigmoid', 'tanh'],\n",
    "\n",
    "      'activation_0': ['relu', 'sigmoid', 'tanh', LeakyReLU],\n",
    "      'activation_1': ['relu', 'sigmoid', 'tanh', LeakyReLU],\n",
    "\n",
    "      'learning_rate': list(np.linspace(0.0001, 0.02, 10)),\n",
    "      'dropout_rate_lstm': list(np.linspace(0.0,0.2, 10)),\n",
    "      'dropout_rate_ffnn': list(np.linspace(0.0,0.2, 10)),\n",
    "      \n",
    "      'batch_size': [4, 8, 16, 32, 48],\n",
    "      'epochs': [300]\n",
    "      }\n",
    "i_shape_lstm = (X_train_Scaled_LSTM.shape[1], X_train_Scaled_LSTM.shape[2])\n",
    "i_shape_ffnn = (X_train_Scaled_ffnn.shape[1], X_train_Scaled_ffnn.shape[2])\n",
    "\n",
    "\n",
    "def create_model(x_train, y_train, x_val, y_val, params):\n",
    "    visible1 = Input(shape=(i_shape_lstm))\n",
    "    dense1 = LSTM(params['lstm_neurons_0'], return_sequences=True, activation=params['lstm_activation_0'], input_shape=i_shape_lstm)(visible1)\n",
    "    dense2 = LSTM(params['lstm_neurons_0'], return_sequences=True, activation=params['lstm_activation_0'], input_shape=i_shape_lstm)(dense1)\n",
    "    do_lstm = Dropout(params['dropout_rate_lstm'])(dense2)\n",
    "    dense3 = LSTM(params['lstm_neurons_0'], return_sequences=True, activation=params['lstm_activation_0'], input_shape=i_shape_lstm)(do_lstm)\n",
    "    flat1 = Flatten()(dense3)\n",
    "\n",
    "    visible2 = Input(shape=(i_shape_ffnn))\n",
    "    dense5 = Dense(params['ffnn_neurons_0'], activation=params['activation_0'])(visible2)\n",
    "    dense6 = Dense(params['ffnn_neurons_0'], activation=params['activation_0'])(dense5)\n",
    "    do_ffnn = Dropout(params['dropout_rate_ffnn'])(dense6)\n",
    "    dense7 = Dense(params['ffnn_neurons_0'], activation=params['activation_0'])(do_ffnn)\n",
    "    flat2 = Flatten()(dense7)\n",
    "\n",
    "    merged = concatenate([flat1, flat2])\n",
    "    dense_f = Dense(params['dense_f_neurons'], activation=params['activation_1'])(merged)\n",
    "    output = Dense(16)(dense_f)\n",
    "    model = Model(inputs=[visible1, visible2], outputs=output)\n",
    "\n",
    "    opt = Adam(lr=params['learning_rate'])\n",
    "    model.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mean_absolute_error'])\n",
    "\n",
    "\n",
    "    out = model.fit(x=x_train, y=y_train, validation_data=[x_val, y_val],\n",
    "                    batch_size=params['batch_size'], epochs=params['epochs'], verbose=2,\n",
    "                    callbacks=[talos.utils.early_stopper(epochs=params['epochs'], mode='moderate',\n",
    "                                                         min_delta=0.001, monitor='mean_absolute_error')])\n",
    "    return out, model\n",
    "\n",
    "\n",
    "h = talos.Scan(x=[X_train_Scaled_LSTM, X_train_Scaled_ffnn], y=Y_train_Scaled, x_val=[X_train_Scaled_LSTM, X_train_Scaled_ffnn], y_val=Y_train_Scaled, params=pp, model=create_model,\n",
    "               experiment_name='MH_DNN_1-3', round_limit=30, print_params=True)\n",
    "h.data.sort_values(by='val_mean_absolute_error', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5328e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc222e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e347e537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347c3e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651cc139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e1c00a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95c4154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2fa3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc213b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af559cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ca9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ee6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb03d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93230bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb020736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd09892d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e06175a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce02bbec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c360f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e3295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba82a9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d49aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf07f802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da074f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a9e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716530db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a04b93b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eff13f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54988d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7665e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6a2140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ddc958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebacfd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814992b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64738c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f18c338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c5ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d70824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e914c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72533d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c1fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2506d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6073e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cfc929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9fb70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c38d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaed3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0280df44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c7120d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c52c975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adb95fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45388c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99527bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c75ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eec8350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f15e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc9ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3827b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51912949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91285fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3daf1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2451bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca068558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d60ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78b7e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20941191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb609ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa791323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf8249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec098c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f6b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d806ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e2975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2b63b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d2ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e52043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60420f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec49924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e229dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c852712a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8257c488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc423f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dc3b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d456daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70f186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d0abd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b8d920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d543db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2f273d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac961ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec28da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36dde85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9be2aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b4521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5651cb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2908fd58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c274ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea84634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2aaf94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451ef62e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8681620c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf2a625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eca213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeff9732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab2e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616262b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29aaf41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d72ada8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c15f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d278a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603c526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac10584d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a9e97a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6780384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce30de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a42e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c403fd6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c07f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66475eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9dd3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732fe759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c7788d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac60fc25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb86d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d2f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db74b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90df06a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f1f15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b86b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51900e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
