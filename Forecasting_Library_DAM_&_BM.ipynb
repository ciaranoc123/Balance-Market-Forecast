{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7887c23b",
   "metadata": {},
   "source": [
    "Found below are the libraries for generating quantile and regular forecasts in both the balancing and day ahead market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2214fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c630ee31",
   "metadata": {},
   "source": [
    "Multi-Headed, Multi-Input Deep neural network Recurrent neural network for quantile forecast in the BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d92df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# months 1-7 search\n",
    "import os ;\n",
    "# os.environ['HDF5_DISABLE_VERSION_CHECK']='2'\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt \n",
    "from datetime import datetime\n",
    "from datetime import timedelta as td\n",
    "import traceback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import concat\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "from math import floor\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from hyperopt import hp, fmin, tpe\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.activations import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/coconnor/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[:7250,:]\n",
    "Y_train=Y.iloc[:7250,:]\n",
    "X_test=X.iloc[7250:8739,:]\n",
    "Y_test=Y.iloc[7250:8739,:]\n",
    "\n",
    "def qloss(qs, y_true, y_pred):\n",
    "    # Pinball loss for multiple quantiles\n",
    "    q = tf.constant(np.array([qs]), dtype=tf.float32)\n",
    "    e = y_true - y_pred\n",
    "    v = tf.maximum(q*e, (q-1)*e)\n",
    "    return K.mean(v)\n",
    "\n",
    "loss_10th_p = lambda y_true, y_pred: qloss(0.1, y_true, y_pred)\n",
    "loss_30th_p = lambda y_true, y_pred: qloss(0.3, y_true, y_pred)\n",
    "loss_50th_p = lambda y_true, y_pred: qloss(0.5, y_true, y_pred)\n",
    "loss_70th_p = lambda y_true, y_pred: qloss(0.7, y_true, y_pred)\n",
    "loss_90th_p = lambda y_true, y_pred: qloss(0.9, y_true, y_pred)\n",
    "\n",
    "\n",
    "rnn_train_LSTM_1 = X_train.loc[:,\"lag_-3x1\":\"lag_-50x1\"]\n",
    "rnn_train_LSTM_2 = X_train.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "rnn_train_LSTM_3 = X_train.loc[:,\"lag_-2x3\":\"lag_-49x3\"]\n",
    "rnn_train_LSTM_4 = X_train.loc[:,\"lag_0x6\":\"lag_-47x6\"]\n",
    "rnn_train_LSTM_5 = X_train.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "\n",
    "rnn_test_LSTM_1 = X_test.loc[:,\"lag_-3x1\": \"lag_-50x1\"]\n",
    "rnn_test_LSTM_2 = X_test.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "rnn_test_LSTM_3 = X_test.loc[:,\"lag_-2x3\": \"lag_-49x3\"]\n",
    "rnn_test_LSTM_4 = X_test.loc[:,\"lag_0x6\" : \"lag_-47x6\"]\n",
    "rnn_test_LSTM_5 = X_test.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "\n",
    "rnn_train_ffnn_1 = X_train.loc[:,\"lag_2x7\": \"lag_17x7\"]\n",
    "rnn_train_ffnn_2 = X_train.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "rnn_train_ffnn_3 = X_train.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "rnn_train_ffnn_4 = X_train.loc[:,\"lag_2x10\": \"lag_17x10\"]\n",
    "rnn_train_ffnn_5 = X_train.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "\n",
    "rnn_test_ffnn_1 = X_test.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "rnn_test_ffnn_2 = X_test.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "rnn_test_ffnn_3 = X_test.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "rnn_test_ffnn_4 = X_test.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "rnn_test_ffnn_5 = X_test.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "\n",
    "rnn_Y = Y_train.loc[:,\"lag_2y\": \"lag_17y\"]\n",
    "\n",
    "X_scaler_LSTM_1 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_2 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_3 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_4 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler_ffnn_1 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_2 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_3 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_4 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train_LSTM_1 = X_scaler_LSTM_1.fit_transform(rnn_train_LSTM_1)\n",
    "rnn_scaled_train_LSTM_2 = X_scaler_LSTM_2.fit_transform(rnn_train_LSTM_2)\n",
    "rnn_scaled_train_LSTM_3 = X_scaler_LSTM_3.fit_transform(rnn_train_LSTM_3)\n",
    "rnn_scaled_train_LSTM_4 = X_scaler_LSTM_4.fit_transform(rnn_train_LSTM_4)\n",
    "rnn_scaled_train_LSTM_5 = X_scaler_LSTM_5.fit_transform(rnn_train_LSTM_5)\n",
    "\n",
    "rnn_scaled_train_ffnn_1 = X_scaler_ffnn_1.fit_transform(rnn_train_ffnn_1)\n",
    "rnn_scaled_train_ffnn_2 = X_scaler_ffnn_2.fit_transform(rnn_train_ffnn_2)\n",
    "rnn_scaled_train_ffnn_3 = X_scaler_ffnn_3.fit_transform(rnn_train_ffnn_3)\n",
    "rnn_scaled_train_ffnn_4 = X_scaler_ffnn_4.fit_transform(rnn_train_ffnn_4)\n",
    "rnn_scaled_train_ffnn_5 = X_scaler_ffnn_5.fit_transform(rnn_train_ffnn_5)\n",
    "\n",
    "Y_train_Scaled = Y_scaler.fit_transform(Y_train)\n",
    "Y_test_scaled = Y_scaler.transform(Y_test)\n",
    "\n",
    "X_train_Scaled_LSTM = np.hstack(\n",
    "    (rnn_scaled_train_LSTM_1, rnn_scaled_train_LSTM_2, rnn_scaled_train_LSTM_3,\n",
    "     rnn_scaled_train_LSTM_4, rnn_scaled_train_LSTM_5)\n",
    ").reshape(rnn_train_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "X_train_Scaled_ffnn = np.hstack(\n",
    "    (rnn_scaled_train_ffnn_1, rnn_scaled_train_ffnn_2, rnn_scaled_train_ffnn_3,\n",
    "     rnn_scaled_train_ffnn_4, rnn_scaled_train_ffnn_5)\n",
    ").reshape(rnn_train_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled_LSTM = np.hstack(\n",
    "    (X_scaler_LSTM_1.transform(rnn_test_LSTM_1), X_scaler_LSTM_2.transform(rnn_test_LSTM_2),\n",
    "     X_scaler_LSTM_3.transform(rnn_test_LSTM_3), X_scaler_LSTM_4.transform(rnn_test_LSTM_4),\n",
    "     X_scaler_LSTM_5.transform(rnn_test_LSTM_5))\n",
    ").reshape(rnn_test_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled_ffnn = np.hstack(\n",
    "    (X_scaler_ffnn_1.transform(rnn_test_ffnn_1), X_scaler_ffnn_2.transform(rnn_test_ffnn_2),\n",
    "     X_scaler_ffnn_3.transform(rnn_test_ffnn_3), X_scaler_ffnn_4.transform(rnn_test_ffnn_4),\n",
    "     X_scaler_ffnn_5.transform(rnn_test_ffnn_5))\n",
    ").reshape(rnn_test_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "i_shape_lstm = (X_train_Scaled_LSTM.shape[1], X_train_Scaled_LSTM.shape[2])\n",
    "i_shape_ffnn = (X_train_Scaled_ffnn.shape[1], X_train_Scaled_ffnn.shape[2])\n",
    "\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                                       test_start_time: dt, test_end_time: dt):\n",
    "\n",
    "    # These are the dataframes that will be returned from the method.\n",
    "    train_X_LSTM = None\n",
    "    train_X_ffnn = None\n",
    "\n",
    "    train_y = None\n",
    "    test_X_LSTM = None\n",
    "    test_X_ffnn = None\n",
    "    test_y = None\n",
    "    test_df = None\n",
    "    train_df = None\n",
    "\n",
    "    try:\n",
    "\n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "        #             return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        # Remove any rows with nan's etc (there shouldn't be any in the input).\n",
    "        participant_df = participant_df.dropna()\n",
    "\n",
    "        date_format = \"%m/%d/%Y %H:%M\"\n",
    "\n",
    "        # The train dataframe, it will be used later to create train_X and train_y.\n",
    "        train_df = None\n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)\n",
    "        train_df = participant_df[\n",
    "            (participant_df.index >= train_start_time_str) & (participant_df.index < train_end_time_str)].copy(\n",
    "            deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\n",
    "                \"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")\n",
    "        #             return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        # Create the test dataframe, it will be used later to create test_X and test_y\n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)\n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format)\n",
    "        test_df = participant_df[\n",
    "            (participant_df.index >= test_start_time_str) & (participant_df.index < test_end_time_str)].copy(\n",
    "            deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\n",
    "                \"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")\n",
    "        #             return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        rnn_train_LSTM_1 = train_df.loc[:,\"lag_-3x1\":\"lag_-50x1\"]\n",
    "        rnn_train_LSTM_2 = train_df.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "        rnn_train_LSTM_3 = train_df.loc[:,\"lag_-2x3\":\"lag_-49x3\"]\n",
    "        rnn_train_LSTM_4 = train_df.loc[:,\"lag_0x6\":\"lag_-47x6\"]\n",
    "        rnn_train_LSTM_5 = train_df.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "\n",
    "        rnn_test_LSTM_1 = test_df.loc[:,\"lag_-3x1\": \"lag_-50x1\"]\n",
    "        rnn_test_LSTM_2 = test_df.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "        rnn_test_LSTM_3 = test_df.loc[:,\"lag_-2x3\": \"lag_-49x3\"]\n",
    "        rnn_test_LSTM_4 = test_df.loc[:,\"lag_0x6\" : \"lag_-47x6\"]\n",
    "        rnn_test_LSTM_5 = test_df.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "        \n",
    "        rnn_train_ffnn_1 = train_df.loc[:,\"lag_2x7\": \"lag_17x7\"]\n",
    "        rnn_train_ffnn_2 = train_df.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "        rnn_train_ffnn_3 = train_df.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "        rnn_train_ffnn_4 = train_df.loc[:,\"lag_2x10\": \"lag_17x10\"]\n",
    "        rnn_train_ffnn_5 = train_df.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "        \n",
    "        rnn_test_ffnn_1 = test_df.loc[:,\"lag_2x7\": \"lag_17x7\"]\n",
    "        rnn_test_ffnn_2 = test_df.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "        rnn_test_ffnn_3 = test_df.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "        rnn_test_ffnn_4 = test_df.loc[:,\"lag_2x10\": \"lag_17x10\"]\n",
    "        rnn_test_ffnn_5 = test_df.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "\n",
    "        rnn_Y = train_df.loc[:,\"lag_2y\": \"lag_17y\"]\n",
    "\n",
    "        X_scaler_LSTM_1 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_LSTM_2 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_LSTM_3 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_LSTM_4 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_LSTM_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "        X_scaler_ffnn_1 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_ffnn_2 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_ffnn_3 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_ffnn_4 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_ffnn_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "        rnn_scaled_train_LSTM_1 = X_scaler_LSTM_1.fit_transform(rnn_train_LSTM_1)\n",
    "        rnn_scaled_train_LSTM_2 = X_scaler_LSTM_2.fit_transform(rnn_train_LSTM_2)\n",
    "        rnn_scaled_train_LSTM_3 = X_scaler_LSTM_3.fit_transform(rnn_train_LSTM_3)\n",
    "        rnn_scaled_train_LSTM_4 = X_scaler_LSTM_4.fit_transform(rnn_train_LSTM_4)\n",
    "        rnn_scaled_train_LSTM_5 = X_scaler_LSTM_5.fit_transform(rnn_train_LSTM_5)\n",
    "\n",
    "        rnn_scaled_train_ffnn_1 = X_scaler_ffnn_1.fit_transform(rnn_train_ffnn_1)\n",
    "        rnn_scaled_train_ffnn_2 = X_scaler_ffnn_2.fit_transform(rnn_train_ffnn_2)\n",
    "        rnn_scaled_train_ffnn_3 = X_scaler_ffnn_3.fit_transform(rnn_train_ffnn_3)\n",
    "        rnn_scaled_train_ffnn_4 = X_scaler_ffnn_4.fit_transform(rnn_train_ffnn_4)\n",
    "        rnn_scaled_train_ffnn_5 = X_scaler_ffnn_5.fit_transform(rnn_train_ffnn_5)\n",
    "\n",
    "        train_y = Y_scaler.fit_transform(rnn_Y)\n",
    "        Y_scaler_n = Y_scaler.fit(rnn_Y)\n",
    "\n",
    "        train_X_LSTM = np.hstack(\n",
    "            (rnn_scaled_train_LSTM_1, rnn_scaled_train_LSTM_2, rnn_scaled_train_LSTM_3,\n",
    "             rnn_scaled_train_LSTM_4, rnn_scaled_train_LSTM_5)\n",
    "        ).reshape(rnn_train_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "        train_X_ffnn = np.hstack(\n",
    "            (rnn_scaled_train_ffnn_1, rnn_scaled_train_ffnn_2, rnn_scaled_train_ffnn_3,\n",
    "             rnn_scaled_train_ffnn_4, rnn_scaled_train_ffnn_5)\n",
    "        ).reshape(rnn_train_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "        test_X_LSTM = np.hstack(\n",
    "            (X_scaler_LSTM_1.transform(rnn_test_LSTM_1), X_scaler_LSTM_2.transform(rnn_test_LSTM_2),\n",
    "             X_scaler_LSTM_3.transform(rnn_test_LSTM_3), X_scaler_LSTM_4.transform(rnn_test_LSTM_4),\n",
    "             X_scaler_LSTM_5.transform(rnn_test_LSTM_5))\n",
    "        ).reshape(rnn_test_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "        test_X_ffnn = np.hstack(\n",
    "            (X_scaler_ffnn_1.transform(rnn_test_ffnn_1), X_scaler_ffnn_2.transform(rnn_test_ffnn_2),\n",
    "             X_scaler_ffnn_3.transform(rnn_test_ffnn_3), X_scaler_ffnn_4.transform(rnn_test_ffnn_4),\n",
    "             X_scaler_ffnn_5.transform(rnn_test_ffnn_5))\n",
    "        ).reshape(rnn_test_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "        test_y = test_df.iloc[:, 0:16]\n",
    "\n",
    "        return train_X_LSTM, train_X_ffnn, train_y, test_X_LSTM, test_X_ffnn, test_y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X_LSTM, train_X_ffnn, train_y, test_X_LSTM, test_X_ffnn, test_y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "\n",
    "def fit_multitarget_model(model, X_train_LSTM, X_train_ffnn, Y_train, X_test_LSTM, X_test_ffnn, Y_test,\n",
    "                          actuals_and_forecast_df, targets, Y_scaler_n):\n",
    "    \"\"\"\n",
    "    Fits the model to the training data.\n",
    "    Then uses the test data to produce a forecast.\n",
    "    Returns a dataframe containing the forecasts and actual values for each of the target variables.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        Model object i.e. randomforestregressor, linear model or other.\n",
    "    X_train : dataframe\n",
    "        The explanatory variables for the train/calibration set, numeric columns may already have been scaled.\n",
    "    Y_train : dataframe\n",
    "        The target variables for the train/calibration set. Might/Mighn't be scaled.\n",
    "    X_test : dataframe\n",
    "        The explanatory variables for the test set, columns may be scaled. It will comprise of 24 rows (1 row for each delivery period in the trading day).\n",
    "    Y_test : dataframe\n",
    "        The target variables for the test set i.e. what we would like to forecast. Similar to the previous bullet point, the dataframe will contain 24 rows.\n",
    "    actuals_and_forecast_df : dataframe\n",
    "        Initially the dataframe will only contain the actual values for each of the targets. At the end of the method it will also contain the forecast values.\n",
    "    targets : [str]\n",
    "        These are the items that we want to predict/forecast.\n",
    "    scale_target_variables: boolean\n",
    "       The target vector, do we want to scale it?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Returns a dataframe containing the forecasts and actual values for each of the target variables i.e. test set forecast and actuals.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "        Y_scaler = Y_scaler.fit(Y_train)\n",
    "        cols = Y.columns.values.tolist()\n",
    "        \n",
    "        es = EarlyStopping(monitor='val_loss', mode='min',  patience=20)\n",
    "        model.fit([X_train_LSTM, X_train_ffnn], Y_train, epochs=200, verbose=2,  callbacks=[es], validation_split=0.10)\n",
    "        model_test_predictions = None\n",
    "        model_test_predictions = pd.DataFrame(Y_scaler_n.inverse_transform(np.array(model.predict([X_test_LSTM, X_test_ffnn])).reshape(5, 16), columns=cols))\n",
    "            \n",
    "\n",
    "        print(model_test_predictions)\n",
    "        print(\"test number of observations: \" + str(len(Y_test)))\n",
    "\n",
    "        \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_10\"] = model_test_predictions.iloc[:1,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_30\"] = model_test_predictions.iloc[1:2,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_50\"] = model_test_predictions.iloc[2:3,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist()\n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_70\"] = model_test_predictions.iloc[3:4,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "           \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_90\"] = model_test_predictions.iloc[4:,i].T.tolist() if len(cols) > 1 else model_test_predictions.tolist()\n",
    "\n",
    "\n",
    "        return actuals_and_forecast_df\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def rolling_walk_forward_validation(model, data, targets, start_time, end_time, training_days, path):\n",
    "    \"\"\"\n",
    "    This method implements the rolling walk forward validation process.\n",
    "    That is,\n",
    "        (a) fit the model on the train data\n",
    "        (b) use the fitted model on the test explanatory variables i.e. forecast 1 day ahead.\n",
    "        (c) Move the training and test datasets forward by 1 day and repeat.\n",
    "    The method will produce\n",
    "        (1) A csv containing the forecast and actual target values i.e. test set output over the horizon of interest.\n",
    "        (2) For each target variable, a graph of the actual and forecast values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: model\n",
    "        The model that will be used to train the data and produce the 1 day ahead forecasts\n",
    "    data: dataframe\n",
    "        Participant dataframe containing the explanatory and target variables.\n",
    "    explanatory_variables_of_interest : [str]\n",
    "        The columns in data that will be used as explanatory variables when fitting the model.\n",
    "        If there are categorical variables we want to use as explanatory variables, they are incorporated via the features_to_encode argument.\n",
    "    targets: [str]\n",
    "        The columns in data that we want to predict/forecast.\n",
    "    features_to_encode: [str]\n",
    "        If there are variables in data that we would like to apply one hot encoding to, we list them here. These one hot encoded vectors are then used as explanatory variables.\n",
    "    prefix_to_include: [str]\n",
    "        Just a string which will be used to name the columns if we apply one hot encoding (related to the features_to_encode argument).\n",
    "     start_time: dt\n",
    "       We will produce a forecast on unseen data for each trading period between [start_time, end_time].\n",
    "     end_time: dt\n",
    "       See previous point.\n",
    "    training_days: int\n",
    "       The number of training days (negative integer expected).\n",
    "    path, unit_name, scenario: str\n",
    "       The combination of path + unit_name + scenario indicate where the csv will be output to.\n",
    "    scale_explanatory_variables: boolean\n",
    "       The explanatory variables, do we want to scale them?\n",
    "    scale_target_variables: boolean\n",
    "       The target variables, do we want to scale them?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Output will include a  csv of the forecast/actual target values and a graph of the same.\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)\n",
    "        results = pd.DataFrame()\n",
    "\n",
    "        # Each time we\n",
    "        # (a) fit the model on the calibration/train data\n",
    "        # (b) apply it to the test data i.e. forecast 1 day ahead.\n",
    "        # Repeat.\n",
    "        date_format = \"%m/%d/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "\n",
    "        while start_time < end_time:\n",
    "\n",
    "            # Train interval\n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time\n",
    "\n",
    "            # Test interval, the test period is always the day ahead forecast\n",
    "            test_start_time = train_end_time + td(hours=8)\n",
    "            test_end_time = test_start_time + td(minutes=30)\n",
    "\n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "\n",
    "            # Generate the calibration and test dataframes.\n",
    "            train_X_LSTM, train_X_ffnn, train_y, test_X_LSTM, test_X_ffnn, test_y, test_df, train_df, Y_scaler_n = generate_train_and_test_dataframes(\n",
    "                participant_df=dat, train_start_time=train_start_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                train_end_time=train_end_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                test_start_time=test_start_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                test_end_time=test_end_time.strftime(\"%m/%d/%Y %H:%M\"))\n",
    "\n",
    "            if train_X_LSTM is None or len(train_X_LSTM) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(\n",
    "                    train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            if test_X_LSTM is None or len(test_X_LSTM) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(\n",
    "                    test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            # Fit the model to the train datasets, produce a forecast and return a dataframe containing the forecast/actuals.\n",
    "            actuals_and_forecast_df = fit_multitarget_model(model=model, Y_scaler_n=Y_scaler_n,\n",
    "                                                            X_train_LSTM=train_X_LSTM, X_train_ffnn=train_X_ffnn,\n",
    "                                                            Y_train=train_y,\n",
    "                                                            X_test_LSTM=test_X_LSTM, X_test_ffnn=test_X_ffnn,\n",
    "                                                            Y_test=test_y,\n",
    "                                                            actuals_and_forecast_df=test_df.iloc[:, 0:16],\n",
    "                                                            targets=Y.columns.values.tolist())\n",
    "\n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "\n",
    "            start_time = start_time + td(hours=8)\n",
    "\n",
    "        results.to_csv(path + \".csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "def create_model():    \n",
    "    visible1 = Input(shape=(i_shape_lstm))\n",
    "    x1=visible1\n",
    "    \n",
    "    for i in range(1):\n",
    "        x1= LSTM(64, return_sequences= True, activation='sigmoid', input_shape=i_shape_lstm)(x1)\n",
    "    x1 = Dropout(0.222222)(x1)\n",
    "    \n",
    "    input_1 = Flatten()(x1)   \n",
    "    \n",
    "    \n",
    "    visible2 = Input(shape=(i_shape_ffnn))   \n",
    "    x2= visible2 \n",
    "    \n",
    "    for i in range(2):\n",
    "        x2 = Dense(64, 'tanh')(x2)\n",
    "    x2 = Dropout(0.088889)(x2)\n",
    "    \n",
    "    input_2 = Flatten()(x2) \n",
    "            \n",
    "    \n",
    "    merged = concatenate([input_1, input_2])\n",
    "    \n",
    "    a = Dense(128, 'tanh')(merged)\n",
    "    b = Dense(128, 'tanh')(merged)\n",
    "    c = Dense(128, 'tanh')(merged)\n",
    "    d = Dense(128, 'tanh')(merged)\n",
    "    e = Dense(128, 'tanh')(merged)\n",
    "        \n",
    "            \n",
    "    for i in range(1):\n",
    "            a = Dense(128, 'relu')(a) \n",
    "    output_1 =  Dense(16, name='out_10')(a)\n",
    "    \n",
    "    for i in range(1):\n",
    "            b = Dense(128, 'relu')(b) \n",
    "    output_2 =  Dense(16, name='out_30')(b)\n",
    "    \n",
    "    for i in range(1):\n",
    "            c = Dense(128, 'relu')(c) \n",
    "    output_3 =  Dense(16, name='out_50')(c)\n",
    "    \n",
    "    for i in range(1):\n",
    "            d = Dense(128, 'relu')(d) \n",
    "    output_4 =  Dense(16, name='out_70')(d)\n",
    "    \n",
    "    for i in range(1):\n",
    "            e = Dense(128, 'relu')(e) \n",
    "    output_5 =  Dense(16, name='out_90')(e)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[visible1, visible2], outputs=[output_1, output_2, output_3, output_4, output_5])    \n",
    "    opt = Adam(learning_rate= 0.000100)\n",
    "\n",
    "    model.compile(loss=[loss_10th_p, loss_30th_p, loss_50th_p, loss_70th_p, loss_90th_p], optimizer=opt)\n",
    "    return model\n",
    "\n",
    "mmo = KerasRegressor(build_fn=create_model, epochs=200, batch_size=32, verbose=2)\n",
    "\n",
    "rolling_walk_forward_validation(model=mmo, data=dat, start_time='6/1/2020 00:00', end_time='9/1/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(), training_days=-210,\n",
    "                                path=\"/home/coconnor/MH_Q_1-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b2d2c2",
   "metadata": {},
   "source": [
    "Multi-Headed Deep neural network Recurrent neural network for forecasting the BM price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6f1873",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# months 1-7 search\n",
    "import os ;\n",
    "# os.environ['HDF5_DISABLE_VERSION_CHECK']='2'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from datetime import timedelta as td\n",
    "import traceback\n",
    "# from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import concat\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from math import floor\n",
    "from bayes_opt import BayesianOptimization\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "# tensorflow &keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "# from keras.layers import concatenate\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "from keras.activations import *\n",
    "\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import talos\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "# dat = pd.read_csv(\"C:/Users/ciara/OneDrive/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "# =============================================================================\n",
    "# dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "dat = pd.read_csv(\"/home/coconnor/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "# =============================================================================\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()\n",
    "\n",
    "\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[:11666,:]\n",
    "Y_train=Y.iloc[:11666,:]\n",
    "X_test=X.iloc[11666:13155,:]\n",
    "Y_test=Y.iloc[11666:13155,:]\n",
    "\n",
    "rnn_train_LSTM_1 = X_train.loc[:,\"lag_-3x1\":\"lag_-50x1\"]\n",
    "rnn_train_LSTM_2 = X_train.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "rnn_train_LSTM_3 = X_train.loc[:,\"lag_-2x3\":\"lag_-49x3\"]\n",
    "rnn_train_LSTM_4 = X_train.loc[:,\"lag_0x6\":\"lag_-47x6\"]\n",
    "rnn_train_LSTM_5 = X_train.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "\n",
    "rnn_test_LSTM_1 = X_test.loc[:,\"lag_-3x1\": \"lag_-50x1\"]\n",
    "rnn_test_LSTM_2 = X_test.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "rnn_test_LSTM_3 = X_test.loc[:,\"lag_-2x3\": \"lag_-49x3\"]\n",
    "rnn_test_LSTM_4 = X_test.loc[:,\"lag_0x6\" : \"lag_-47x6\"]\n",
    "rnn_test_LSTM_5 = X_test.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "\n",
    "rnn_train_ffnn_1 = X_train.loc[:,\"lag_2x7\": \"lag_17x7\"]\n",
    "rnn_train_ffnn_2 = X_train.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "rnn_train_ffnn_3 = X_train.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "rnn_train_ffnn_4 = X_train.loc[:,\"lag_2x10\": \"lag_17x10\"]\n",
    "rnn_train_ffnn_5 = X_train.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "\n",
    "rnn_test_ffnn_1 = X_test.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "rnn_test_ffnn_2 = X_test.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "rnn_test_ffnn_3 = X_test.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "rnn_test_ffnn_4 = X_test.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "rnn_test_ffnn_5 = X_test.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "\n",
    "rnn_Y = Y_train.loc[:,\"lag_2y\": \"lag_17y\"]\n",
    "\n",
    "X_scaler_LSTM_1 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_2 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_3 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_4 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler_ffnn_1 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_2 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_3 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_4 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train_LSTM_1 = X_scaler_LSTM_1.fit_transform(rnn_train_LSTM_1)\n",
    "rnn_scaled_train_LSTM_2 = X_scaler_LSTM_2.fit_transform(rnn_train_LSTM_2)\n",
    "rnn_scaled_train_LSTM_3 = X_scaler_LSTM_3.fit_transform(rnn_train_LSTM_3)\n",
    "rnn_scaled_train_LSTM_4 = X_scaler_LSTM_4.fit_transform(rnn_train_LSTM_4)\n",
    "rnn_scaled_train_LSTM_5 = X_scaler_LSTM_5.fit_transform(rnn_train_LSTM_5)\n",
    "\n",
    "rnn_scaled_train_ffnn_1 = X_scaler_ffnn_1.fit_transform(rnn_train_ffnn_1)\n",
    "rnn_scaled_train_ffnn_2 = X_scaler_ffnn_2.fit_transform(rnn_train_ffnn_2)\n",
    "rnn_scaled_train_ffnn_3 = X_scaler_ffnn_3.fit_transform(rnn_train_ffnn_3)\n",
    "rnn_scaled_train_ffnn_4 = X_scaler_ffnn_4.fit_transform(rnn_train_ffnn_4)\n",
    "rnn_scaled_train_ffnn_5 = X_scaler_ffnn_5.fit_transform(rnn_train_ffnn_5)\n",
    "\n",
    "Y_train_Scaled = Y_scaler.fit_transform(Y_train)\n",
    "Y_test_scaled = Y_scaler.transform(Y_test)\n",
    "\n",
    "X_train_Scaled_LSTM = np.hstack(\n",
    "    (rnn_scaled_train_LSTM_1, rnn_scaled_train_LSTM_2, rnn_scaled_train_LSTM_3,\n",
    "     rnn_scaled_train_LSTM_4, rnn_scaled_train_LSTM_5)\n",
    ").reshape(rnn_train_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "X_train_Scaled_ffnn = np.hstack(\n",
    "    (rnn_scaled_train_ffnn_1, rnn_scaled_train_ffnn_2, rnn_scaled_train_ffnn_3,\n",
    "     rnn_scaled_train_ffnn_4, rnn_scaled_train_ffnn_5)\n",
    ").reshape(rnn_train_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled_LSTM = np.hstack(\n",
    "    (X_scaler_LSTM_1.transform(rnn_test_LSTM_1), X_scaler_LSTM_2.transform(rnn_test_LSTM_2),\n",
    "     X_scaler_LSTM_3.transform(rnn_test_LSTM_3), X_scaler_LSTM_4.transform(rnn_test_LSTM_4),\n",
    "     X_scaler_LSTM_5.transform(rnn_test_LSTM_5))\n",
    ").reshape(rnn_test_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled_ffnn = np.hstack(\n",
    "    (X_scaler_ffnn_1.transform(rnn_test_ffnn_1), X_scaler_ffnn_2.transform(rnn_test_ffnn_2),\n",
    "     X_scaler_ffnn_3.transform(rnn_test_ffnn_3), X_scaler_ffnn_4.transform(rnn_test_ffnn_4),\n",
    "     X_scaler_ffnn_5.transform(rnn_test_ffnn_5))\n",
    ").reshape(rnn_test_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "i_shape_lstm = (X_train_Scaled_LSTM.shape[1], X_train_Scaled_LSTM.shape[2])\n",
    "i_shape_ffnn = (X_train_Scaled_ffnn.shape[1], X_train_Scaled_ffnn.shape[2])\n",
    "\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                                       test_start_time: dt, test_end_time: dt):\n",
    "\n",
    "\n",
    "    train_X_LSTM = None\n",
    "    train_X_ffnn = None\n",
    "\n",
    "    train_y = None\n",
    "    test_X_LSTM = None\n",
    "    test_X_ffnn = None\n",
    "    test_y = None\n",
    "    test_df = None\n",
    "    train_df = None\n",
    "\n",
    "    try:\n",
    "\n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "\n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        participant_df = participant_df.dropna()\n",
    "\n",
    "        date_format = \"%m/%d/%Y %H:%M\"\n",
    "\n",
    "        train_df = None\n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)\n",
    "        train_df = participant_df[\n",
    "            (participant_df.index >= train_start_time_str) & (participant_df.index < train_end_time_str)].copy(\n",
    "            deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\n",
    "                \"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")\n",
    "\n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)\n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format)\n",
    "        test_df = participant_df[\n",
    "            (participant_df.index >= test_start_time_str) & (participant_df.index < test_end_time_str)].copy(\n",
    "            deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\n",
    "                \"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")\n",
    "\n",
    "        rnn_train_LSTM_1 = train_df.loc[:,\"lag_-3x1\":\"lag_-50x1\"]\n",
    "        rnn_train_LSTM_2 = train_df.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "        rnn_train_LSTM_3 = train_df.loc[:,\"lag_-2x3\":\"lag_-49x3\"]\n",
    "        rnn_train_LSTM_4 = train_df.loc[:,\"lag_0x6\":\"lag_-47x6\"]\n",
    "        rnn_train_LSTM_5 = train_df.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "\n",
    "        rnn_test_LSTM_1 = test_df.loc[:,\"lag_-3x1\": \"lag_-50x1\"]\n",
    "        rnn_test_LSTM_2 = test_df.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "        rnn_test_LSTM_3 = test_df.loc[:,\"lag_-2x3\": \"lag_-49x3\"]\n",
    "        rnn_test_LSTM_4 = test_df.loc[:,\"lag_0x6\" : \"lag_-47x6\"]\n",
    "        rnn_test_LSTM_5 = test_df.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "        \n",
    "        rnn_train_ffnn_1 = train_df.loc[:,\"lag_2x7\": \"lag_17x7\"]\n",
    "        rnn_train_ffnn_2 = train_df.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "        rnn_train_ffnn_3 = train_df.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "        rnn_train_ffnn_4 = train_df.loc[:,\"lag_2x10\": \"lag_17x10\"]\n",
    "        rnn_train_ffnn_5 = train_df.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "        \n",
    "        rnn_test_ffnn_1 = test_df.loc[:,\"lag_2x7\": \"lag_17x7\"]\n",
    "        rnn_test_ffnn_2 = test_df.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "        rnn_test_ffnn_3 = test_df.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "        rnn_test_ffnn_4 = test_df.loc[:,\"lag_2x10\": \"lag_17x10\"]\n",
    "        rnn_test_ffnn_5 = test_df.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "\n",
    "        rnn_Y = train_df.loc[:,\"lag_2y\": \"lag_17y\"]\n",
    "        \n",
    "        X_scaler_LSTM_1 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_LSTM_2 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_LSTM_3 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_LSTM_4 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_LSTM_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "        X_scaler_ffnn_1 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_ffnn_2 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_ffnn_3 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_ffnn_4 = preprocessing.MinMaxScaler()\n",
    "        X_scaler_ffnn_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "        rnn_scaled_train_LSTM_1 = X_scaler_LSTM_1.fit_transform(rnn_train_LSTM_1)\n",
    "        rnn_scaled_train_LSTM_2 = X_scaler_LSTM_2.fit_transform(rnn_train_LSTM_2)\n",
    "        rnn_scaled_train_LSTM_3 = X_scaler_LSTM_3.fit_transform(rnn_train_LSTM_3)\n",
    "        rnn_scaled_train_LSTM_4 = X_scaler_LSTM_4.fit_transform(rnn_train_LSTM_4)\n",
    "        rnn_scaled_train_LSTM_5 = X_scaler_LSTM_5.fit_transform(rnn_train_LSTM_5)\n",
    "\n",
    "        rnn_scaled_train_ffnn_1 = X_scaler_ffnn_1.fit_transform(rnn_train_ffnn_1)\n",
    "        rnn_scaled_train_ffnn_2 = X_scaler_ffnn_2.fit_transform(rnn_train_ffnn_2)\n",
    "        rnn_scaled_train_ffnn_3 = X_scaler_ffnn_3.fit_transform(rnn_train_ffnn_3)\n",
    "        rnn_scaled_train_ffnn_4 = X_scaler_ffnn_4.fit_transform(rnn_train_ffnn_4)\n",
    "        rnn_scaled_train_ffnn_5 = X_scaler_ffnn_5.fit_transform(rnn_train_ffnn_5)\n",
    "\n",
    "        train_y = Y_scaler.fit_transform(rnn_Y)\n",
    "        Y_scaler_n = Y_scaler.fit(rnn_Y)\n",
    "\n",
    "        train_X_LSTM = np.hstack(\n",
    "            (rnn_scaled_train_LSTM_1, rnn_scaled_train_LSTM_2, rnn_scaled_train_LSTM_3,\n",
    "             rnn_scaled_train_LSTM_4, rnn_scaled_train_LSTM_5)\n",
    "        ).reshape(rnn_train_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "        train_X_ffnn = np.hstack(\n",
    "            (rnn_scaled_train_ffnn_1, rnn_scaled_train_ffnn_2, rnn_scaled_train_ffnn_3,\n",
    "             rnn_scaled_train_ffnn_4, rnn_scaled_train_ffnn_5)\n",
    "        ).reshape(rnn_train_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "        test_X_LSTM = np.hstack(\n",
    "            (X_scaler_LSTM_1.transform(rnn_test_LSTM_1), X_scaler_LSTM_2.transform(rnn_test_LSTM_2),\n",
    "             X_scaler_LSTM_3.transform(rnn_test_LSTM_3), X_scaler_LSTM_4.transform(rnn_test_LSTM_4),\n",
    "             X_scaler_LSTM_5.transform(rnn_test_LSTM_5))\n",
    "        ).reshape(rnn_test_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "        test_X_ffnn = np.hstack(\n",
    "            (X_scaler_ffnn_1.transform(rnn_test_ffnn_1), X_scaler_ffnn_2.transform(rnn_test_ffnn_2),\n",
    "             X_scaler_ffnn_3.transform(rnn_test_ffnn_3), X_scaler_ffnn_4.transform(rnn_test_ffnn_4),\n",
    "             X_scaler_ffnn_5.transform(rnn_test_ffnn_5))\n",
    "        ).reshape(rnn_test_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "        test_y = test_df.iloc[:, 0:16]\n",
    "\n",
    "        return train_X_LSTM, train_X_ffnn, train_y, test_X_LSTM, test_X_ffnn, test_y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X_LSTM, train_X_ffnn, train_y, test_X_LSTM, test_X_ffnn, test_y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "\n",
    "def fit_multitarget_model(model, X_train_LSTM, X_train_ffnn, Y_train, X_test_LSTM, X_test_ffnn, Y_test,\n",
    "                          actuals_and_forecast_df, targets, Y_scaler_n):\n",
    "\n",
    "    try:\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "        Y_scaler = Y_scaler.fit(Y_train)\n",
    "        cols = Y.columns.values.tolist()\n",
    "\n",
    "        model.fit([X_train_LSTM, X_train_ffnn], Y_train)\n",
    "        model_test_predictions = None\n",
    "        #         model_test_predictions = model.predict(X_test)\n",
    "        model_test_predictions = pd.DataFrame(\n",
    "            Y_scaler_n.inverse_transform(model.predict([X_test_LSTM, X_test_ffnn]).reshape(1, 16)), columns=cols,\n",
    "            index=Y_test.index)\n",
    "        print(model_test_predictions)\n",
    "        print(\"test number of observations: \" + str(len(Y_test)))\n",
    "        model_test_mse = mean_squared_error(Y_test, model_test_predictions)\n",
    "        model_test_rmse = round(np.sqrt(model_test_mse), 2)\n",
    "        model_test_mae = round(mean_absolute_error(Y_test, model_test_predictions), 2)\n",
    "        print(\"test rmse: \" + str(model_test_rmse))\n",
    "        print(\"test mae: \" + str(model_test_mae))\n",
    "\n",
    "        for i in range(0, len(cols)):\n",
    "            actuals_and_forecast_df[cols[i] + \"_Forecast\"] = model_test_predictions.iloc[:, i].tolist() if len(\n",
    "                cols) > 1 else model_test_predictions.tolist()\n",
    "            predictor_test_mse = mean_squared_error(Y_test[cols[i]], model_test_predictions.iloc[:, i]) if len(\n",
    "                cols) > 1 else mean_squared_error(Y_test[cols[i]], model_test_predictions.tolist())\n",
    "            predictor_test_rmse = round(np.sqrt(predictor_test_mse), 2)\n",
    "            predictor_test_mae = round(mean_absolute_error(Y_test[cols[i]], model_test_predictions.iloc[:, i]),\n",
    "                                       2) if len(cols) > 1 else round(\n",
    "                mean_absolute_error(Y_test[cols[i]], model_test_predictions.tolist()), 2)\n",
    "            print(cols[i] + \" test rmse: \" + str(predictor_test_rmse))\n",
    "            print(cols[i] + \" test mae: \" + str(predictor_test_mae))\n",
    "\n",
    "        Error_i = ([model_test_rmse, model_test_mae])\n",
    "        print(Error_i)\n",
    "        actuals_and_forecast_df = actuals_and_forecast_df.append(Error_i)\n",
    "\n",
    "\n",
    "\n",
    "        return actuals_and_forecast_df\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def rolling_walk_forward_validation(model, data, targets, start_time, end_time, training_days, path):\n",
    "\n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)\n",
    "        results = pd.DataFrame()\n",
    "\n",
    "\n",
    "        date_format = \"%m/%d/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "\n",
    "        while start_time < end_time:\n",
    "\n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time\n",
    "\n",
    "            test_start_time = train_end_time + td(hours=8)\n",
    "            test_end_time = test_start_time + td(minutes=30)\n",
    "\n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "\n",
    "            train_X_LSTM, train_X_ffnn, train_y, test_X_LSTM, test_X_ffnn, test_y, test_df, train_df, Y_scaler_n = generate_train_and_test_dataframes(\n",
    "                participant_df=dat, train_start_time=train_start_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                train_end_time=train_end_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                test_start_time=test_start_time.strftime(\"%m/%d/%Y %H:%M\"),\n",
    "                test_end_time=test_end_time.strftime(\"%m/%d/%Y %H:%M\"))\n",
    "\n",
    "            if train_X_LSTM is None or len(train_X_LSTM) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(\n",
    "                    train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            if test_X_LSTM is None or len(test_X_LSTM) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(\n",
    "                    test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            actuals_and_forecast_df = fit_multitarget_model(model=model, Y_scaler_n=Y_scaler_n,\n",
    "                                                            X_train_LSTM=train_X_LSTM, X_train_ffnn=train_X_ffnn,\n",
    "                                                            Y_train=train_y,\n",
    "                                                            X_test_LSTM=test_X_LSTM, X_test_ffnn=test_X_ffnn,\n",
    "                                                            Y_test=test_y,\n",
    "                                                            actuals_and_forecast_df=test_df.iloc[:, 0:16],\n",
    "                                                            targets=Y.columns.values.tolist())\n",
    "\n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "\n",
    "            start_time = start_time + td(hours=8)\n",
    "\n",
    "        results.to_csv(path + \".csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "def create_model():    \n",
    "            visible1 = Input(shape=(i_shape_lstm))            \n",
    "            dense1 = LSTM(128, return_sequences= True, activation='tanh' , input_shape=i_shape_lstm)(visible1)\n",
    "            dense2 = LSTM(128, return_sequences= True, activation='tanh' , input_shape=i_shape_lstm)(dense1)\n",
    "            do_lstm = Dropout(0.044444, seed=123)(dense2)\n",
    "            dense3 = LSTM(128)(do_lstm)\n",
    "            flat1 = Flatten()(dense3)    \n",
    "    \n",
    "            visible2 = Input(shape=(i_shape_ffnn))        \n",
    "            dense5 = Dense(16, activation='relu')(visible2)        \n",
    "            dense6 = Dense(16, activation=LeakyReLU)(dense5)\n",
    "            do_ffnn = Dropout(0.200000, seed=123)(dense6)\n",
    "            dense7 = Dense(16, activation='relu')(do_ffnn)\n",
    "            flat2 = Flatten()(dense7)\n",
    "    \n",
    "            merged = concatenate([flat1, flat2])\n",
    "            dense_f = Dense(256, activation='relu')(merged)        \n",
    "            outputs = Dense(16)(dense_f)\n",
    "\n",
    "            model = Model(inputs=[visible1, visible2], outputs=outputs)    \n",
    "            opt = Adam(lr=0.004522)\n",
    "            model.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mean_absolute_error'])\n",
    "            return model\n",
    "        \n",
    "es = EarlyStopping(monitor='mean_absolute_error', mode='min', verbose=0, patience=20)\n",
    "mmo = KerasRegressor(build_fn=create_model, epochs=500, batch_size=48, verbose=2, callbacks=[es])\n",
    "\n",
    "rolling_walk_forward_validation(model=mmo, data=dat, start_time='9/1/2020 00:00', end_time='12/1/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(), training_days=-300,\n",
    "                                path=\"/home/coconnor/BM_results_MH_1-13\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a1dcaa",
   "metadata": {},
   "source": [
    "light gradient boosting machine for quantile forecast in the BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8663faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "from datetime import datetime\n",
    "# from pandas import Timedelta as td\n",
    "import traceback\n",
    "# from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "date_format=\"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/coconnor/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat=pd.DataFrame(dat)\n",
    "dat=dat.bfill(axis ='rows')\n",
    "dat=dat.ffill(axis ='rows')\n",
    "dat=dat._get_numeric_data()\n",
    "\n",
    "Y=dat.iloc[:, 0:16]\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                        test_start_time: dt, test_end_time: dt):\n",
    "\n",
    "    train_X = None\n",
    "    train_y = None \n",
    "    test_X = None \n",
    "    test_y = None \n",
    "    test_df = None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        participant_df = participant_df.dropna()\n",
    "\n",
    "        date_format=\"%m/%d/%Y %H:%M\"\n",
    "\n",
    "        train_df = None\n",
    "        \n",
    "\n",
    "        \n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)    \n",
    "        train_df = participant_df[(participant_df.index>=train_start_time_str) & (participant_df.index<train_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")            \n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "\n",
    "        \n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)    \n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format) \n",
    "        test_df = participant_df[(participant_df.index>=test_start_time_str) & (participant_df.index<test_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")            \n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "\n",
    "        train_X = train_df.iloc[:, 16:]\n",
    "        test_X = test_df.iloc[:, 16:]\n",
    "        train_y = train_df.iloc[:, 0:16]\n",
    "        test_y = test_df.iloc[:, 0:16]\n",
    "                                \n",
    "        return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X, train_y, test_X, test_y, test_df\n",
    "    \n",
    "    \n",
    "def fit_multitarget_model(model_1, model_2, model_3, model_4, model_5, X_train, Y_train, X_test, Y_test, actuals_and_forecast_df, targets):\n",
    "\n",
    "    try:\n",
    "        \n",
    "#         model.fit(X_train, Y_train) if len(targets) > 1 else model.fit(X_train, Y_train.values.ravel())  \n",
    "        model_1.fit(X_train, Y_train) \n",
    "        model_2.fit(X_train, Y_train) \n",
    "        model_3.fit(X_train, Y_train) \n",
    "        model_4.fit(X_train, Y_train) \n",
    "        model_5.fit(X_train, Y_train) \n",
    "\n",
    "\n",
    "        model_test_predictions_1=None  \n",
    "        model_test_predictions_3=None  \n",
    "        model_test_predictions_5=None  \n",
    "        model_test_predictions_7=None  \n",
    "        model_test_predictions_9=None  \n",
    "        model_test_predictions_1 = model_1.predict(X_test)     \n",
    "        model_test_predictions_3 = model_2.predict(X_test) \n",
    "        model_test_predictions_5 = model_3.predict(X_test)     \n",
    "        model_test_predictions_7 = model_4.predict(X_test)     \n",
    "        model_test_predictions_9 = model_5.predict(X_test)          \n",
    "                    \n",
    "        cols = Y_train.columns.values.tolist()   \n",
    "        \n",
    "        \n",
    "        print(\"train number of observations: \" + str(len(Y_train)))\n",
    "\n",
    "        \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_10\"] = model_test_predictions_1[:,i].tolist() if len(cols) > 1 else model_test_predictions_1.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_30\"] = model_test_predictions_3[:,i].tolist() if len(cols) > 1 else model_test_predictions_3.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_50\"] = model_test_predictions_5[:,i].tolist() if len(cols) > 1 else model_test_predictions_5.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_70\"] = model_test_predictions_7[:,i].tolist() if len(cols) > 1 else model_test_predictions_7.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_90\"] = model_test_predictions_9[:,i].tolist() if len(cols) > 1 else model_test_predictions_9.tolist() \n",
    "          \n",
    "           \n",
    "        return actuals_and_forecast_df\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "def rolling_walk_forward_validation(model_1, model_2, model_3, model_4, model_5, data, targets, start_time, end_time, training_days, path):\n",
    "\n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)            \n",
    "        results = pd.DataFrame()\n",
    "            \n",
    "        #Each time we \n",
    "        # (a) fit the model on the calibration/train data\n",
    "        # (b) apply it to the test data i.e. forecast 1 day ahead.\n",
    "        #Repeat.\n",
    "        date_format=\"%m/%d/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "        \n",
    "        while start_time < end_time:\n",
    "            \n",
    "            #Train interval\n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time \n",
    "    \n",
    "            #Test interval, the test period is always the day ahead forecast\n",
    "            test_start_time = train_end_time + td(hours=8)\n",
    "            test_end_time = test_start_time + td(minutes=30)\n",
    "            \n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "    \n",
    "            #Generate the calibration and test dataframes.\n",
    "            train_X, train_y, test_X, test_y, test_df = generate_train_and_test_dataframes(participant_df=dat, train_start_time=train_start_time.strftime(\"%m/%d/%Y %H:%M\"), train_end_time=train_end_time.strftime(\"%m/%d/%Y %H:%M\"), \n",
    "                            test_start_time=test_start_time.strftime(\"%m/%d/%Y %H:%M\"), test_end_time=test_end_time.strftime(\"%m/%d/%Y %H:%M\"))\n",
    "            \n",
    "            if train_X is None or len(train_X) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "    \n",
    "            if test_X is None or len(test_X) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "            \n",
    "            actuals_and_forecast_df = fit_multitarget_model(model_1=model_1,model_2=model_2,model_3=model_3, model_4=model_4,model_5=model_5, X_train=train_X, Y_train=train_y,\n",
    "                                            X_test=test_X, Y_test=test_y, actuals_and_forecast_df=test_df,targets=Y.columns.values.tolist())\n",
    "    \n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "            print(results)\n",
    "            start_time = start_time + td(hours=8)\n",
    "            \n",
    "        results.to_csv(path  + \".csv\", index = False)\n",
    "        \n",
    "          \n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "\n",
    "\n",
    "rolling_walk_forward_validation(model_1=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.1, learning_rate = 0.05, num_leaves=10,  max_depth = 4, n_estimators =  100)),\n",
    "                                model_2=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.3, learning_rate = 0.05, num_leaves=10,  max_depth = 4, n_estimators =  100)),\n",
    "                                model_3=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.5, learning_rate = 0.05, num_leaves=10,  max_depth = 4, n_estimators =  100)),\n",
    "                                model_4=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.7, learning_rate = 0.05, num_leaves=10,  max_depth = 4, n_estimators =  100)),\n",
    "                                model_5=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.9, learning_rate = 0.05, num_leaves=10,  max_depth = 4, n_estimators =  100)),\n",
    "                                data=dat, start_time='06/1/2020 00:00',end_time='09/1/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(),training_days=-210, path=\"/home/coconnor/lgbm_Q_1-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b82b10",
   "metadata": {},
   "source": [
    "Single headed Deep neural network for quantile forecast in the DAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee48b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os;\n",
    "# os.environ['HDF5_DISABLE_VERSION_CHECK']='2'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from datetime import timedelta as td\n",
    "import traceback\n",
    "# from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import concat\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from math import floor\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "# tensorflow &keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "date_format = \"%d/%m/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/coconnor/DAM_VAR_1-3.csv\", index_col=\"DeliveryPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "\n",
    "Y = dat.iloc[:, 0:24]\n",
    "X = dat.iloc[:, 24:]\n",
    "X_train = X.iloc[:12360, :]\n",
    "Y_train = Y.iloc[:12360, :]\n",
    "X_test = X.iloc[12360:13104, :]\n",
    "Y_test = Y.iloc[12360:13104, :]\n",
    "\n",
    "\n",
    "def qloss(qs, y_true, y_pred):\n",
    "    # Pinball loss for multiple quantiles\n",
    "    q = tf.constant(np.array([qs]), dtype=tf.float32)\n",
    "    e = y_true - y_pred\n",
    "    v = tf.maximum(q * e, (q - 1) * e)\n",
    "    return K.mean(v)\n",
    "\n",
    "\n",
    "loss_10th_p = lambda y_true, y_pred: qloss(0.1, y_true, y_pred)\n",
    "loss_30th_p = lambda y_true, y_pred: qloss(0.3, y_true, y_pred)\n",
    "loss_50th_p = lambda y_true, y_pred: qloss(0.5, y_true, y_pred)\n",
    "loss_70th_p = lambda y_true, y_pred: qloss(0.7, y_true, y_pred)\n",
    "loss_90th_p = lambda y_true, y_pred: qloss(0.9, y_true, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "rnn_train_1 = X_train.loc[:, \"EURPrices-24\":\"EURPrices-167\"]\n",
    "rnn_train_2 = X_train.loc[:, \"WF\": \"WF-143\"]\n",
    "rnn_train_3 = X_train.loc[:, \"DF\": \"DF-143\"]\n",
    "\n",
    "rnn_test_1 = X_test.loc[:, \"EURPrices-24\":\"EURPrices-167\"]\n",
    "rnn_test_2 = X_test.loc[:, \"WF\": \"WF-143\"]\n",
    "rnn_test_3 = X_test.loc[:, \"DF\": \"DF-143\"]\n",
    "\n",
    "rnn_Y = Y_train.loc[:, \"EURPrices\":\"EURPrices+23\"]\n",
    "rnn_test_Y = Y_test.loc[:, \"EURPrices\":\"EURPrices+23\"]\n",
    "\n",
    "X_scaler1 = preprocessing.MinMaxScaler()\n",
    "X_scaler2 = preprocessing.MinMaxScaler()\n",
    "X_scaler3 = preprocessing.MinMaxScaler()\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train_1 = X_scaler1.fit_transform(rnn_train_1)\n",
    "rnn_scaled_train_2 = X_scaler2.fit_transform(rnn_train_2)\n",
    "rnn_scaled_train_3 = X_scaler3.fit_transform(rnn_train_3)\n",
    "\n",
    "Y_train_Scaled = Y_scaler.fit_transform(rnn_Y)\n",
    "Y_test_scaled = Y_scaler.transform(rnn_test_Y)\n",
    "\n",
    "X_train_Scaled = np.hstack(\n",
    "    (rnn_scaled_train_1, rnn_scaled_train_2, rnn_scaled_train_3)\n",
    ").reshape(rnn_train_1.shape[0], 3, 144).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled = np.hstack(\n",
    "    (X_scaler1.transform(rnn_test_1), X_scaler2.transform(rnn_test_2), X_scaler3.transform(rnn_test_3))\n",
    ").reshape(rnn_test_1.shape[0], 3, 144).transpose(0, 2, 1)\n",
    "\n",
    "i_shape = (X_train_Scaled.shape[1], X_train_Scaled.shape[2])\n",
    "\n",
    "\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                                       test_start_time: dt, test_end_time: dt):\n",
    "\n",
    "\n",
    "    # These are the dataframes that will be returned from the method.\n",
    "    train_X = None\n",
    "    train_y = None\n",
    "    test_X = None\n",
    "    test_y = None\n",
    "    test_df = None\n",
    "    train_df = None\n",
    "\n",
    "    try:\n",
    "\n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "        #             return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        # Remove any rows with nan's etc (there shouldn't be any in the input).\n",
    "        participant_df = participant_df.dropna()\n",
    "\n",
    "        date_format = \"%d/%m/%Y %H:%M\"\n",
    "\n",
    "        # The train dataframe, it will be used later to create train_X and train_y.\n",
    "        train_df = None\n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)\n",
    "        train_df = participant_df[\n",
    "            (participant_df.index >= train_start_time_str) & (participant_df.index < train_end_time_str)].copy(\n",
    "            deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\n",
    "                \"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")\n",
    "        #             return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        # Create the test dataframe, it will be used later to create test_X and test_y\n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)\n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format)\n",
    "        test_df = participant_df[\n",
    "            (participant_df.index >= test_start_time_str) & (participant_df.index < test_end_time_str)].copy(\n",
    "            deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\n",
    "                \"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")\n",
    "        #             return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "        rnn_train_1 = train_df.loc[:, \"EURPrices-24\":\"EURPrices-167\"]\n",
    "        rnn_train_2 = train_df.loc[:, \"WF\": \"WF-143\"]\n",
    "        rnn_train_3 = train_df.loc[:, \"DF\": \"DF-143\"]\n",
    "\n",
    "        rnn_test_1 = test_df.loc[:, \"EURPrices-24\":\"EURPrices-167\"]\n",
    "        rnn_test_2 = test_df.loc[:, \"WF\": \"WF-143\"]\n",
    "        rnn_test_3 = test_df.loc[:, \"DF\": \"DF-143\"]\n",
    "\n",
    "        rnn_Y = train_df.loc[:, \"EURPrices\":\"EURPrices+23\"]\n",
    "        rnn_test_Y = test_df.loc[:, \"EURPrices\":\"EURPrices+23\"]\n",
    "\n",
    "        X_scaler1 = preprocessing.MinMaxScaler()\n",
    "        X_scaler2 = preprocessing.MinMaxScaler()\n",
    "        X_scaler3 = preprocessing.MinMaxScaler()\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "        rnn_scaled_train_1 = X_scaler1.fit_transform(rnn_train_1)\n",
    "        rnn_scaled_train_2 = X_scaler2.fit_transform(rnn_train_2)\n",
    "        rnn_scaled_train_3 = X_scaler3.fit_transform(rnn_train_3)\n",
    "\n",
    "        train_y = Y_scaler.fit_transform(rnn_Y)\n",
    "        Y_scaler_n = Y_scaler.fit(rnn_Y)\n",
    "\n",
    "        Y_test_scaled = Y_scaler.transform(rnn_test_Y)\n",
    "\n",
    "        train_X = np.hstack(\n",
    "            (rnn_scaled_train_1, rnn_scaled_train_2, rnn_scaled_train_3)\n",
    "        ).reshape(rnn_train_1.shape[0], 3, 144).transpose(0, 2, 1)\n",
    "\n",
    "        test_X = np.hstack(\n",
    "            (X_scaler1.transform(rnn_test_1), X_scaler2.transform(rnn_test_2), X_scaler3.transform(rnn_test_3))\n",
    "        ).reshape(rnn_test_1.shape[0], 3, 144).transpose(0, 2, 1)\n",
    "\n",
    "        test_y = test_df.iloc[:, 0:24]\n",
    "\n",
    "        return train_X, train_y, test_X, test_y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X, train_y, test_X, test_y, test_df, train_df, Y_scaler_n\n",
    "\n",
    "\n",
    "def fit_multitarget_model(model, X_train, Y_train, X_test, Y_test, actuals_and_forecast_df, targets, Y_scaler_n):\n",
    "    try:\n",
    "        Y_scaler = preprocessing.MinMaxScaler()\n",
    "        Y_scaler = Y_scaler.fit(Y_train)\n",
    "        cols = Y.columns.values.tolist()\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min',  patience=20)\n",
    "\n",
    "        model.fit(X_train, Y_train, epochs=200, verbose=2,  callbacks=[es], validation_split=0.1)\n",
    "        model_test_predictions=None\n",
    "        model_test_predictions = pd.DataFrame(Y_scaler_n.inverse_transform(np.array(model.predict(X_test).reshape(5,24))), columns=cols)\n",
    "\n",
    "        print(model_test_predictions)\n",
    "        print(\"test number of observations: \" + str(len(Y_test)))\n",
    "\n",
    "        for i in range(0, len(cols)):\n",
    "            actuals_and_forecast_df[cols[i] + \"_Forecast_10\"] = model_test_predictions.iloc[:1, i].T.tolist() if len(\n",
    "                cols) > 1 else model_test_predictions.tolist()\n",
    "\n",
    "        for i in range(0, len(cols)):\n",
    "            actuals_and_forecast_df[cols[i] + \"_Forecast_30\"] = model_test_predictions.iloc[1:2, i].T.tolist() if len(\n",
    "                cols) > 1 else model_test_predictions.tolist()\n",
    "\n",
    "        for i in range(0, len(cols)):\n",
    "            actuals_and_forecast_df[cols[i] + \"_Forecast_50\"] = model_test_predictions.iloc[2:3, i].T.tolist() if len(\n",
    "                cols) > 1 else model_test_predictions.tolist()\n",
    "\n",
    "        for i in range(0, len(cols)):\n",
    "            actuals_and_forecast_df[cols[i] + \"_Forecast_70\"] = model_test_predictions.iloc[3:4, i].T.tolist() if len(\n",
    "                cols) > 1 else model_test_predictions.tolist()\n",
    "\n",
    "        for i in range(0, len(cols)):\n",
    "            actuals_and_forecast_df[cols[i] + \"_Forecast_90\"] = model_test_predictions.iloc[4:, i].T.tolist() if len(\n",
    "                cols) > 1 else model_test_predictions.tolist()\n",
    "\n",
    "        return actuals_and_forecast_df\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def rolling_walk_forward_validation(model, data, targets, start_time, end_time, training_days, path):\n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)\n",
    "        results = pd.DataFrame()\n",
    "\n",
    "        date_format = \"%d/%m/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "\n",
    "        while start_time < end_time:\n",
    "\n",
    "            # Train interval\n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time\n",
    "\n",
    "            # Test interval, the test period is always the day ahead forecast\n",
    "            test_start_time = train_end_time + td(hours=0)\n",
    "            test_end_time = test_start_time + td(hours=1)\n",
    "\n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "\n",
    "            # Generate the calibration and test dataframes.\n",
    "            train_X, train_y, test_X, test_y, test_df, train_df, Y_scaler_n = generate_train_and_test_dataframes(\n",
    "                participant_df=dat, train_start_time=train_start_time.strftime(\"%d/%m/%Y %H:%M\"),\n",
    "                train_end_time=train_end_time.strftime(\"%d/%m/%Y %H:%M\"),\n",
    "                test_start_time=test_start_time.strftime(\"%d/%m/%Y %H:%M\"),\n",
    "                test_end_time=test_end_time.strftime(\"%d/%m/%Y %H:%M\"))\n",
    "\n",
    "            if train_X is None or len(train_X) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(\n",
    "                    train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            if test_X is None or len(test_X) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(\n",
    "                    test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "\n",
    "            # Fit the model to the train datasets, produce a forecast and return a dataframe containing the forecast/actuals.\n",
    "            actuals_and_forecast_df = fit_multitarget_model(model=model, Y_scaler_n=Y_scaler_n, X_train=train_X,\n",
    "                                                            Y_train=train_y,\n",
    "                                                            X_test=test_X, Y_test=test_y,\n",
    "                                                            actuals_and_forecast_df=test_df.iloc[:, 0:24],\n",
    "                                                            targets=Y.columns.values.tolist())\n",
    "\n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "            start_time = start_time + td(hours=24)\n",
    "\n",
    "        results.to_csv(path + \".csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    net_input = Input(shape=i_shape)\n",
    "\n",
    "    x = Flatten(input_shape=i_shape)(net_input)\n",
    "\n",
    "    for i in range(3):\n",
    "        x = Dense(144, 'sigmoid')(x)\n",
    "\n",
    "    for i in range(3):\n",
    "        x = Dense(240, 'tanh')(x)\n",
    "    x = Dropout(0.311111)(x)\n",
    "\n",
    "    a = Dense(24, 'tanh')(x)\n",
    "    b = Dense(24, 'tanh')(x)\n",
    "    c = Dense(24, 'tanh')(x)\n",
    "    d = Dense(24, 'tanh')(x)\n",
    "    e = Dense(24, 'tanh')(x)\n",
    "\n",
    "    for i in range(1):\n",
    "        a = Dense(48, 'relu')(a)\n",
    "    output_1 = Dense(24, name='out_10')(a)\n",
    "\n",
    "    for i in range(1):\n",
    "        b = Dense(48, 'relu')(b)\n",
    "    output_2 = Dense(24, name='out_30')(b)\n",
    "\n",
    "    for i in range(1):\n",
    "        c = Dense(48, 'relu')(c)\n",
    "    output_3 = Dense(24, name='out_50')(c)\n",
    "\n",
    "    for i in range(1):\n",
    "        d = Dense(48, 'relu')(d)\n",
    "    output_4 = Dense(24, name='out_70')(d)\n",
    "\n",
    "    for i in range(1):\n",
    "        e = Dense(48, 'relu')(e)\n",
    "    output_5 = Dense(24, name='out_90')(e)\n",
    "\n",
    "    opt = Adam(learning_rate=0.000100)\n",
    "    model = Model(inputs=net_input, outputs=[output_1, output_2, output_3, output_4, output_5])\n",
    "    model.compile(loss=[loss_10th_p, loss_30th_p, loss_50th_p, loss_70th_p, loss_90th_p], optimizer=opt)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "mmo = KerasRegressor(build_fn=create_model, epochs=200, batch_size=16, verbose=2)\n",
    "\n",
    "rolling_walk_forward_validation(model=mmo, data=dat, start_time='1/6/2020 00:00', end_time='1/9/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(), training_days=-210,\n",
    "                                path=\"/home/coconnor/SH_Q_DAM_1-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ba1d92",
   "metadata": {},
   "source": [
    "light gradient boosting library for quantile forecast in the DAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a721da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "from datetime import datetime\n",
    "# from pandas import Timedelta as td\n",
    "import traceback\n",
    "# from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "date_format = \"%d/%m/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/coconnor/DAM_VAR_1-3.csv\", index_col=\"DeliveryPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "\n",
    "Y = dat.iloc[:, 0:24]\n",
    "\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                        test_start_time: dt, test_end_time: dt):\n",
    "\n",
    "    train_X = None\n",
    "    train_y = None \n",
    "    test_X = None \n",
    "    test_y = None \n",
    "    test_df = None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        participant_df = participant_df.dropna()\n",
    "\n",
    "        date_format = \"%d/%m/%Y %H:%M\"\n",
    "\n",
    "        train_df = None\n",
    "        \n",
    "\n",
    "        \n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)    \n",
    "        train_df = participant_df[(participant_df.index>=train_start_time_str) & (participant_df.index<train_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")            \n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "\n",
    "        \n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)    \n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format) \n",
    "        test_df = participant_df[(participant_df.index>=test_start_time_str) & (participant_df.index<test_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")            \n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "\n",
    "        train_X = train_df.iloc[:, 24:]\n",
    "        test_X = test_df.iloc[:, 24:]\n",
    "        train_y = train_df.iloc[:, 0:24]\n",
    "        test_y = test_df.iloc[:, 0:24]\n",
    "                                \n",
    "        return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X, train_y, test_X, test_y, test_df\n",
    "    \n",
    "    \n",
    "def fit_multitarget_model(model_1, model_2, model_3, model_4, model_5, X_train, Y_train, X_test, Y_test, actuals_and_forecast_df, targets):\n",
    "\n",
    "    try:\n",
    "        \n",
    "#         model.fit(X_train, Y_train) if len(targets) > 1 else model.fit(X_train, Y_train.values.ravel())  \n",
    "        model_1.fit(X_train, Y_train) \n",
    "        model_2.fit(X_train, Y_train) \n",
    "        model_3.fit(X_train, Y_train) \n",
    "        model_4.fit(X_train, Y_train) \n",
    "        model_5.fit(X_train, Y_train) \n",
    "\n",
    "\n",
    "        model_test_predictions_1=None  \n",
    "        model_test_predictions_3=None  \n",
    "        model_test_predictions_5=None  \n",
    "        model_test_predictions_7=None  \n",
    "        model_test_predictions_9=None  \n",
    "        model_test_predictions_1 = model_1.predict(X_test)     \n",
    "        model_test_predictions_3 = model_2.predict(X_test) \n",
    "        model_test_predictions_5 = model_3.predict(X_test)     \n",
    "        model_test_predictions_7 = model_4.predict(X_test)     \n",
    "        model_test_predictions_9 = model_5.predict(X_test)          \n",
    "                    \n",
    "        cols = Y_train.columns.values.tolist()   \n",
    "        \n",
    "        \n",
    "        print(\"train number of observations: \" + str(len(Y_train)))\n",
    "\n",
    "        \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_10\"] = model_test_predictions_1[:,i].tolist() if len(cols) > 1 else model_test_predictions_1.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_30\"] = model_test_predictions_3[:,i].tolist() if len(cols) > 1 else model_test_predictions_3.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_50\"] = model_test_predictions_5[:,i].tolist() if len(cols) > 1 else model_test_predictions_5.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_70\"] = model_test_predictions_7[:,i].tolist() if len(cols) > 1 else model_test_predictions_7.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_90\"] = model_test_predictions_9[:,i].tolist() if len(cols) > 1 else model_test_predictions_9.tolist() \n",
    "          \n",
    "           \n",
    "        return actuals_and_forecast_df\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "def rolling_walk_forward_validation(model_1, model_2, model_3, model_4, model_5, data, targets, start_time, end_time, training_days, path):\n",
    "\n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)            \n",
    "        results = pd.DataFrame()\n",
    "            \n",
    "\n",
    "        date_format = \"%d/%m/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "        \n",
    "        while start_time < end_time:\n",
    "            \n",
    "            #Train interval\n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time \n",
    "    \n",
    "            #Test interval, the test period is always the day ahead forecast\n",
    "            test_start_time = train_end_time + td(hours=0)\n",
    "            test_end_time = test_start_time + td(hours=1)\n",
    "            \n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "    \n",
    "            #Generate the calibration and test dataframes.\n",
    "            train_X, train_y, test_X, test_y, test_df = generate_train_and_test_dataframes(participant_df=dat, train_start_time=train_start_time.strftime(\"%d/%m/%Y %H:%M\"), train_end_time=train_end_time.strftime(\"%d/%m/%Y %H:%M\"), \n",
    "                            test_start_time=test_start_time.strftime(\"%d/%m/%Y %H:%M\"), test_end_time=test_end_time.strftime(\"%d/%m/%Y %H:%M\"))\n",
    "            \n",
    "            if train_X is None or len(train_X) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "    \n",
    "            if test_X is None or len(test_X) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "            \n",
    "            actuals_and_forecast_df = fit_multitarget_model(model_1=model_1,model_2=model_2,model_3=model_3, model_4=model_4,model_5=model_5, X_train=train_X, Y_train=train_y,\n",
    "                                            X_test=test_X, Y_test=test_y, actuals_and_forecast_df=test_df,targets=Y.columns.values.tolist())\n",
    "    \n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "            print(results)\n",
    "            start_time = start_time + td(hours=24)\n",
    "            \n",
    "        results.to_csv(path  + \".csv\", index = False)\n",
    "        \n",
    "          \n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "\n",
    "\n",
    "rolling_walk_forward_validation(model_1=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.1, learning_rate = 0.01, num_leaves=20,  max_depth = 18, n_estimators =  60000)),\n",
    "                                model_2=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.3, learning_rate = 0.01, num_leaves=20,  max_depth = 18, n_estimators =  60000)),\n",
    "                                model_3=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.5, learning_rate = 0.01, num_leaves=20,  max_depth = 18, n_estimators =  60000)),\n",
    "                                model_4=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.7, learning_rate = 0.01, num_leaves=20,  max_depth = 18, n_estimators =  60000)),\n",
    "                                model_5=MultiOutputRegressor(lgb.LGBMRegressor(objective = 'quantile', alpha=0.9, learning_rate = 0.01, num_leaves=20,  max_depth = 18, n_estimators =  60000)),\n",
    "                                data=dat, start_time='1/6/2020 00:00', end_time='1/9/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(),training_days=-210, path=\"/home/coconnor/lgbm_Q_DAM_1-3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c7961",
   "metadata": {},
   "source": [
    "random forest & KNN library for quantile forecast in the DAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "from datetime import datetime\n",
    "# from pandas import Timedelta as td\n",
    "import traceback\n",
    "# from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn_quantile import RandomForestQuantileRegressor\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "date_format = \"%d/%m/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/coconnor/DAM_VAR_1-3.csv\", index_col=\"DeliveryPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "\n",
    "Y = dat.iloc[:, 0:24]\n",
    "\n",
    "def generate_train_and_test_dataframes(participant_df: pd.DataFrame, train_start_time: dt, train_end_time: dt, \\\n",
    "                        test_start_time: dt, test_end_time: dt):\n",
    "\n",
    "    train_X = None\n",
    "    train_y = None \n",
    "    test_X = None \n",
    "    test_y = None \n",
    "    test_df = None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        if len(participant_df) == 0:\n",
    "            print(\"Warning: generate_train_and_test_dataframes method, participant_df has 0 rows. Ending.\")\n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "        original_columns = list(participant_df.columns)\n",
    "\n",
    "        participant_df = participant_df.dropna()\n",
    "\n",
    "        date_format = \"%d/%m/%Y %H:%M\"\n",
    "\n",
    "        train_df = None\n",
    "        \n",
    "\n",
    "        \n",
    "        train_start_time_str = dt.datetime.strptime(train_start_time, date_format)\n",
    "        train_end_time_str = dt.datetime.strptime(train_end_time, date_format)    \n",
    "        train_df = participant_df[(participant_df.index>=train_start_time_str) & (participant_df.index<train_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if train_df is None or len(train_df) == 0:\n",
    "            print(\"Don't have a train dataframe for train_start_time: \" + train_start_time_str + \", train_end_time: \" + train_end_time_str + \", exiting.\")            \n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "\n",
    "        \n",
    "        test_start_time_str = dt.datetime.strptime(test_start_time, date_format)    \n",
    "        test_end_time_str = dt.datetime.strptime(test_end_time, date_format) \n",
    "        test_df = participant_df[(participant_df.index>=test_start_time_str) & (participant_df.index<test_end_time_str)].copy(deep=\"True\")\n",
    "\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\"Don't have a test dataframe for test_start_time: \" + test_start_time_str + \", test_end_time: \" + test_end_time_str + \", exiting.\")            \n",
    "            return train_X, train_y, test_X, test_y, test_df\n",
    "\n",
    "\n",
    "        train_X = train_df.iloc[:, 24:]\n",
    "        test_X = test_df.iloc[:, 24:]\n",
    "        train_y = train_df.iloc[:, 0:24]\n",
    "        test_y = test_df.iloc[:, 0:24]\n",
    "                                \n",
    "        return train_X, train_y, test_X, test_y, test_df\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: generate_train_and_test_dataframes method.\")\n",
    "        traceback.print_exc()\n",
    "        return train_X, train_y, test_X, test_y, test_df\n",
    "    \n",
    "    \n",
    "def fit_multitarget_model(model, X_train, Y_train, X_test, Y_test, actuals_and_forecast_df, targets):\n",
    "  \n",
    "    try:\n",
    "        \n",
    "        model.fit(X_train, Y_train) \n",
    "\n",
    "        model_test_predictions=None  \n",
    "        model_test_predictions = model.predict(X_test).reshape(5,24)                            \n",
    "        cols = Y_train.columns.values.tolist()   \n",
    "        \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_10\"] = model_test_predictions[0:1,i].tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "            \n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_30\"] = model_test_predictions[1:2,i].tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_50\"] = model_test_predictions[2:3,i].tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_70\"] = model_test_predictions[3:4,i].tolist() if len(cols) > 1 else model_test_predictions.tolist() \n",
    "\n",
    "        for i in range(0,len(cols)):    \n",
    "            actuals_and_forecast_df[cols[i]+\"_Forecast_90\"] = model_test_predictions[4:,i].tolist() if len(cols) > 1 else model_test_predictions.tolist()\n",
    "            \n",
    "        print(\"train number of observations: \" + str(len(Y_train)))\n",
    "        \n",
    "        \n",
    "        \n",
    "           \n",
    "        return actuals_and_forecast_df\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"Error: fit_multitarget_model method.\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "def rolling_walk_forward_validation(model, data, targets, start_time, end_time, training_days, path):\n",
    " \n",
    "    try:\n",
    "\n",
    "        all_columns = list(data.columns)            \n",
    "        results = pd.DataFrame()\n",
    "            \n",
    "\n",
    "        date_format=\"%d/%m/%Y %H:%M\"\n",
    "        start_time = dt.datetime.strptime(start_time, date_format)\n",
    "        end_time = dt.datetime.strptime(end_time, date_format)\n",
    "        \n",
    "        while start_time < end_time:\n",
    "            \n",
    "            train_start_time = start_time + td(days=training_days)\n",
    "            train_end_time = start_time \n",
    "    \n",
    "            test_start_time = train_end_time + td(hours=0)\n",
    "            test_end_time = test_start_time + td(hours=1)\n",
    "            \n",
    "            print(\"train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \\\n",
    "                  \", test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time))\n",
    "    \n",
    "            train_X, train_y, test_X, test_y, test_df = generate_train_and_test_dataframes(participant_df=dat, train_start_time=train_start_time.strftime(\"%d/%m/%Y %H:%M\"), train_end_time=train_end_time.strftime(\"%d/%m/%Y %H:%M\"), \n",
    "                            test_start_time=test_start_time.strftime(\"%d/%m/%Y %H:%M\"), test_end_time=test_end_time.strftime(\"%d/%m/%Y %H:%M\"))\n",
    "            \n",
    "            if train_X is None or len(train_X) == 0:\n",
    "                print(\"Don't have a train dataframe for train_start_time: \" + str(train_start_time) + \", train_end_time: \" + str(train_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "    \n",
    "            if test_X is None or len(test_X) == 0:\n",
    "                print(\"Don't have a test dataframe for test_start_time: \" + str(test_start_time) + \", test_end_time: \" + str(test_end_time) + \", skipping.\")\n",
    "                start_time = start_time + td(days=training_days)\n",
    "                continue\n",
    "            \n",
    "            actuals_and_forecast_df = fit_multitarget_model(model=model, X_train=train_X, Y_train=train_y,\n",
    "                                            X_test=test_X, Y_test=test_y, actuals_and_forecast_df=test_df,targets=Y.columns.values.tolist())\n",
    "    \n",
    "            results = results.append(actuals_and_forecast_df)\n",
    "            print(results)\n",
    "            start_time = start_time + td(hours=24)\n",
    "            \n",
    "        results.to_csv(path  + \".csv\", index = False)\n",
    "        \n",
    "          \n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error: rolling_walk_forward_validation method.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "rolling_walk_forward_validation(model = MultiOutputRegressor(RandomForestQuantileRegressor(q=[0.10, 0.30, 0.50, 0.70, 0.90], max_depth=2 , min_samples_leaf=2 , n_estimators=100 , min_samples_split=2 )),\n",
    "                                data=dat, start_time='1/6/2020 00:00', end_time='1/9/2020  00:00',\n",
    "                                targets=Y.columns.values.tolist(),training_days=-210, path=\"/home/coconnor/rf_Q_DAM_1-3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
