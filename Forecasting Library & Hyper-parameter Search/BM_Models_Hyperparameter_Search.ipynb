{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4833fcb2",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pandas import concat\n",
    "from math import sqrt\n",
    "from sklearn import preprocessing\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.utils import check_array\n",
    "# from epftoolbox.models import LEAR\n",
    "from epftoolbox.models import evaluate_lear_in_test_dataset\n",
    "from epftoolbox.evaluation import sMAPE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.robust import mad\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LassoLarsIC, Lasso\n",
    "from epftoolbox.data import scaling\n",
    "from epftoolbox.data import read_data\n",
    "from epftoolbox.evaluation import MAE, sMAPE\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning]\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.layers import concatenate, Flatten, Dense, BatchNormalization, Dropout\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import \n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, LSTM, concatenate\n",
    "from tensorflow.keras import optimizers, initializers\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.activations import *\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ca76bc",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d43410",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "date_parse = lambda date: dt.datetime.strptime(date, date_format)\n",
    "dat = pd.read_csv(\"/home/ciaran/Documents/BM_data.csv\", index_col=\"SettlementPeriod\", parse_dates=True, date_parser=date_parse)\n",
    "\n",
    "dat = dat.drop([\"index\"], axis=1)\n",
    "dat = pd.DataFrame(dat)\n",
    "dat = dat.bfill(axis='rows')\n",
    "dat = dat.ffill(axis='rows')\n",
    "dat = dat._get_numeric_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08cb707",
   "metadata": {},
   "source": [
    "Random Forest Hyper-parameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a8337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#months1-7(210)\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[:8738,:]\n",
    "Y_train=Y.iloc[:8738,:]\n",
    "X_test=X.iloc[8738:8739,:]\n",
    "Y_test=Y.iloc[8738:8739,:]\n",
    "\n",
    "#months1-10(300)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:13154,:]\n",
    "# Y_train=Y.iloc[:13154,:]\n",
    "# X_test=X.iloc[13154:13155,:]\n",
    "# Y_test=Y.iloc[13154:13155,:]\n",
    "\n",
    "#months1-13(390)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:17522,:]\n",
    "# Y_train=Y.iloc[:17522,:]\n",
    "# X_test=X.iloc[17522:17523,:]\n",
    "# Y_test=Y.iloc[17522:17523,:]\n",
    "\n",
    "#months1-16(450)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:21842,:]\n",
    "# Y_train=Y.iloc[:21842,:]\n",
    "# X_test=X.iloc[21842:21843,:]\n",
    "# Y_test=Y.iloc[21842:21843,:]\n",
    "\n",
    "def hyperParameterTuning(X_train, Y_train):\n",
    "       param_tuning = {'bootstrap': [True],\n",
    "                   'max_depth': [40, 60, 80],\n",
    "                   'max_features': [40, 60, 80],\n",
    "                   'min_samples_leaf' : [20, 40, 60],\n",
    "                   'min_samples_split' : [2, 4, 6],\n",
    "                   'n_estimators': [800, 1600, 2400]\n",
    "                  }\n",
    " \n",
    "       RF_Regressor = RandomForestRegressor()\n",
    "\n",
    "       gsearch = GridSearchCV(estimator = RF_Regressor,\n",
    "                          param_grid = param_tuning,                        \n",
    "                           scoring = 'neg_mean_absolute_error', #MAE\n",
    "                          cv = 10,\n",
    "                          n_jobs = -1,\n",
    "                          verbose = 3)\n",
    "\n",
    "       gsearch.fit(X_train,Y_train)\n",
    "       return gsearch.best_params_\n",
    "\n",
    "hyperParameterTuning(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72379b93",
   "metadata": {},
   "source": [
    "XGB Hyper-parameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf9dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#months1-7(210)\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[:8738,:]\n",
    "Y_train=Y.iloc[:8738,:]\n",
    "X_test=X.iloc[8738:8739,:]\n",
    "Y_test=Y.iloc[8738:8739,:]\n",
    "\n",
    "#months1-10(300)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:13154,:]\n",
    "# Y_train=Y.iloc[:13154,:]\n",
    "# X_test=X.iloc[13154:13155,:]\n",
    "# Y_test=Y.iloc[13154:13155,:]\n",
    "\n",
    "#months1-13(390)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:17522,:]\n",
    "# Y_train=Y.iloc[:17522,:]\n",
    "# X_test=X.iloc[17522:17523,:]\n",
    "# Y_test=Y.iloc[17522:17523,:]\n",
    "\n",
    "#months1-16(450)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:21842,:]\n",
    "# Y_train=Y.iloc[:21842,:]\n",
    "# X_test=X.iloc[21842:21843,:]\n",
    "# Y_test=Y.iloc[21842:21843,:]\n",
    "\n",
    "def hyperParameterTuning(X_train, Y_train):\n",
    "       param_tuning = {'max_depth': [1, 2, 4],\n",
    "                       'learning_rate': [.01, 0.03, 0.05, 0.08, 0.1],\n",
    "                       'n_estimators': [25, 50, 75, 100, 200]\n",
    "                  }\n",
    " \n",
    "       RF_Regressor = XGBRegressor()\n",
    "    \n",
    "       gsearch = GridSearchCV(estimator = RF_Regressor,\n",
    "                          param_grid = param_tuning,                        \n",
    "                           scoring = 'neg_mean_absolute_error', #MAE\n",
    "#                            scoring = 'neg_mean_squared_error',  #MSE\n",
    "                          cv = 10,\n",
    "                          n_jobs = -1,\n",
    "                          verbose = 0)\n",
    "\n",
    "       gsearch.fit(X_train,Y_train)\n",
    "       return gsearch.best_params_\n",
    "\n",
    "hyperParameterTuning(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26224a2c",
   "metadata": {},
   "source": [
    "Support Vector Regression Models Hyper-parameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#months1-7(210)\n",
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[:8738,:]\n",
    "Y_train=Y.iloc[:8738,:]\n",
    "X_test=X.iloc[8738:8739,:]\n",
    "Y_test=Y.iloc[8738:8739,:]\n",
    "\n",
    "#months1-10(300)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:13154,:]\n",
    "# Y_train=Y.iloc[:13154,:]\n",
    "# X_test=X.iloc[13154:13155,:]\n",
    "# Y_test=Y.iloc[13154:13155,:]\n",
    "\n",
    "#months1-13(390)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:17522,:]\n",
    "# Y_train=Y.iloc[:17522,:]\n",
    "# X_test=X.iloc[17522:17523,:]\n",
    "# Y_test=Y.iloc[17522:17523,:]\n",
    "\n",
    "#months1-16(450)\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:21842,:]\n",
    "# Y_train=Y.iloc[:21842,:]\n",
    "# X_test=X.iloc[21842:21843,:]\n",
    "# Y_test=Y.iloc[21842:21843,:]\n",
    "\n",
    "def hyperParameterTuning(X_train, Y_train):\n",
    "       param_tuning = { 'C': [0.01, 0.1],\n",
    "                        'gamma': [0.0001, 0.001, 0.005],\n",
    "                        'epsilon': [0.001, 0.1, 0.3],\n",
    "                        'kernel': [\"rbf\"]\n",
    "                      }\n",
    " \n",
    "       RF_Regressor = SVR()\n",
    "\n",
    "       gsearch = GridSearchCV(estimator = RF_Regressor,\n",
    "                          param_grid = param_tuning,                        \n",
    "                           scoring = 'neg_mean_absolute_error', #MAE\n",
    "#                            scoring = 'neg_mean_squared_error',  #MSE\n",
    "                          cv = 2,\n",
    "                          n_jobs = -1,\n",
    "                          verbose = 0)\n",
    "\n",
    "       gsearch.fit(X_train,Y_train)\n",
    "       return gsearch.best_params_\n",
    "\n",
    "hyperParameterTuning(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a135616",
   "metadata": {},
   "source": [
    "Single Headed DNN Models Hyper-paramater Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc71f0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[:7250,:]\n",
    "Y_train=Y.iloc[:7250,:]\n",
    "X_test=X.iloc[7250:8739,:]\n",
    "Y_test=Y.iloc[7250:8739,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:11666,:]\n",
    "# Y_train=Y.iloc[:11666,:]\n",
    "# X_test=X.iloc[11666:13155,:]\n",
    "# Y_test=Y.iloc[11666:13155,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:16082,:]\n",
    "# Y_train=Y.iloc[:16082,:]\n",
    "# X_test=X.iloc[16082:17523,:]\n",
    "# Y_test=Y.iloc[16082:17523,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:20498,:]\n",
    "# Y_train=Y.iloc[:20498,:]\n",
    "# X_test=X.iloc[20498:21843,:]\n",
    "# Y_test=Y.iloc[20498:21843,:]\n",
    "\n",
    "rnn_train1_a=X_train.loc[:,\"lag_-3x1\":\"lag_-18x1\"]\n",
    "rnn_train1_b=X_train.loc[:,\"lag_-19x1\":\"lag_-34x1\"]\n",
    "rnn_train1_c=X_train.loc[:,\"lag_-35x1\":\"lag_-50x1\"]        \n",
    "rnn_train2_a=X_train.loc[:,\"lag_-3x2\":\"lag_-18x2\"]\n",
    "rnn_train2_b=X_train.loc[:,\"lag_-19x2\":\"lag_-34x2\"]\n",
    "rnn_train2_c=X_train.loc[:,\"lag_-35x2\":\"lag_-50x2\"]        \n",
    "rnn_train3_a=X_train.loc[:,\"lag_-2x3\":\"lag_-17x3\"]\n",
    "rnn_train3_b=X_train.loc[:,\"lag_-18x3\":\"lag_-33x3\"]\n",
    "rnn_train3_c=X_train.loc[:,\"lag_-34x3\":\"lag_-49x3\"]      \n",
    "rnn_train4_a=X_train.loc[:,\"lag_0x6\":\"lag_-15x6\"]\n",
    "rnn_train4_b=X_train.loc[:,\"lag_-16x6\":\"lag_-31x6\"]\n",
    "rnn_train4_c=X_train.loc[:,\"lag_-32x6\":\"lag_-47x6\"]        \n",
    "rnn_train5_a=X_train.loc[:,\"lag_-2x12\":\"lag_-17x12\"]\n",
    "rnn_train5_b=X_train.loc[:,\"lag_-18x12\":\"lag_-33x12\"]\n",
    "rnn_train5_c=X_train.loc[:,\"lag_-34x12\":\"lag_-49x12\"]        \n",
    "rnn_train6=X_train.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "rnn_train7=X_train.loc[:,\"lag_2x8\":\"lag_17x8\"]\n",
    "rnn_train8=X_train.loc[:,\"lag_2x9\":\"lag_17x9\"]\n",
    "rnn_train9=X_train.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "rnn_train10=X_train.loc[:,\"lag_2x11\":\"lag_17x11\"]        \n",
    "        \n",
    "rnn_test1_a= X_test.loc[:,\"lag_-3x1\":\"lag_-18x1\"]\n",
    "rnn_test1_b= X_test.loc[:,\"lag_-19x1\":\"lag_-34x1\"]\n",
    "rnn_test1_c= X_test.loc[:,\"lag_-35x1\":\"lag_-50x1\"]\n",
    "rnn_test2_a= X_test.loc[:,\"lag_-3x2\":\"lag_-18x2\"]\n",
    "rnn_test2_b= X_test.loc[:,\"lag_-19x2\":\"lag_-34x2\"]\n",
    "rnn_test2_c= X_test.loc[:,\"lag_-35x2\":\"lag_-50x2\"]\n",
    "rnn_test3_a= X_test.loc[:,\"lag_-2x3\":\"lag_-17x3\"]\n",
    "rnn_test3_b= X_test.loc[:,\"lag_-18x3\":\"lag_-33x3\"]\n",
    "rnn_test3_c= X_test.loc[:,\"lag_-34x3\":\"lag_-49x3\"]\n",
    "rnn_test4_a= X_test.loc[:,\"lag_0x6\":\"lag_-15x6\"]\n",
    "rnn_test4_b= X_test.loc[:,\"lag_-16x6\":\"lag_-31x6\"]\n",
    "rnn_test4_c= X_test.loc[:,\"lag_-32x6\":\"lag_-47x6\"]\n",
    "rnn_test5_a= X_test.loc[:,\"lag_-2x12\":\"lag_-17x12\"]\n",
    "rnn_test5_b= X_test.loc[:,\"lag_-18x12\":\"lag_-33x12\"]\n",
    "rnn_test5_c= X_test.loc[:,\"lag_-34x12\":\"lag_-49x12\"]\n",
    "rnn_test6=X_test.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "rnn_test7=X_test.loc[:,\"lag_2x8\":\"lag_17x8\"]\n",
    "rnn_test8=X_test.loc[:,\"lag_2x9\":\"lag_17x9\"]\n",
    "rnn_test9=X_test.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "rnn_test10=X_test.loc[:,\"lag_2x11\":\"lag_17x11\"]\n",
    "\n",
    "rnn_Y=Y_train.loc[:,\"lag_2y\" : \"lag_17y\"]\n",
    "\n",
    "X_scaler1_a = preprocessing.MinMaxScaler()\n",
    "X_scaler1_b = preprocessing.MinMaxScaler()\n",
    "X_scaler1_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler2_a = preprocessing.MinMaxScaler()\n",
    "X_scaler2_b = preprocessing.MinMaxScaler()\n",
    "X_scaler2_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler3_a = preprocessing.MinMaxScaler()\n",
    "X_scaler3_b = preprocessing.MinMaxScaler()\n",
    "X_scaler3_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler4_a = preprocessing.MinMaxScaler()\n",
    "X_scaler4_b = preprocessing.MinMaxScaler()\n",
    "X_scaler4_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler5_a = preprocessing.MinMaxScaler()\n",
    "X_scaler5_b = preprocessing.MinMaxScaler()\n",
    "X_scaler5_c = preprocessing.MinMaxScaler()\n",
    "\n",
    "\n",
    "X_scaler6 = preprocessing.MinMaxScaler()\n",
    "X_scaler7 = preprocessing.MinMaxScaler()\n",
    "X_scaler8 = preprocessing.MinMaxScaler()\n",
    "X_scaler9 = preprocessing.MinMaxScaler()\n",
    "X_scaler10 = preprocessing.MinMaxScaler()\n",
    "\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train1_a = X_scaler1_a.fit_transform(rnn_train1_a)\n",
    "rnn_scaled_train1_b = X_scaler1_b.fit_transform(rnn_train1_b)\n",
    "rnn_scaled_train1_c = X_scaler1_c.fit_transform(rnn_train1_c)\n",
    "\n",
    "rnn_scaled_train2_a = X_scaler2_a.fit_transform(rnn_train2_a)\n",
    "rnn_scaled_train2_b = X_scaler2_b.fit_transform(rnn_train2_b)\n",
    "rnn_scaled_train2_c = X_scaler2_c.fit_transform(rnn_train2_c)\n",
    "\n",
    "rnn_scaled_train3_a = X_scaler3_a.fit_transform(rnn_train3_a)\n",
    "rnn_scaled_train3_b = X_scaler3_b.fit_transform(rnn_train3_b)\n",
    "rnn_scaled_train3_c = X_scaler3_c.fit_transform(rnn_train3_c)\n",
    "\n",
    "rnn_scaled_train4_a = X_scaler4_a.fit_transform(rnn_train4_a)\n",
    "rnn_scaled_train4_b = X_scaler4_b.fit_transform(rnn_train4_b)\n",
    "rnn_scaled_train4_c = X_scaler4_c.fit_transform(rnn_train4_c)\n",
    "\n",
    "rnn_scaled_train5_a = X_scaler5_a.fit_transform(rnn_train5_a)\n",
    "rnn_scaled_train5_b = X_scaler5_b.fit_transform(rnn_train5_b)\n",
    "rnn_scaled_train5_c = X_scaler5_c.fit_transform(rnn_train5_c)\n",
    "\n",
    "rnn_scaled_train6 = X_scaler6.fit_transform(rnn_train6)\n",
    "rnn_scaled_train7 = X_scaler7.fit_transform(rnn_train7)\n",
    "rnn_scaled_train8 = X_scaler8.fit_transform(rnn_train8)\n",
    "rnn_scaled_train9 = X_scaler9.fit_transform(rnn_train9)\n",
    "rnn_scaled_train10 = X_scaler10.fit_transform(rnn_train10)\n",
    "\n",
    "Y_train_Scaled   = Y_scaler.fit_transform(Y_train)\n",
    "Y_test_scaled = Y_scaler.transform(Y_test)\n",
    "\n",
    "X_train_Scaled = np.hstack(\n",
    "    (rnn_scaled_train1_a, rnn_scaled_train1_b, rnn_scaled_train1_c, rnn_scaled_train2_a, rnn_scaled_train2_b, rnn_scaled_train2_c,\n",
    "     rnn_scaled_train3_a, rnn_scaled_train3_b, rnn_scaled_train3_c, rnn_scaled_train4_a, rnn_scaled_train4_b, rnn_scaled_train4_c,\n",
    "     rnn_scaled_train5_a, rnn_scaled_train5_b, rnn_scaled_train5_c,rnn_scaled_train6,rnn_scaled_train7, rnn_scaled_train8,\n",
    "     rnn_scaled_train9, rnn_scaled_train10)\n",
    ").reshape(rnn_train6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled = np.hstack(\n",
    "    (X_scaler1_a.transform(rnn_test1_a),X_scaler1_b.transform(rnn_test1_b),X_scaler1_c.transform(rnn_test1_c),\n",
    "     X_scaler2_a.transform(rnn_test2_a),X_scaler2_b.transform(rnn_test2_b),X_scaler2_c.transform(rnn_test2_c),\n",
    "     X_scaler3_a.transform(rnn_test3_a),X_scaler3_b.transform(rnn_test3_b),X_scaler3_c.transform(rnn_test3_c),\n",
    "     X_scaler4_a.transform(rnn_test4_a),X_scaler4_b.transform(rnn_test4_b),X_scaler4_c.transform(rnn_test4_c),\n",
    "     X_scaler5_a.transform(rnn_test5_a),X_scaler5_b.transform(rnn_test5_b),X_scaler5_c.transform(rnn_test5_c),\n",
    "     X_scaler6.transform(rnn_test6),X_scaler7.transform(rnn_test7),X_scaler8.transform(rnn_test8),\n",
    "     X_scaler9.transform(rnn_test9), X_scaler10.transform(rnn_test10))\n",
    ").reshape(rnn_test6.shape[0], 20, 16).transpose(0, 2, 1)\n",
    "\n",
    "\n",
    "score_acc = make_scorer(mean_absolute_error)\n",
    "mse = make_scorer(MSE, greater_is_better=False)\n",
    "i_shape=(X_train_Scaled.shape[1], X_train_Scaled.shape[2])\n",
    "      \n",
    "pp ={\n",
    "    'neurons_0': list(range(16, 256, 16)),\n",
    "    'activation_0':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "    'neurons_1': list(range(16, 256, 16)),\n",
    "    'activation_1':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "    'neurons_2': list(range(16, 256, 16)),\n",
    "    'activation_2':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "    'neurons_3': list(range(16, 256, 16)),\n",
    "    'activation_3':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "    'neurons_4': list(range(16, 256, 16)),\n",
    "    'activation_4':['relu', 'sigmoid',  'tanh', LeakyReLU],\n",
    "\n",
    "    'learning_rate': list(np.linspace(0.0001,0.02, 10)),        \n",
    "    'layers1': list(range(1,3, 1)),\n",
    "    'layers2': list(range(1,3, 1)),\n",
    "    'layers3': list(range(1,3, 1)),\n",
    "    'layers4': list(range(1,3, 1)),\n",
    "    'dropout_rate': list(np.linspace(0.0,0.2, 10)),\n",
    "    'batch_size': list(range(16, 64, 16)),\n",
    "    'epochs': [200]\n",
    "}\n",
    "\n",
    "def create_model(x_train, y_train, x_val, y_val, params):\n",
    "            nn = Sequential()\n",
    "            nn.add(Flatten(input_shape=i_shape))\n",
    "\n",
    "            for i in range(params['layers1']):\n",
    "                nn.add(Dense(params['neurons_0'], input_shape=i_shape, activation=params['activation_0']))\n",
    "                \n",
    "            for i in range(params['layers2']):\n",
    "                nn.add(Dense(params['neurons_1'], activation=params['activation_1']))\n",
    "   \n",
    "            for i in range(params['layers3']):\n",
    "                nn.add(Dense(params['neurons_2'], activation=params['activation_2']))\n",
    "            \n",
    "            nn.add(Dropout(params['dropout_rate'], seed=123))\n",
    "            \n",
    "            for i in range(params['layers4']):\n",
    "                nn.add(Dense(params['neurons_3'], activation=params['activation_3']))\n",
    "                \n",
    "            nn.add(Dense(params['neurons_4'], activation=params['activation_4']))\n",
    "            nn.add(Dense(16))\n",
    "            opt = Adam(lr = params['learning_rate'])\n",
    "            nn.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mean_absolute_error'])\n",
    "            out=nn.fit(x_train, y_train,validation_data=[x_val, y_val], batch_size=params['batch_size'], epochs=params['epochs'], verbose=2, \n",
    "                          callbacks=[talos.utils.early_stopper(epochs=params['epochs'], mode='moderate', \n",
    "                                                               min_delta=0.001, monitor='mean_absolute_error')])\n",
    "            return out, nn\n",
    "        \n",
    "h = talos.Scan(x=X_train_Scaled,y=Y_train_Scaled,x_val=X_test_Scaled,  y_val=Y_test_scaled, params=pp, model=create_model,val_split=0.2,\n",
    "            experiment_name ='bm_1-3', random_method='quantum', round_limit=30, print_params=True)\n",
    "h.data.sort_values(by='val_mean_absolute_error', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3618b0e",
   "metadata": {},
   "source": [
    "Multi-headed DNN RNN Hyper-paramater Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b71fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=dat.iloc[:, 0:16]\n",
    "X=dat.iloc[:,16:]\n",
    "X_train=X.iloc[:7250,:]\n",
    "Y_train=Y.iloc[:7250,:]\n",
    "X_test=X.iloc[7250:8739,:]\n",
    "Y_test=Y.iloc[7250:8739,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:11666,:]\n",
    "# Y_train=Y.iloc[:11666,:]\n",
    "# X_test=X.iloc[11666:13155,:]\n",
    "# Y_test=Y.iloc[11666:13155,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:16082,:]\n",
    "# Y_train=Y.iloc[:16082,:]\n",
    "# X_test=X.iloc[16082:17523,:]\n",
    "# Y_test=Y.iloc[16082:17523,:]\n",
    "\n",
    "# Y=dat.iloc[:, 0:16]\n",
    "# X=dat.iloc[:,16:]\n",
    "# X_train=X.iloc[:20498,:]\n",
    "# Y_train=Y.iloc[:20498,:]\n",
    "# X_test=X.iloc[20498:21843,:]\n",
    "# Y_test=Y.iloc[20498:21843,:]\n",
    "\n",
    "# Extracting specific lagged features for LSTM and FFNN models\n",
    "rnn_train_LSTM_1 = X_train.loc[:,\"lag_-3x1\":\"lag_-50x1\"]\n",
    "rnn_train_LSTM_2 = X_train.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "rnn_train_LSTM_3 = X_train.loc[:,\"lag_-2x3\":\"lag_-49x3\"]\n",
    "rnn_train_LSTM_4 = X_train.loc[:,\"lag_0x6\":\"lag_-47x6\"]\n",
    "rnn_train_LSTM_5 = X_train.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "\n",
    "rnn_test_LSTM_1 = X_test.loc[:,\"lag_-3x1\": \"lag_-50x1\"]\n",
    "rnn_test_LSTM_2 = X_test.loc[:,\"lag_-3x2\": \"lag_-50x2\"]\n",
    "rnn_test_LSTM_3 = X_test.loc[:,\"lag_-2x3\": \"lag_-49x3\"]\n",
    "rnn_test_LSTM_4 = X_test.loc[:,\"lag_0x6\" : \"lag_-47x6\"]\n",
    "rnn_test_LSTM_5 = X_test.loc[:,\"lag_-2x12\": \"lag_-49x12\"]\n",
    "\n",
    "rnn_train_ffnn_1 = X_train.loc[:,\"lag_2x7\": \"lag_17x7\"]\n",
    "rnn_train_ffnn_2 = X_train.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "rnn_train_ffnn_3 = X_train.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "rnn_train_ffnn_4 = X_train.loc[:,\"lag_2x10\": \"lag_17x10\"]\n",
    "rnn_train_ffnn_5 = X_train.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "\n",
    "rnn_test_ffnn_1 = X_test.loc[:,\"lag_2x7\":\"lag_17x7\"]\n",
    "rnn_test_ffnn_2 = X_test.loc[:,\"lag_2x8\": \"lag_17x8\"]\n",
    "rnn_test_ffnn_3 = X_test.loc[:,\"lag_2x9\": \"lag_17x9\"]\n",
    "rnn_test_ffnn_4 = X_test.loc[:,\"lag_2x10\":\"lag_17x10\"]\n",
    "rnn_test_ffnn_5 = X_test.loc[:,\"lag_2x11\": \"lag_17x11\"]\n",
    "\n",
    "rnn_Y = Y_train.loc[:,\"lag_2y\": \"lag_17y\"]\n",
    "\n",
    "X_scaler_LSTM_1 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_2 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_3 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_4 = preprocessing.MinMaxScaler()\n",
    "X_scaler_LSTM_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaler_ffnn_1 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_2 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_3 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_4 = preprocessing.MinMaxScaler()\n",
    "X_scaler_ffnn_5 = preprocessing.MinMaxScaler()\n",
    "\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "rnn_scaled_train_LSTM_1 = X_scaler_LSTM_1.fit_transform(rnn_train_LSTM_1)\n",
    "rnn_scaled_train_LSTM_2 = X_scaler_LSTM_2.fit_transform(rnn_train_LSTM_2)\n",
    "rnn_scaled_train_LSTM_3 = X_scaler_LSTM_3.fit_transform(rnn_train_LSTM_3)\n",
    "rnn_scaled_train_LSTM_4 = X_scaler_LSTM_4.fit_transform(rnn_train_LSTM_4)\n",
    "rnn_scaled_train_LSTM_5 = X_scaler_LSTM_5.fit_transform(rnn_train_LSTM_5)\n",
    "\n",
    "rnn_scaled_train_ffnn_1 = X_scaler_ffnn_1.fit_transform(rnn_train_ffnn_1)\n",
    "rnn_scaled_train_ffnn_2 = X_scaler_ffnn_2.fit_transform(rnn_train_ffnn_2)\n",
    "rnn_scaled_train_ffnn_3 = X_scaler_ffnn_3.fit_transform(rnn_train_ffnn_3)\n",
    "rnn_scaled_train_ffnn_4 = X_scaler_ffnn_4.fit_transform(rnn_train_ffnn_4)\n",
    "rnn_scaled_train_ffnn_5 = X_scaler_ffnn_5.fit_transform(rnn_train_ffnn_5)\n",
    "\n",
    "Y_train_Scaled = Y_scaler.fit_transform(Y_train)\n",
    "Y_test_scaled = Y_scaler.transform(Y_test)\n",
    "\n",
    "X_train_Scaled_LSTM = np.hstack(\n",
    "    (rnn_scaled_train_LSTM_1, rnn_scaled_train_LSTM_2, rnn_scaled_train_LSTM_3,\n",
    "     rnn_scaled_train_LSTM_4, rnn_scaled_train_LSTM_5)\n",
    ").reshape(rnn_train_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "X_train_Scaled_ffnn = np.hstack(\n",
    "    (rnn_scaled_train_ffnn_1, rnn_scaled_train_ffnn_2, rnn_scaled_train_ffnn_3,\n",
    "     rnn_scaled_train_ffnn_4, rnn_scaled_train_ffnn_5)\n",
    ").reshape(rnn_train_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled_LSTM = np.hstack(\n",
    "    (X_scaler_LSTM_1.transform(rnn_test_LSTM_1), X_scaler_LSTM_2.transform(rnn_test_LSTM_2),\n",
    "     X_scaler_LSTM_3.transform(rnn_test_LSTM_3), X_scaler_LSTM_4.transform(rnn_test_LSTM_4),\n",
    "     X_scaler_LSTM_5.transform(rnn_test_LSTM_5))\n",
    ").reshape(rnn_test_LSTM_1.shape[0], 5, 48).transpose(0, 2, 1)\n",
    "\n",
    "X_test_Scaled_ffnn = np.hstack(\n",
    "    (X_scaler_ffnn_1.transform(rnn_test_ffnn_1), X_scaler_ffnn_2.transform(rnn_test_ffnn_2),\n",
    "     X_scaler_ffnn_3.transform(rnn_test_ffnn_3), X_scaler_ffnn_4.transform(rnn_test_ffnn_4),\n",
    "     X_scaler_ffnn_5.transform(rnn_test_ffnn_5))\n",
    ").reshape(rnn_test_ffnn_1.shape[0], 5, 16).transpose(0, 2, 1)\n",
    "\n",
    "pp = {'lstm_neurons_0': [16, 32, 64, 128, 192, 256],\n",
    "\n",
    "      'ffnn_neurons_0': [16, 32, 64, 128, 192, 256],\n",
    "\n",
    "      'dense_f_neurons': [16, 32, 64, 128, 192, 256],\n",
    "      'lstm_activation_0': ['relu', 'sigmoid', 'tanh'],\n",
    "\n",
    "      'activation_0': ['relu', 'sigmoid', 'tanh', LeakyReLU],\n",
    "      'activation_1': ['relu', 'sigmoid', 'tanh', LeakyReLU],\n",
    "\n",
    "      'learning_rate': list(np.linspace(0.0001, 0.02, 10)),\n",
    "      'dropout_rate_lstm': list(np.linspace(0.0,0.2, 10)),\n",
    "      'dropout_rate_ffnn': list(np.linspace(0.0,0.2, 10)),\n",
    "      \n",
    "      'batch_size': [4, 8, 16, 32, 48],\n",
    "      'epochs': [300]\n",
    "      }\n",
    "i_shape_lstm = (X_train_Scaled_LSTM.shape[1], X_train_Scaled_LSTM.shape[2])\n",
    "i_shape_ffnn = (X_train_Scaled_ffnn.shape[1], X_train_Scaled_ffnn.shape[2])\n",
    "\n",
    "\n",
    "def create_model(x_train, y_train, x_val, y_val, params):\n",
    "    visible1 = Input(shape=(i_shape_lstm))\n",
    "    dense1 = LSTM(params['lstm_neurons_0'], return_sequences=True, activation=params['lstm_activation_0'], input_shape=i_shape_lstm)(visible1)\n",
    "    dense2 = LSTM(params['lstm_neurons_0'], return_sequences=True, activation=params['lstm_activation_0'], input_shape=i_shape_lstm)(dense1)\n",
    "    do_lstm = Dropout(params['dropout_rate_lstm'])(dense2)\n",
    "    dense3 = LSTM(params['lstm_neurons_0'], return_sequences=True, activation=params['lstm_activation_0'], input_shape=i_shape_lstm)(do_lstm)\n",
    "    flat1 = Flatten()(dense3)\n",
    "\n",
    "    visible2 = Input(shape=(i_shape_ffnn))\n",
    "    dense5 = Dense(params['ffnn_neurons_0'], activation=params['activation_0'])(visible2)\n",
    "    dense6 = Dense(params['ffnn_neurons_0'], activation=params['activation_0'])(dense5)\n",
    "    do_ffnn = Dropout(params['dropout_rate_ffnn'])(dense6)\n",
    "    dense7 = Dense(params['ffnn_neurons_0'], activation=params['activation_0'])(do_ffnn)\n",
    "    flat2 = Flatten()(dense7)\n",
    "\n",
    "    merged = concatenate([flat1, flat2])\n",
    "    dense_f = Dense(params['dense_f_neurons'], activation=params['activation_1'])(merged)\n",
    "    output = Dense(16)(dense_f)\n",
    "    model = Model(inputs=[visible1, visible2], outputs=output)\n",
    "\n",
    "    opt = Adam(lr=params['learning_rate'])\n",
    "    model.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mean_absolute_error'])\n",
    "\n",
    "\n",
    "    out = model.fit(x=x_train, y=y_train, validation_data=[x_val, y_val],\n",
    "                    batch_size=params['batch_size'], epochs=params['epochs'], verbose=2,\n",
    "                    callbacks=[talos.utils.early_stopper(epochs=params['epochs'], mode='moderate',\n",
    "                                                         min_delta=0.001, monitor='mean_absolute_error')])\n",
    "    return out, model\n",
    "\n",
    "\n",
    "h = talos.Scan(x=[X_train_Scaled_LSTM, X_train_Scaled_ffnn], y=Y_train_Scaled, x_val=[X_train_Scaled_LSTM, X_train_Scaled_ffnn], y_val=Y_train_Scaled, params=pp, model=create_model,\n",
    "               experiment_name='MH_DNN_1-3', round_limit=30, print_params=True)\n",
    "h.data.sort_values(by='val_mean_absolute_error', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20941191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb609ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
